{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1b85f46",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Training a Neural Exchange-Correlation Functional\n",
    "\n",
    "This example demonstrates training a neural exchange-correlation (XC) functional\n",
    "from electron density data. Neural XC functionals can learn complex\n",
    "exchange-correlation energy patterns beyond traditional LDA/GGA approximations.\n",
    "\n",
    "**Key Concepts:**\n",
    "- Exchange-correlation energy in DFT\n",
    "- Density feature extraction\n",
    "- Attention mechanisms for non-local correlations\n",
    "- Physics constraints (negative XC energy, proper scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea38ae54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "SEED = 42\n",
    "HIDDEN_SIZES = (64, 64, 32)  # Neural network hidden layer sizes\n",
    "NUM_ATTENTION_HEADS = 4\n",
    "USE_ATTENTION = True\n",
    "USE_ADVANCED_FEATURES = True\n",
    "DROPOUT_RATE = 0.0  # No dropout for inference stability\n",
    "\n",
    "# Training configuration\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 100\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_TRAIN_SAMPLES = 500\n",
    "NUM_TEST_SAMPLES = 100\n",
    "GRID_POINTS = 32  # Points per density sample\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = \"docs/assets/examples/neural_xc_functional\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314edd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"Opifex Example: Training Neural XC Functional\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87541409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import optax\n",
    "from flax import nnx\n",
    "\n",
    "\n",
    "print(f\"JAX backend: {jax.default_backend()}\")\n",
    "print(f\"JAX devices: {jax.devices()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db17601",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from opifex.neural.quantum import NeuralXCFunctional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a60718a",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 1: Generate Training Data\n",
    "\n",
    "We generate synthetic electron density data and compute reference XC energies\n",
    "using the Local Density Approximation (LDA). The neural XC functional will\n",
    "learn to predict these energies from the density.\n",
    "\n",
    "**LDA XC Energy:**\n",
    "$$E_{xc}^{LDA} = -C_x \\\\int \\\\rho^{4/3} dr$$\n",
    "\n",
    "where $C_x \\\\approx 0.738$ for exchange."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d467c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"Generating training data...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "def generate_density_sample(key: jax.Array, grid_points: int) -> jax.Array:\n",
    "    \"\"\"Generate a physically reasonable electron density sample.\n",
    "\n",
    "    Uses a superposition of Gaussian functions to mimic atomic densities.\n",
    "    \"\"\"\n",
    "    key1, key2, key3 = jax.random.split(key, 3)\n",
    "\n",
    "    # Number of atomic centers (1-4)\n",
    "    n_centers = int(jax.random.randint(key1, (), 1, 5))\n",
    "\n",
    "    # Generate density as sum of Gaussians\n",
    "    x = jnp.linspace(-5.0, 5.0, grid_points)\n",
    "    density = jnp.zeros(grid_points)\n",
    "\n",
    "    for i in range(n_centers):\n",
    "        # Random center position and width\n",
    "        center = jax.random.uniform(\n",
    "            jax.random.fold_in(key2, i), (), minval=-3.0, maxval=3.0\n",
    "        )\n",
    "        width = jax.random.uniform(\n",
    "            jax.random.fold_in(key3, i), (), minval=0.5, maxval=2.0\n",
    "        )\n",
    "        amplitude = jax.random.uniform(\n",
    "            jax.random.fold_in(key1, i + 10), (), minval=0.5, maxval=2.0\n",
    "        )\n",
    "\n",
    "        # Add Gaussian contribution\n",
    "        density = density + amplitude * jnp.exp(-((x - center) ** 2) / (2 * width**2))\n",
    "\n",
    "    # Ensure positive density\n",
    "    density = jnp.maximum(density, 1e-10)\n",
    "\n",
    "    # Normalize to reasonable electron count (1-10 electrons)\n",
    "    target_electrons = jax.random.uniform(key1, (), minval=1.0, maxval=10.0)\n",
    "\n",
    "    return density / jnp.sum(density) * target_electrons\n",
    "\n",
    "\n",
    "def compute_density_gradients(density: jax.Array) -> jax.Array:\n",
    "    \"\"\"Compute density gradients (simplified 1D -> 3D gradient).\"\"\"\n",
    "    # Compute 1D gradient using finite differences\n",
    "    grad_1d = jnp.gradient(density)\n",
    "\n",
    "    # Expand to 3D gradient format (gradient only in first direction)\n",
    "    gradients = jnp.zeros((density.shape[0], 3))\n",
    "\n",
    "    return gradients.at[:, 0].set(grad_1d)\n",
    "\n",
    "\n",
    "def compute_lda_xc_energy(density: jax.Array) -> jax.Array:\n",
    "    \"\"\"Compute LDA exchange-correlation energy per grid point.\n",
    "\n",
    "    Uses Dirac exchange formula: E_x = -C_x * rho^(4/3)\n",
    "    Simplified correlation: E_c = -C_c * rho * log(1 + rho)\n",
    "    \"\"\"\n",
    "    # Exchange energy (Dirac)\n",
    "    c_x = 0.738  # Exchange coefficient\n",
    "    exchange = -c_x * jnp.power(jnp.maximum(density, 1e-12), 4 / 3)\n",
    "\n",
    "    # Correlation energy (simplified Wigner-like)\n",
    "    c_c = 0.044  # Correlation coefficient\n",
    "    correlation = -c_c * density * jnp.log1p(density + 1e-12)\n",
    "\n",
    "    return exchange + correlation\n",
    "\n",
    "\n",
    "# Generate training data\n",
    "print(f\"  Training samples: {NUM_TRAIN_SAMPLES}\")\n",
    "print(f\"  Test samples: {NUM_TEST_SAMPLES}\")\n",
    "print(f\"  Grid points per sample: {GRID_POINTS}\")\n",
    "\n",
    "key = jax.random.PRNGKey(SEED)\n",
    "\n",
    "# Generate training densities\n",
    "train_keys = jax.random.split(key, NUM_TRAIN_SAMPLES + 1)\n",
    "key = train_keys[0]\n",
    "train_densities = jnp.stack(\n",
    "    [generate_density_sample(k, GRID_POINTS) for k in train_keys[1:]]\n",
    ")\n",
    "\n",
    "# Generate test densities\n",
    "test_keys = jax.random.split(key, NUM_TEST_SAMPLES + 1)\n",
    "key = test_keys[0]\n",
    "test_densities = jnp.stack(\n",
    "    [generate_density_sample(k, GRID_POINTS) for k in test_keys[1:]]\n",
    ")\n",
    "\n",
    "# Compute gradients\n",
    "train_gradients = jnp.stack([compute_density_gradients(d) for d in train_densities])\n",
    "test_gradients = jnp.stack([compute_density_gradients(d) for d in test_densities])\n",
    "\n",
    "# Compute reference LDA XC energies\n",
    "train_xc_ref = jnp.stack([compute_lda_xc_energy(d) for d in train_densities])\n",
    "test_xc_ref = jnp.stack([compute_lda_xc_energy(d) for d in test_densities])\n",
    "\n",
    "print()\n",
    "print(f\"  Train densities shape: {train_densities.shape}\")\n",
    "print(f\"  Train gradients shape: {train_gradients.shape}\")\n",
    "print(f\"  Train XC reference shape: {train_xc_ref.shape}\")\n",
    "print()\n",
    "print(f\"  Test densities shape: {test_densities.shape}\")\n",
    "print(f\"  Test gradients shape: {test_gradients.shape}\")\n",
    "print(f\"  Test XC reference shape: {test_xc_ref.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e987da93",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 2: Create Neural XC Functional\n",
    "\n",
    "The Neural XC Functional uses:\n",
    "1. **Density Feature Extractor**: Extracts physics-informed features\n",
    "2. **Attention Mechanism**: Captures non-local correlations\n",
    "3. **Physics Constraints**: Ensures negative XC energy and proper scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5395b62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"Creating Neural XC Functional...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "rngs = nnx.Rngs(SEED)\n",
    "\n",
    "model = NeuralXCFunctional(\n",
    "    hidden_sizes=HIDDEN_SIZES,\n",
    "    activation=nnx.gelu,\n",
    "    use_attention=USE_ATTENTION,\n",
    "    num_attention_heads=NUM_ATTENTION_HEADS,\n",
    "    use_advanced_features=USE_ADVANCED_FEATURES,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    rngs=rngs,\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "param_count = sum(p.size for p in jax.tree_util.tree_leaves(nnx.state(model)))\n",
    "print(f\"  Hidden sizes: {HIDDEN_SIZES}\")\n",
    "print(f\"  Use attention: {USE_ATTENTION}\")\n",
    "print(f\"  Attention heads: {NUM_ATTENTION_HEADS}\")\n",
    "print(f\"  Use advanced features: {USE_ADVANCED_FEATURES}\")\n",
    "print(f\"  Total parameters: {param_count:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "test_output = model(train_densities[:1], train_gradients[:1], deterministic=True)\n",
    "print(f\"  Test output shape: {test_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cdc4d5",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 3: Define Loss Function and Training Loop\n",
    "\n",
    "We train the neural XC functional to minimize the mean squared error\n",
    "between predicted and reference LDA XC energies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e49f76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"Setting up training...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "def loss_fn(model, densities, gradients, targets):\n",
    "    \"\"\"Mean squared error loss for XC energy prediction.\"\"\"\n",
    "    predictions = model(densities, gradients, deterministic=True)\n",
    "    return jnp.mean((predictions - targets) ** 2)\n",
    "\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = nnx.Optimizer(model, optax.adam(LEARNING_RATE), wrt=nnx.Param)\n",
    "\n",
    "print(\"  Optimizer: Adam\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Number of epochs: {NUM_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eccfbed",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 4: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dac358e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"Training Neural XC Functional...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "\n",
    "@nnx.jit\n",
    "def train_step(model, optimizer, densities, gradients, targets):\n",
    "    \"\"\"Perform a single training step.\"\"\"\n",
    "    loss, grads = nnx.value_and_grad(loss_fn)(model, densities, gradients, targets)\n",
    "    optimizer.update(model, grads)\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "n_batches = NUM_TRAIN_SAMPLES // BATCH_SIZE\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_losses = []\n",
    "\n",
    "    # Shuffle training data\n",
    "    key, shuffle_key = jax.random.split(key)\n",
    "    perm = jax.random.permutation(shuffle_key, NUM_TRAIN_SAMPLES)\n",
    "    shuffled_densities = train_densities[perm]\n",
    "    shuffled_gradients = train_gradients[perm]\n",
    "    shuffled_targets = train_xc_ref[perm]\n",
    "\n",
    "    # Mini-batch training\n",
    "    for batch_idx in range(n_batches):\n",
    "        start_idx = batch_idx * BATCH_SIZE\n",
    "        end_idx = start_idx + BATCH_SIZE\n",
    "\n",
    "        batch_densities = shuffled_densities[start_idx:end_idx]\n",
    "        batch_gradients = shuffled_gradients[start_idx:end_idx]\n",
    "        batch_targets = shuffled_targets[start_idx:end_idx]\n",
    "\n",
    "        loss = train_step(\n",
    "            model, optimizer, batch_densities, batch_gradients, batch_targets\n",
    "        )\n",
    "        epoch_losses.append(float(loss))\n",
    "\n",
    "    # Record epoch loss\n",
    "    epoch_loss = jnp.mean(jnp.array(epoch_losses))\n",
    "    train_losses.append(float(epoch_loss))\n",
    "\n",
    "    # Compute test loss\n",
    "    test_loss = float(loss_fn(model, test_densities, test_gradients, test_xc_ref))\n",
    "    test_losses.append(test_loss)\n",
    "\n",
    "    # Log progress\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(\n",
    "            f\"  Epoch {epoch + 1:3d}/{NUM_EPOCHS}: \"\n",
    "            f\"train_loss = {epoch_loss:.6f}, test_loss = {test_loss:.6f}\"\n",
    "        )\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print()\n",
    "print(\"Training complete!\")\n",
    "print(f\"  Training time: {training_time:.1f}s\")\n",
    "print(f\"  Final train loss: {train_losses[-1]:.6f}\")\n",
    "print(f\"  Final test loss: {test_losses[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfc4512",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 5: Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d394ecd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"Evaluating model performance...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Get predictions on test set\n",
    "test_predictions = model(test_densities, test_gradients, deterministic=True)\n",
    "\n",
    "# Compute metrics\n",
    "mse = float(jnp.mean((test_predictions - test_xc_ref) ** 2))\n",
    "mae = float(jnp.mean(jnp.abs(test_predictions - test_xc_ref)))\n",
    "r2 = float(\n",
    "    1\n",
    "    - jnp.sum((test_xc_ref - test_predictions) ** 2)\n",
    "    / jnp.sum((test_xc_ref - jnp.mean(test_xc_ref)) ** 2)\n",
    ")\n",
    "\n",
    "# Per-sample correlation\n",
    "correlations = []\n",
    "for i in range(NUM_TEST_SAMPLES):\n",
    "    corr = jnp.corrcoef(test_predictions[i], test_xc_ref[i])[0, 1]\n",
    "    if jnp.isfinite(corr):\n",
    "        correlations.append(float(corr))\n",
    "\n",
    "mean_correlation = jnp.mean(jnp.array(correlations)) if correlations else 0.0\n",
    "\n",
    "print(f\"  Mean Squared Error (MSE): {mse:.6e}\")\n",
    "print(f\"  Mean Absolute Error (MAE): {mae:.6e}\")\n",
    "print(f\"  R-squared (R2): {r2:.4f}\")\n",
    "print(f\"  Mean Correlation: {mean_correlation:.4f}\")\n",
    "\n",
    "# Check physics constraints\n",
    "print()\n",
    "print(\"Physics Constraint Verification:\")\n",
    "all_negative = float(jnp.mean(test_predictions < 0))\n",
    "print(f\"  XC energy negative: {all_negative * 100:.1f}% of predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5911da7",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 6: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc76233",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"Generating visualizations...\")\n",
    "\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470e4b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 1: Training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Left: Training curves\n",
    "ax1 = axes[0]\n",
    "epochs = jnp.arange(1, NUM_EPOCHS + 1)\n",
    "ax1.semilogy(epochs, train_losses, \"b-\", linewidth=2, label=\"Train Loss\")\n",
    "ax1.semilogy(epochs, test_losses, \"r--\", linewidth=2, label=\"Test Loss\")\n",
    "ax1.set_xlabel(\"Epoch\", fontsize=12)\n",
    "ax1.set_ylabel(\"MSE Loss\", fontsize=12)\n",
    "ax1.set_title(\"Training Progress\", fontsize=14)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Prediction vs Reference scatter\n",
    "ax2 = axes[1]\n",
    "ax2.scatter(\n",
    "    test_xc_ref.flatten(),\n",
    "    test_predictions.flatten(),\n",
    "    alpha=0.3,\n",
    "    s=5,\n",
    "    c=\"blue\",\n",
    ")\n",
    "# Perfect prediction line\n",
    "min_val = min(test_xc_ref.min(), test_predictions.min())\n",
    "max_val = max(test_xc_ref.max(), test_predictions.max())\n",
    "ax2.plot([min_val, max_val], [min_val, max_val], \"r--\", linewidth=2, label=\"Perfect\")\n",
    "ax2.set_xlabel(\"Reference XC Energy (LDA)\", fontsize=12)\n",
    "ax2.set_ylabel(\"Predicted XC Energy\", fontsize=12)\n",
    "ax2.set_title(f\"Prediction vs Reference (R2 = {r2:.3f})\", fontsize=14)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/training_curves.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"  Saved: {OUTPUT_DIR}/training_curves.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ccd68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 2: Sample predictions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Select 6 test samples\n",
    "sample_indices = [0, 10, 20, 30, 40, 50]\n",
    "\n",
    "for idx, ax in zip(sample_indices, axes.flatten(), strict=False):\n",
    "    x = jnp.arange(GRID_POINTS)\n",
    "\n",
    "    # Plot density (scaled for visibility)\n",
    "    density_scaled = test_densities[idx] / test_densities[idx].max() * 0.5\n",
    "    ax.fill_between(x, 0, density_scaled, alpha=0.3, color=\"gray\", label=\"Density\")\n",
    "\n",
    "    # Plot XC energies\n",
    "    ax.plot(x, test_xc_ref[idx], \"b-\", linewidth=2, label=\"Reference (LDA)\")\n",
    "    ax.plot(x, test_predictions[idx], \"r--\", linewidth=2, label=\"Predicted\")\n",
    "\n",
    "    # Compute sample correlation\n",
    "    sample_corr = float(jnp.corrcoef(test_predictions[idx], test_xc_ref[idx])[0, 1])\n",
    "\n",
    "    ax.set_xlabel(\"Grid Point\", fontsize=10)\n",
    "    ax.set_ylabel(\"XC Energy\", fontsize=10)\n",
    "    ax.set_title(f\"Sample {idx} (corr = {sample_corr:.3f})\", fontsize=12)\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Sample XC Energy Predictions\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/sample_predictions.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"  Saved: {OUTPUT_DIR}/sample_predictions.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612248d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 3: Error analysis\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Left: Error distribution\n",
    "ax1 = axes[0]\n",
    "errors = (test_predictions - test_xc_ref).flatten()\n",
    "ax1.hist(errors, bins=50, density=True, alpha=0.7, color=\"blue\", edgecolor=\"black\")\n",
    "ax1.axvline(0, color=\"r\", linestyle=\"--\", linewidth=2)\n",
    "ax1.axvline(\n",
    "    jnp.mean(errors),\n",
    "    color=\"g\",\n",
    "    linestyle=\"-\",\n",
    "    linewidth=2,\n",
    "    label=f\"Mean: {jnp.mean(errors):.4f}\",\n",
    ")\n",
    "ax1.set_xlabel(\"Prediction Error\", fontsize=12)\n",
    "ax1.set_ylabel(\"Density\", fontsize=12)\n",
    "ax1.set_title(\"Error Distribution\", fontsize=14)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Right: Error vs density magnitude\n",
    "ax2 = axes[1]\n",
    "density_mag = jnp.abs(test_densities).flatten()\n",
    "errors_flat = jnp.abs(errors)\n",
    "ax2.scatter(density_mag, errors_flat, alpha=0.2, s=3, c=\"blue\")\n",
    "ax2.set_xlabel(\"Density Magnitude\", fontsize=12)\n",
    "ax2.set_ylabel(\"Absolute Error\", fontsize=12)\n",
    "ax2.set_title(\"Error vs Density\", fontsize=14)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/error_analysis.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"  Saved: {OUTPUT_DIR}/error_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e52876",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 7: Assess Chemical Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c02a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"Chemical Accuracy Assessment:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Use built-in assessment method\n",
    "accuracy_metrics = model.assess_chemical_accuracy(\n",
    "    test_densities[:10],\n",
    "    test_gradients[:10],\n",
    "    reference_energy=test_xc_ref[:10],\n",
    "    deterministic=True,\n",
    ")\n",
    "\n",
    "for key, value in accuracy_metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        if abs(value) < 1e-3 or abs(value) > 1e3:\n",
    "            print(f\"  {key}: {value:.6e}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value:.6f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2651dd8",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf277f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"Model Configuration:\")\n",
    "print(f\"  Hidden sizes: {HIDDEN_SIZES}\")\n",
    "print(f\"  Attention: {USE_ATTENTION} ({NUM_ATTENTION_HEADS} heads)\")\n",
    "print(f\"  Advanced features: {USE_ADVANCED_FEATURES}\")\n",
    "print(f\"  Parameters: {param_count:,}\")\n",
    "print()\n",
    "print(\"Training:\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Training samples: {NUM_TRAIN_SAMPLES}\")\n",
    "print(f\"  Training time: {training_time:.1f}s\")\n",
    "print(f\"  Final train loss: {train_losses[-1]:.6e}\")\n",
    "print(f\"  Final test loss: {test_losses[-1]:.6e}\")\n",
    "print()\n",
    "print(\"Evaluation:\")\n",
    "print(f\"  MSE: {mse:.6e}\")\n",
    "print(f\"  MAE: {mae:.6e}\")\n",
    "print(f\"  R-squared: {r2:.4f}\")\n",
    "print(f\"  Mean correlation: {mean_correlation:.4f}\")\n",
    "print()\n",
    "print(\"Physics Constraints:\")\n",
    "print(f\"  Negative XC energy: {all_negative * 100:.1f}%\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e155700",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"Neural XC Functional training example completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "py:percent,ipynb",
   "notebook_metadata_filter": "kernelspec,language_info,-jupytext.text_representation.jupytext_version,-jupytext.notebook_metadata_filter,-jupytext.cell_metadata_filter"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
