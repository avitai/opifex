{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be6e6b9e",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Enhanced Calibration Methods\n",
    "\n",
    "| Property | Value |\n",
    "|---|---|\n",
    "| **Level** | Advanced |\n",
    "| **Runtime** | ~5 min (CPU) |\n",
    "| **Prerequisites** | JAX, Flax NNX, Uncertainty Quantification |\n",
    "\n",
    "## Overview\n",
    "\n",
    "This example demonstrates the advanced uncertainty calibration capabilities\n",
    "implemented in the Opifex framework, including Platt Scaling, Isotonic\n",
    "Regression, Conformal Prediction, and Enhanced Temperature Scaling with\n",
    "physics constraints.\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "1. Apply Platt Scaling for binary classification calibration\n",
    "2. Use Isotonic Regression for non-parametric calibration\n",
    "3. Implement Conformal Prediction for coverage guarantees\n",
    "4. Configure Temperature Scaling with physics constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fd16de",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b54bb7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "\n",
    "from opifex.neural.bayesian import (\n",
    "    CalibrationTools,\n",
    "    ConformalPrediction,\n",
    "    IsotonicRegression,\n",
    "    PlattScaling,\n",
    "    TemperatureScaling,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91745b3",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Synthetic Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577c08c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(key, n_samples=1000):\n",
    "    \"\"\"Generate synthetic data for calibration demonstration.\"\"\"\n",
    "    # Generate features\n",
    "    X = jax.random.normal(key, (n_samples, 2))\n",
    "\n",
    "    # Generate logits with some predictive signal\n",
    "    true_logits = 0.5 * X[:, 0] - 0.3 * X[:, 1] + 0.2 * jnp.sin(X[:, 0])\n",
    "\n",
    "    # Add noise to create miscalibrated predictions\n",
    "    noisy_logits = true_logits + 0.3 * jax.random.normal(key, (n_samples,))\n",
    "\n",
    "    # Generate binary labels based on true logits\n",
    "    probabilities = jax.nn.sigmoid(true_logits)\n",
    "    labels = jax.random.bernoulli(key, probabilities)\n",
    "\n",
    "    # Generate regression targets for conformal prediction demo\n",
    "    regression_targets = true_logits + 0.2 * jax.random.normal(key, (n_samples,))\n",
    "\n",
    "    return X, noisy_logits, labels, regression_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1239c1e",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Platt Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2164aa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_platt_scaling():\n",
    "    \"\"\"Demonstrate Platt scaling for binary classification calibration.\"\"\"\n",
    "    print()\n",
    "    print(\"PLATT SCALING DEMONSTRATION\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Initialize components\n",
    "    key = jax.random.PRNGKey(42)\n",
    "    rngs = nnx.Rngs(42)\n",
    "\n",
    "    # Generate data\n",
    "    _X, logits, labels, _ = generate_synthetic_data(key, n_samples=500)\n",
    "\n",
    "    # Split data for calibration\n",
    "    train_logits, calib_logits = logits[:300], logits[300:]\n",
    "    train_labels, calib_labels = labels[:300], labels[300:]\n",
    "\n",
    "    # Initialize Platt scaling\n",
    "    platt_scaler = PlattScaling(rngs=rngs)\n",
    "\n",
    "    print(\n",
    "        f\"Initial parameters: A={platt_scaler.a.value:.3f}, B={platt_scaler.b.value:.3f}\"\n",
    "    )\n",
    "\n",
    "    # Fit Platt scaling on training data\n",
    "    platt_scaler.fit(train_logits, train_labels, max_iterations=100)\n",
    "\n",
    "    print(\n",
    "        f\"Fitted parameters: A={platt_scaler.a.value:.3f}, B={platt_scaler.b.value:.3f}\"\n",
    "    )\n",
    "\n",
    "    # Apply calibration to validation data\n",
    "    uncalibrated_probs = jax.nn.sigmoid(calib_logits)\n",
    "    calibrated_probs = platt_scaler(calib_logits)\n",
    "\n",
    "    # Compute calibration quality metrics\n",
    "    def expected_calibration_error(probs, labels, n_bins=10):\n",
    "        \"\"\"Compute Expected Calibration Error.\"\"\"\n",
    "        bin_boundaries = jnp.linspace(0, 1, n_bins + 1)\n",
    "        bin_lowers = bin_boundaries[:-1]\n",
    "        bin_uppers = bin_boundaries[1:]\n",
    "\n",
    "        ece = 0\n",
    "        for bin_lower, bin_upper in zip(bin_lowers, bin_uppers, strict=False):\n",
    "            in_bin = (probs > bin_lower) & (probs <= bin_upper)\n",
    "            prop_in_bin = jnp.mean(in_bin)\n",
    "\n",
    "            if prop_in_bin > 0:\n",
    "                accuracy_in_bin = jnp.mean(labels[in_bin])\n",
    "                avg_confidence_in_bin = jnp.mean(probs[in_bin])\n",
    "                ece += jnp.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "\n",
    "        return ece\n",
    "\n",
    "    uncalib_ece = expected_calibration_error(uncalibrated_probs, calib_labels)\n",
    "    calib_ece = expected_calibration_error(calibrated_probs, calib_labels)\n",
    "\n",
    "    print(f\"Uncalibrated ECE: {uncalib_ece:.4f}\")\n",
    "    print(f\"Calibrated ECE: {calib_ece:.4f}\")\n",
    "    print(f\"ECE Improvement: {((uncalib_ece - calib_ece) / uncalib_ece * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b939d3b6",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Isotonic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d44d085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_isotonic_regression():\n",
    "    \"\"\"Demonstrate isotonic regression for non-parametric calibration.\"\"\"\n",
    "    print()\n",
    "    print(\"ISOTONIC REGRESSION DEMONSTRATION\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Initialize components\n",
    "    key = jax.random.PRNGKey(123)\n",
    "    rngs = nnx.Rngs(123)\n",
    "\n",
    "    # Generate data with non-linear calibration needs\n",
    "    _X, logits, labels, _ = generate_synthetic_data(key, n_samples=800)\n",
    "\n",
    "    # Convert logits to confidences with some distortion\n",
    "    raw_confidences = jax.nn.sigmoid(logits)\n",
    "    # Create systematic miscalibration\n",
    "    distorted_confidences = raw_confidences**1.5  # Over-confident predictions\n",
    "\n",
    "    # Split data\n",
    "    train_conf, test_conf = distorted_confidences[:500], distorted_confidences[500:]\n",
    "    train_labels, _test_labels = labels[:500], labels[500:]\n",
    "\n",
    "    # Initialize isotonic regression\n",
    "    isotonic_regressor = IsotonicRegression(n_bins=25, rngs=rngs)\n",
    "\n",
    "    print(f\"Training isotonic regression on {len(train_conf)} samples...\")\n",
    "\n",
    "    # Fit isotonic regression\n",
    "    isotonic_regressor.fit(train_conf, train_labels)\n",
    "\n",
    "    # Apply calibration\n",
    "    calibrated_confidences = isotonic_regressor(test_conf)\n",
    "\n",
    "    # Compute reliability metrics\n",
    "    def reliability_diagram_data(confidences, accuracies, n_bins=10):\n",
    "        \"\"\"Compute reliability diagram data.\"\"\"\n",
    "        bin_boundaries = jnp.linspace(0, 1, n_bins + 1)\n",
    "        bin_centers = (bin_boundaries[:-1] + bin_boundaries[1:]) / 2\n",
    "        bin_confidences = []\n",
    "        bin_accuracies = []\n",
    "\n",
    "        for i in range(n_bins):\n",
    "            in_bin = (confidences >= bin_boundaries[i]) & (\n",
    "                confidences < bin_boundaries[i + 1]\n",
    "            )\n",
    "            if jnp.sum(in_bin) > 0:\n",
    "                bin_conf = jnp.mean(confidences[in_bin])\n",
    "                bin_acc = jnp.mean(accuracies[in_bin])\n",
    "                bin_confidences.append(bin_conf)\n",
    "                bin_accuracies.append(bin_acc)\n",
    "            else:\n",
    "                bin_confidences.append(bin_centers[i])\n",
    "                bin_accuracies.append(bin_centers[i])\n",
    "\n",
    "        return jnp.array(bin_confidences), jnp.array(bin_accuracies)\n",
    "\n",
    "    # Before calibration\n",
    "    before_conf, before_acc = reliability_diagram_data(test_conf, _test_labels)\n",
    "    before_reliability = jnp.mean(jnp.abs(before_conf - before_acc))\n",
    "\n",
    "    # After calibration\n",
    "    after_conf, after_acc = reliability_diagram_data(\n",
    "        calibrated_confidences, _test_labels\n",
    "    )\n",
    "    after_reliability = jnp.mean(jnp.abs(after_conf - after_acc))\n",
    "\n",
    "    print(f\"Average reliability gap before: {before_reliability:.4f}\")\n",
    "    print(f\"Average reliability gap after: {after_reliability:.4f}\")\n",
    "    print(\n",
    "        f\"Reliability improvement: {((before_reliability - after_reliability) / before_reliability * 100):.1f}%\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98970786",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Conformal Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b416bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_conformal_prediction():\n",
    "    \"\"\"Demonstrate conformal prediction for coverage guarantees.\"\"\"\n",
    "    print()\n",
    "    print(\"CONFORMAL PREDICTION DEMONSTRATION\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Initialize components\n",
    "    key = jax.random.PRNGKey(456)\n",
    "    rngs = nnx.Rngs(456)\n",
    "\n",
    "    # Generate regression data\n",
    "    _X, logits, _, targets = generate_synthetic_data(key, n_samples=600)\n",
    "\n",
    "    # Use logits as predictions (with some error)\n",
    "    predictions = logits + 0.1 * jax.random.normal(key, logits.shape)\n",
    "\n",
    "    # Split data for conformal prediction\n",
    "    calib_pred, test_pred = predictions[:300], predictions[300:]\n",
    "    calib_targets, test_targets = targets[:300], targets[300:]\n",
    "\n",
    "    # Test different coverage levels\n",
    "    coverage_levels = [0.80, 0.90, 0.95]\n",
    "\n",
    "    print(\"Testing different coverage levels:\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    for coverage in coverage_levels:\n",
    "        alpha = 1 - coverage\n",
    "\n",
    "        # Initialize conformal predictor\n",
    "        conformal_predictor = ConformalPrediction(alpha=alpha, rngs=rngs)\n",
    "\n",
    "        # Calibrate using calibration set\n",
    "        conformal_predictor.calibrate(calib_pred, calib_targets)\n",
    "\n",
    "        # Generate prediction intervals for test set\n",
    "        lower_bounds, upper_bounds = conformal_predictor.predict_intervals(test_pred)\n",
    "\n",
    "        # Compute empirical coverage\n",
    "        empirical_coverage = conformal_predictor.compute_coverage(\n",
    "            lower_bounds, upper_bounds, test_targets\n",
    "        )\n",
    "\n",
    "        # Compute average interval width\n",
    "        avg_width = jnp.mean(upper_bounds - lower_bounds)\n",
    "\n",
    "        print(f\"Target coverage: {coverage:.0%}\")\n",
    "        print(f\"Empirical coverage: {empirical_coverage:.3f}\")\n",
    "        print(f\"Average interval width: {avg_width:.3f}\")\n",
    "        print(f\"Coverage error: {abs(empirical_coverage - coverage):.3f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36f3927",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Enhanced Temperature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52392b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_enhanced_temperature_scaling():\n",
    "    \"\"\"Demonstrate enhanced temperature scaling with adaptive features.\"\"\"\n",
    "    print()\n",
    "    print(\"ENHANCED TEMPERATURE SCALING DEMONSTRATION\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Initialize components\n",
    "    key = jax.random.PRNGKey(789)\n",
    "    rngs = nnx.Rngs(789)\n",
    "\n",
    "    # Generate data\n",
    "    X, logits, labels, targets = generate_synthetic_data(key, n_samples=400)\n",
    "\n",
    "    # Split data\n",
    "    test_X = X[200:]\n",
    "    train_logits, test_logits = logits[:200], logits[200:]\n",
    "    _train_labels, _test_labels = labels[:200], labels[200:]\n",
    "    _test_targets = targets[200:]\n",
    "\n",
    "    # Initialize enhanced temperature scaling\n",
    "    temp_scaler = TemperatureScaling(\n",
    "        physics_constraints=[\"energy_conservation\"],\n",
    "        adaptive=True,\n",
    "        learning_rate=0.02,\n",
    "        rngs=rngs,\n",
    "    )\n",
    "\n",
    "    print(f\"Initial temperature: {temp_scaler.temperature.value:.3f}\")\n",
    "\n",
    "    # Optimize temperature\n",
    "    optimized_temp = temp_scaler.optimize_temperature(\n",
    "        train_logits, _train_labels.astype(int)\n",
    "    )\n",
    "\n",
    "    print(f\"Optimized temperature: {optimized_temp:.3f}\")\n",
    "\n",
    "    # Apply temperature scaling to test data\n",
    "    calibrated_preds, aleatoric_uncertainty = temp_scaler(test_logits[:, None], test_X)\n",
    "    calibrated_preds = calibrated_preds.squeeze()\n",
    "    aleatoric_uncertainty = aleatoric_uncertainty.squeeze()\n",
    "\n",
    "    # Test adaptive temperature scaling\n",
    "    adaptive_temps = temp_scaler.adaptive_temperature_scaling(\n",
    "        test_logits, aleatoric_uncertainty, _test_targets\n",
    "    )\n",
    "\n",
    "    print(f\"Average adaptive temperature: {jnp.mean(adaptive_temps):.3f}\")\n",
    "    print(f\"Temperature std: {jnp.std(adaptive_temps):.3f}\")\n",
    "    print(f\"Average aleatoric uncertainty: {jnp.mean(aleatoric_uncertainty):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2269e9dc",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Integrated Calibration Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2286dcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_integrated_calibration_pipeline():\n",
    "    \"\"\"Demonstrate integrated use of multiple calibration methods.\"\"\"\n",
    "    print()\n",
    "    print(\"INTEGRATED CALIBRATION PIPELINE DEMONSTRATION\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Initialize components\n",
    "    key = jax.random.PRNGKey(999)\n",
    "    rngs = nnx.Rngs(999)\n",
    "\n",
    "    # Generate comprehensive dataset\n",
    "    X, logits, labels, targets = generate_synthetic_data(key, n_samples=1000)\n",
    "\n",
    "    # Split data into train/calib/test\n",
    "    _train_X, _calib_X, test_X = X[:400], X[400:700], X[700:]\n",
    "    train_logits, calib_logits, test_logits = (\n",
    "        logits[:400],\n",
    "        logits[400:700],\n",
    "        logits[700:],\n",
    "    )\n",
    "    train_labels, calib_labels, test_labels = (\n",
    "        labels[:400],\n",
    "        labels[400:700],\n",
    "        labels[700:],\n",
    "    )\n",
    "    train_targets, calib_targets, test_targets = (\n",
    "        targets[:400],\n",
    "        targets[400:700],\n",
    "        targets[700:],\n",
    "    )\n",
    "\n",
    "    print(\"Setting up integrated calibration pipeline...\")\n",
    "\n",
    "    # 1. Enhanced CalibrationTools for assessment\n",
    "    calibration_tools = CalibrationTools(rngs=rngs)\n",
    "\n",
    "    # Convert logits to predictions for regression assessment\n",
    "    regression_preds = train_logits + 0.1 * jax.random.normal(key, train_logits.shape)\n",
    "    uncertainties = jnp.abs(0.2 * jax.random.normal(key, train_logits.shape)) + 0.1\n",
    "\n",
    "    initial_metrics = calibration_tools.assess_calibration(\n",
    "        regression_preds, uncertainties, train_targets, num_bins=10\n",
    "    )\n",
    "\n",
    "    print(f\"Initial ECE: {initial_metrics['expected_calibration_error']:.4f}\")\n",
    "    print(f\"Initial MCE: {initial_metrics['maximum_calibration_error']:.4f}\")\n",
    "\n",
    "    # 2. Apply Platt scaling for classification\n",
    "    platt_scaler = PlattScaling(rngs=rngs)\n",
    "    platt_scaler.fit(train_logits, train_labels)\n",
    "    calib_class_probs = platt_scaler(calib_logits)\n",
    "\n",
    "    # 3. Apply isotonic regression for further refinement\n",
    "    isotonic_regressor = IsotonicRegression(n_bins=20, rngs=rngs)\n",
    "    isotonic_regressor.fit(calib_class_probs, calib_labels)\n",
    "    _refined_probs = isotonic_regressor(calib_class_probs)\n",
    "\n",
    "    # 4. Apply conformal prediction for regression\n",
    "    conformal_predictor = ConformalPrediction(alpha=0.1, rngs=rngs)\n",
    "    conformal_predictor.calibrate(calib_logits, calib_targets)\n",
    "\n",
    "    # 5. Enhanced temperature scaling\n",
    "    temp_scaler = TemperatureScaling(\n",
    "        physics_constraints=[\"energy_conservation\"], adaptive=True, rngs=rngs\n",
    "    )\n",
    "    temp_scaler.optimize_temperature(train_logits, train_labels.astype(int))\n",
    "\n",
    "    # Apply full pipeline to test data\n",
    "    print()\n",
    "    print(\"Applying full calibration pipeline to test data...\")\n",
    "\n",
    "    # Classification pipeline\n",
    "    test_class_probs = platt_scaler(test_logits)\n",
    "    test_refined_probs = isotonic_regressor(test_class_probs)\n",
    "\n",
    "    # Regression pipeline\n",
    "    test_lower, test_upper = conformal_predictor.predict_intervals(test_logits)\n",
    "    _test_calibrated, test_uncertainty = temp_scaler(test_logits[:, None], test_X)\n",
    "\n",
    "    # Final assessment\n",
    "    final_coverage = conformal_predictor.compute_coverage(\n",
    "        test_lower, test_upper, test_targets\n",
    "    )\n",
    "\n",
    "    # Classification calibration assessment\n",
    "    def compute_ece(probs, labels):\n",
    "        \"\"\"Simple ECE computation.\"\"\"\n",
    "        accuracies = (probs > 0.5) == (labels > 0.5)\n",
    "        return jnp.mean(jnp.abs(probs - accuracies))\n",
    "\n",
    "    original_ece = compute_ece(jax.nn.sigmoid(test_logits), test_labels)\n",
    "    refined_ece = compute_ece(test_refined_probs, test_labels)\n",
    "\n",
    "    print()\n",
    "    print(\"Final Results:\")\n",
    "    print(f\"Classification ECE improvement: {original_ece:.4f} -> {refined_ece:.4f}\")\n",
    "    print(f\"Regression coverage: {final_coverage:.3f} (target: 0.900)\")\n",
    "    print(f\"Average uncertainty: {jnp.mean(test_uncertainty.squeeze()):.3f}\")\n",
    "    print(\"Successful integration of all calibration methods!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7099c0a1",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Results Summary + Next Steps\n",
    "\n",
    "After running this demo you should observe:\n",
    "\n",
    "- **Platt Scaling** reduces the Expected Calibration Error (ECE) for binary\n",
    "  classification by fitting a sigmoid transformation to raw logits\n",
    "- **Isotonic Regression** provides non-parametric calibration that handles\n",
    "  arbitrary miscalibration shapes\n",
    "- **Conformal Prediction** delivers finite-sample coverage guarantees at\n",
    "  user-specified confidence levels (80%, 90%, 95%)\n",
    "- **Temperature Scaling** adapts per-sample temperatures using physics\n",
    "  constraints for scientific applications\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Combine calibration methods with UQNO for end-to-end uncertainty pipelines\n",
    "- Apply conformal prediction to PDE solver outputs for reliability bounds\n",
    "- Use adaptive temperature scaling in physics-informed training loops\n",
    "- Benchmark calibration quality on real scientific datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7044a788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Run all calibration demonstrations.\"\"\"\n",
    "    print(\"Opifex Enhanced Calibration Methods Demonstration\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    print(\"This demo showcases the advanced uncertainty calibration capabilities\")\n",
    "    print(\"implemented in the Opifex framework, providing advanced\")\n",
    "    print(\"calibration methods for scientific machine learning applications.\")\n",
    "\n",
    "    try:\n",
    "        # Run individual method demonstrations\n",
    "        demonstrate_platt_scaling()\n",
    "        demonstrate_isotonic_regression()\n",
    "        demonstrate_conformal_prediction()\n",
    "        demonstrate_enhanced_temperature_scaling()\n",
    "\n",
    "        # Run integrated pipeline demonstration\n",
    "        demonstrate_integrated_calibration_pipeline()\n",
    "\n",
    "        print()\n",
    "        print(\"ALL DEMONSTRATIONS COMPLETED SUCCESSFULLY!\")\n",
    "        print()\n",
    "        print(\"=\" * 60)\n",
    "        print()\n",
    "        print(\"Key achievements demonstrated:\")\n",
    "        print(\"  Platt Scaling: Parametric binary classification calibration\")\n",
    "        print(\"  Isotonic Regression: Non-parametric monotonic calibration\")\n",
    "        print(\"  Conformal Prediction: Finite-sample coverage guarantees\")\n",
    "        print(\"  Enhanced Temperature Scaling: Adaptive physics-aware calibration\")\n",
    "        print(\"  Integrated Pipeline: Seamless combination of all methods\")\n",
    "        print()\n",
    "        print(\"The Opifex framework now provides enterprise-grade uncertainty\")\n",
    "        print(\"calibration capabilities for scientific computing applications!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print()\n",
    "        print(f\"Error during demonstration: {e}\")\n",
    "        print(\"Please check the implementation and try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669d649e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "py:percent,ipynb",
   "main_language": "python",
   "notebook_metadata_filter": "kernelspec,language_info,-jupytext.text_representation.jupytext_version,-jupytext.notebook_metadata_filter,-jupytext.cell_metadata_filter"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
