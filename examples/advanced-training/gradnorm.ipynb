{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bae61b5",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# GradNorm: Automatic Loss Balancing for PINNs\n",
    "\n",
    "This example demonstrates how to use GradNorm for automatic loss weight\n",
    "balancing in multi-objective PINN training. GradNorm dynamically adjusts\n",
    "weights to equalize gradient contributions across loss components.\n",
    "\n",
    "**Key Concepts:**\n",
    "- Multi-objective PINN training (PDE + BC + IC losses)\n",
    "- Gradient magnitude monitoring\n",
    "- Automatic weight adaptation via GradNorm\n",
    "- Training rate balancing across loss components\n",
    "\n",
    "**SciML Context:**\n",
    "PINNs with multiple loss terms (PDE residual, boundary conditions, initial\n",
    "conditions) often suffer from gradient imbalance - one loss dominates and\n",
    "prevents others from decreasing. GradNorm solves this automatically.\n",
    "\n",
    "**Key Result:**\n",
    "GradNorm achieves balanced loss reduction across all components, avoiding\n",
    "the common failure mode of boundary/initial conditions being poorly satisfied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac665b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "SEED = 42\n",
    "N_COLLOCATION = 500\n",
    "N_BOUNDARY = 100\n",
    "N_INITIAL = 100\n",
    "LEARNING_RATE = 1e-3\n",
    "TRAINING_STEPS = 1000\n",
    "GRADNORM_ALPHA = 1.5  # Asymmetry parameter (0 = equal, higher = more balancing)\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = \"docs/assets/examples/gradnorm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dbb7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"Opifex Example: GradNorm Loss Balancing for PINNs\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702b596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optax\n",
    "from flax import nnx\n",
    "\n",
    "\n",
    "print(f\"JAX backend: {jax.default_backend()}\")\n",
    "print(f\"JAX devices: {jax.devices()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1192a00",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from opifex.core.physics.gradnorm import (\n",
    "    compute_gradient_norms,\n",
    "    GradNormBalancer,\n",
    "    GradNormConfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6577bd",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 1: Define the Problem\n",
    "\n",
    "We solve the heat equation with Dirichlet boundary conditions:\n",
    "    u_t = alpha * u_xx on [0, 1] x [0, T]\n",
    "    u(x, 0) = sin(pi*x)\n",
    "    u(0, t) = u(1, t) = 0\n",
    "\n",
    "Exact solution: u(x, t) = sin(pi*x) * exp(-pi^2*alpha*t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5136538",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeatEquationPINN(nnx.Module):\n",
    "    \"\"\"PINN for 1D heat equation.\"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dims: list[int] | None = None, *, rngs: nnx.Rngs):\n",
    "        super().__init__()\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [32, 32, 32]\n",
    "        layers = []\n",
    "        in_dim = 2  # x, t\n",
    "\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nnx.Linear(in_dim, hidden_dim, rngs=rngs))\n",
    "            in_dim = hidden_dim\n",
    "\n",
    "        layers.append(nnx.Linear(in_dim, 1, rngs=rngs))\n",
    "        self.layers = nnx.List(layers)\n",
    "\n",
    "    def __call__(self, xt: jax.Array) -> jax.Array:\n",
    "        \"\"\"Forward pass through the PINN.\"\"\"\n",
    "        h = xt\n",
    "        for layer in self.layers[:-1]:\n",
    "            h = jnp.tanh(layer(h))\n",
    "        return self.layers[-1](h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0b6af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"Creating PINN model...\")\n",
    "\n",
    "pinn = HeatEquationPINN(hidden_dims=[32, 32, 32], rngs=nnx.Rngs(SEED))\n",
    "n_params = sum(x.size for x in jax.tree_util.tree_leaves(nnx.state(pinn, nnx.Param)))\n",
    "print(\"  Architecture: [2] -> [32] -> [32] -> [32] -> [1]\")\n",
    "print(f\"  Parameters: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcea124f",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 2: Generate Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7919e4b0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"Generating training data...\")\n",
    "\n",
    "key = jax.random.PRNGKey(SEED)\n",
    "ALPHA = 0.01  # Thermal diffusivity\n",
    "T_MAX = 0.5\n",
    "\n",
    "# Domain points for PDE residual\n",
    "key, subkey = jax.random.split(key)\n",
    "x_domain = jax.random.uniform(subkey, (N_COLLOCATION, 1), minval=0.0, maxval=1.0)\n",
    "key, subkey = jax.random.split(key)\n",
    "t_domain = jax.random.uniform(subkey, (N_COLLOCATION, 1), minval=0.0, maxval=T_MAX)\n",
    "xt_domain = jnp.concatenate([x_domain, t_domain], axis=1)\n",
    "\n",
    "# Boundary points (x=0 and x=1)\n",
    "key, subkey = jax.random.split(key)\n",
    "t_boundary = jax.random.uniform(subkey, (N_BOUNDARY, 1), minval=0.0, maxval=T_MAX)\n",
    "xt_left = jnp.concatenate([jnp.zeros((N_BOUNDARY, 1)), t_boundary], axis=1)\n",
    "xt_right = jnp.concatenate([jnp.ones((N_BOUNDARY, 1)), t_boundary], axis=1)\n",
    "xt_boundary = jnp.concatenate([xt_left, xt_right], axis=0)\n",
    "\n",
    "# Initial condition points (t=0)\n",
    "key, subkey = jax.random.split(key)\n",
    "x_initial = jax.random.uniform(subkey, (N_INITIAL, 1), minval=0.0, maxval=1.0)\n",
    "xt_initial = jnp.concatenate([x_initial, jnp.zeros((N_INITIAL, 1))], axis=1)\n",
    "u_initial = jnp.sin(jnp.pi * x_initial)  # sin(πx) at t=0\n",
    "\n",
    "print(f\"  Domain points: {xt_domain.shape}\")\n",
    "print(f\"  Boundary points: {xt_boundary.shape}\")\n",
    "print(f\"  Initial points: {xt_initial.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5f4241",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 3: Define Loss Functions\n",
    "\n",
    "We have three loss components:\n",
    "1. PDE residual: u_t - alpha * u_xx = 0\n",
    "2. Boundary condition: u(0, t) = u(1, t) = 0\n",
    "3. Initial condition: u(x, 0) = sin(pi*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e521354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pde_residual(pinn, xt_domain, alpha):\n",
    "    \"\"\"Compute heat equation residual: u_t - α * u_xx = 0.\"\"\"\n",
    "\n",
    "    def u_scalar(xt_single):\n",
    "        return pinn(xt_single.reshape(1, 2)).squeeze()\n",
    "\n",
    "    def residual_single(xt_single):\n",
    "        # First derivatives\n",
    "        du = jax.grad(u_scalar)(xt_single)\n",
    "        du_dt = du[1]\n",
    "\n",
    "        # Second derivative in x\n",
    "        def du_dx_fn(xt):\n",
    "            return jax.grad(u_scalar)(xt)[0]\n",
    "\n",
    "        d2u_dx2 = jax.grad(du_dx_fn)(xt_single)[0]\n",
    "\n",
    "        # PDE: u_t - alpha * u_xx = 0\n",
    "        return du_dt - alpha * d2u_dx2\n",
    "\n",
    "    return jax.vmap(residual_single)(xt_domain)\n",
    "\n",
    "\n",
    "def pde_loss_fn(pinn):\n",
    "    \"\"\"PDE residual loss.\"\"\"\n",
    "    residual = compute_pde_residual(pinn, xt_domain, ALPHA)\n",
    "    return jnp.mean(residual**2)\n",
    "\n",
    "\n",
    "def bc_loss_fn(pinn):\n",
    "    \"\"\"Boundary condition loss: u(0, t) = u(1, t) = 0.\"\"\"\n",
    "    u_bc = pinn(xt_boundary).squeeze()\n",
    "    return jnp.mean(u_bc**2)\n",
    "\n",
    "\n",
    "def ic_loss_fn(pinn):\n",
    "    \"\"\"Initial condition loss: u(x, 0) = sin(πx).\"\"\"\n",
    "    u_ic = pinn(xt_initial).squeeze()\n",
    "    u_target = u_initial.squeeze()\n",
    "    return jnp.mean((u_ic - u_target) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a338a3a",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 4: Setup GradNorm Balancer\n",
    "\n",
    "GradNorm automatically balances the three loss components based on their\n",
    "gradient magnitudes and training rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923c2eaf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"Setting up GradNorm balancer...\")\n",
    "\n",
    "config = GradNormConfig(\n",
    "    alpha=GRADNORM_ALPHA,  # Asymmetry parameter\n",
    "    learning_rate=0.01,  # Learning rate for weight updates\n",
    "    update_frequency=1,  # Update weights every step\n",
    ")\n",
    "\n",
    "balancer = GradNormBalancer(\n",
    "    num_losses=3,  # PDE, BC, IC\n",
    "    config=config,\n",
    "    rngs=nnx.Rngs(SEED),\n",
    ")\n",
    "\n",
    "print(f\"  GradNorm alpha: {config.alpha}\")\n",
    "print(f\"  Weight learning rate: {config.learning_rate}\")\n",
    "print(f\"  Initial weights: {balancer.weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d26487",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 5: Train with GradNorm\n",
    "\n",
    "We train the PINN with automatic loss weight balancing and compare to fixed weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5c86e8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"Training PINN with GradNorm...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create optimizer\n",
    "opt = nnx.Optimizer(pinn, optax.adam(LEARNING_RATE), wrt=nnx.Param)\n",
    "\n",
    "# Loss functions as list\n",
    "loss_fns = [pde_loss_fn, bc_loss_fn, ic_loss_fn]\n",
    "loss_names = [\"PDE\", \"BC\", \"IC\"]\n",
    "\n",
    "# Compute initial losses\n",
    "initial_losses = jnp.array([fn(pinn) for fn in loss_fns])\n",
    "balancer.set_initial_losses(initial_losses)\n",
    "print(\n",
    "    f\"Initial losses: PDE={float(initial_losses[0]):.4f}, \"\n",
    "    f\"BC={float(initial_losses[1]):.4f}, IC={float(initial_losses[2]):.4f}\"\n",
    ")\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    \"step\": [],\n",
    "    \"total_loss\": [],\n",
    "    \"pde_loss\": [],\n",
    "    \"bc_loss\": [],\n",
    "    \"ic_loss\": [],\n",
    "    \"weight_pde\": [],\n",
    "    \"weight_bc\": [],\n",
    "    \"weight_ic\": [],\n",
    "}\n",
    "\n",
    "\n",
    "@nnx.jit\n",
    "def compute_losses(pinn):\n",
    "    \"\"\"Compute all individual losses.\"\"\"\n",
    "    return jnp.array([pde_loss_fn(pinn), bc_loss_fn(pinn), ic_loss_fn(pinn)])\n",
    "\n",
    "\n",
    "def train_step_gradnorm(pinn, opt, balancer):\n",
    "    \"\"\"Training step with GradNorm.\"\"\"\n",
    "    # Compute individual losses\n",
    "    losses = compute_losses(pinn)\n",
    "\n",
    "    # Compute gradient norms\n",
    "    grad_norms = compute_gradient_norms(pinn, loss_fns)\n",
    "\n",
    "    # Update GradNorm weights\n",
    "    initial = balancer.get_initial_losses()\n",
    "    if initial is not None:\n",
    "        balancer.update_weights(grad_norms, losses, initial)\n",
    "\n",
    "    # Compute weighted loss and gradients\n",
    "    def total_loss_fn(model):\n",
    "        ls = jnp.array([pde_loss_fn(model), bc_loss_fn(model), ic_loss_fn(model)])\n",
    "        return balancer.compute_weighted_loss(ls)\n",
    "\n",
    "    total_loss, grads = nnx.value_and_grad(total_loss_fn)(pinn)\n",
    "    opt.update(pinn, grads)\n",
    "\n",
    "    return total_loss, losses\n",
    "\n",
    "\n",
    "for step in range(TRAINING_STEPS):\n",
    "    total_loss, losses = train_step_gradnorm(pinn, opt, balancer)\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        weights = balancer.weights\n",
    "        history[\"step\"].append(step)\n",
    "        history[\"total_loss\"].append(float(total_loss))\n",
    "        history[\"pde_loss\"].append(float(losses[0]))\n",
    "        history[\"bc_loss\"].append(float(losses[1]))\n",
    "        history[\"ic_loss\"].append(float(losses[2]))\n",
    "        history[\"weight_pde\"].append(float(weights[0]))\n",
    "        history[\"weight_bc\"].append(float(weights[1]))\n",
    "        history[\"weight_ic\"].append(float(weights[2]))\n",
    "\n",
    "        print(\n",
    "            f\"  Step {step:4d}: loss={total_loss:.6e}, \"\n",
    "            f\"PDE={losses[0]:.4e}, BC={losses[1]:.4e}, IC={losses[2]:.4e}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"           weights: PDE={weights[0]:.3f}, \"\n",
    "            f\"BC={weights[1]:.3f}, IC={weights[2]:.3f}\"\n",
    "        )\n",
    "\n",
    "# Final step\n",
    "weights = balancer.weights\n",
    "history[\"step\"].append(TRAINING_STEPS)\n",
    "history[\"total_loss\"].append(float(total_loss))\n",
    "history[\"pde_loss\"].append(float(losses[0]))\n",
    "history[\"bc_loss\"].append(float(losses[1]))\n",
    "history[\"ic_loss\"].append(float(losses[2]))\n",
    "history[\"weight_pde\"].append(float(weights[0]))\n",
    "history[\"weight_bc\"].append(float(weights[1]))\n",
    "history[\"weight_ic\"].append(float(weights[2]))\n",
    "\n",
    "print(\n",
    "    f\"  Step {TRAINING_STEPS:4d}: loss={total_loss:.6e}, \"\n",
    "    f\"PDE={losses[0]:.4e}, BC={losses[1]:.4e}, IC={losses[2]:.4e}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153a8c85",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 6: Train with Fixed Weights (Baseline)\n",
    "\n",
    "For comparison, we train another PINN with fixed equal weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eddffe",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"Training PINN with fixed weights (baseline)...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create fresh model\n",
    "pinn_fixed = HeatEquationPINN(hidden_dims=[32, 32, 32], rngs=nnx.Rngs(SEED))\n",
    "opt_fixed = nnx.Optimizer(pinn_fixed, optax.adam(LEARNING_RATE), wrt=nnx.Param)\n",
    "\n",
    "fixed_history = {\n",
    "    \"step\": [],\n",
    "    \"total_loss\": [],\n",
    "    \"pde_loss\": [],\n",
    "    \"bc_loss\": [],\n",
    "    \"ic_loss\": [],\n",
    "}\n",
    "\n",
    "FIXED_WEIGHT = 1.0  # Equal weights for all\n",
    "\n",
    "\n",
    "def make_fixed_pde_loss():\n",
    "    \"\"\"Create PDE loss function for fixed weights baseline.\"\"\"\n",
    "\n",
    "    def fn(model):\n",
    "        residual = compute_pde_residual(model, xt_domain, ALPHA)\n",
    "        return jnp.mean(residual**2)\n",
    "\n",
    "    return fn\n",
    "\n",
    "\n",
    "def make_fixed_bc_loss():\n",
    "    \"\"\"Create BC loss function for fixed weights baseline.\"\"\"\n",
    "\n",
    "    def fn(model):\n",
    "        u_bc = model(xt_boundary).squeeze()\n",
    "        return jnp.mean(u_bc**2)\n",
    "\n",
    "    return fn\n",
    "\n",
    "\n",
    "def make_fixed_ic_loss():\n",
    "    \"\"\"Create IC loss function for fixed weights baseline.\"\"\"\n",
    "\n",
    "    def fn(model):\n",
    "        u_ic = model(xt_initial).squeeze()\n",
    "        u_target = u_initial.squeeze()\n",
    "        return jnp.mean((u_ic - u_target) ** 2)\n",
    "\n",
    "    return fn\n",
    "\n",
    "\n",
    "@nnx.jit\n",
    "def train_step_fixed(pinn, opt):\n",
    "    \"\"\"Training step with fixed weights.\"\"\"\n",
    "\n",
    "    def total_loss_fn(model):\n",
    "        pde = compute_pde_residual(model, xt_domain, ALPHA)\n",
    "        pde_loss = jnp.mean(pde**2)\n",
    "        bc_loss = jnp.mean(model(xt_boundary).squeeze() ** 2)\n",
    "        ic_loss = jnp.mean((model(xt_initial).squeeze() - u_initial.squeeze()) ** 2)\n",
    "        return FIXED_WEIGHT * (pde_loss + bc_loss + ic_loss), (\n",
    "            pde_loss,\n",
    "            bc_loss,\n",
    "            ic_loss,\n",
    "        )\n",
    "\n",
    "    (total_loss, losses), grads = nnx.value_and_grad(total_loss_fn, has_aux=True)(pinn)\n",
    "    opt.update(pinn, grads)\n",
    "\n",
    "    return total_loss, losses\n",
    "\n",
    "\n",
    "for step in range(TRAINING_STEPS):\n",
    "    total_loss, losses = train_step_fixed(pinn_fixed, opt_fixed)\n",
    "    pde_l, bc_l, ic_l = float(losses[0]), float(losses[1]), float(losses[2])\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        fixed_history[\"step\"].append(step)\n",
    "        fixed_history[\"total_loss\"].append(float(total_loss))\n",
    "        fixed_history[\"pde_loss\"].append(pde_l)\n",
    "        fixed_history[\"bc_loss\"].append(bc_l)\n",
    "        fixed_history[\"ic_loss\"].append(ic_l)\n",
    "\n",
    "        print(\n",
    "            f\"  Step {step:4d}: loss={total_loss:.6e}, \"\n",
    "            f\"PDE={pde_l:.4e}, BC={bc_l:.4e}, IC={ic_l:.4e}\"\n",
    "        )\n",
    "\n",
    "# Final step\n",
    "fixed_history[\"step\"].append(TRAINING_STEPS)\n",
    "fixed_history[\"total_loss\"].append(float(total_loss))\n",
    "fixed_history[\"pde_loss\"].append(pde_l)\n",
    "fixed_history[\"bc_loss\"].append(bc_l)\n",
    "fixed_history[\"ic_loss\"].append(ic_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b1b8e3",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 7: Evaluate Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63f27b5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"Evaluating solutions...\")\n",
    "\n",
    "# Evaluation grid\n",
    "x_eval = jnp.linspace(0, 1, 50)\n",
    "t_eval = jnp.linspace(0, T_MAX, 50)\n",
    "X, T = jnp.meshgrid(x_eval, t_eval)\n",
    "xt_eval = jnp.stack([X.ravel(), T.ravel()], axis=1)\n",
    "\n",
    "# Exact solution\n",
    "U_exact = jnp.sin(jnp.pi * X) * jnp.exp(-(jnp.pi**2) * ALPHA * T)\n",
    "\n",
    "# GradNorm solution\n",
    "U_gradnorm = pinn(xt_eval).squeeze().reshape(50, 50)\n",
    "l2_gradnorm = float(jnp.sqrt(jnp.mean((U_gradnorm - U_exact) ** 2)))\n",
    "\n",
    "# Fixed weights solution\n",
    "U_fixed = pinn_fixed(xt_eval).squeeze().reshape(50, 50)\n",
    "l2_fixed = float(jnp.sqrt(jnp.mean((U_fixed - U_exact) ** 2)))\n",
    "\n",
    "print(f\"  GradNorm L2 error: {l2_gradnorm:.6e}\")\n",
    "print(f\"  Fixed weights L2 error: {l2_fixed:.6e}\")\n",
    "print(f\"  Improvement: {(l2_fixed - l2_gradnorm) / l2_fixed * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501fe6f0",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 8: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a8ec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"Generating visualizations...\")\n",
    "\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "mpl.use(\"Agg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35dd52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 1: Loss comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Total loss\n",
    "ax1 = axes[0, 0]\n",
    "ax1.semilogy(\n",
    "    history[\"step\"], history[\"total_loss\"], \"b-\", label=\"GradNorm\", linewidth=2\n",
    ")\n",
    "ax1.semilogy(\n",
    "    fixed_history[\"step\"],\n",
    "    fixed_history[\"total_loss\"],\n",
    "    \"r--\",\n",
    "    label=\"Fixed\",\n",
    "    linewidth=2,\n",
    ")\n",
    "ax1.set_xlabel(\"Training Step\", fontsize=12)\n",
    "ax1.set_ylabel(\"Total Loss (log scale)\", fontsize=12)\n",
    "ax1.set_title(\"Total Loss Comparison\", fontsize=14)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Individual losses - GradNorm\n",
    "ax2 = axes[0, 1]\n",
    "ax2.semilogy(history[\"step\"], history[\"pde_loss\"], \"b-\", label=\"PDE\", linewidth=2)\n",
    "ax2.semilogy(history[\"step\"], history[\"bc_loss\"], \"g-\", label=\"BC\", linewidth=2)\n",
    "ax2.semilogy(history[\"step\"], history[\"ic_loss\"], \"r-\", label=\"IC\", linewidth=2)\n",
    "ax2.set_xlabel(\"Training Step\", fontsize=12)\n",
    "ax2.set_ylabel(\"Loss (log scale)\", fontsize=12)\n",
    "ax2.set_title(\"GradNorm: Individual Losses\", fontsize=14)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Individual losses - Fixed weights\n",
    "ax3 = axes[1, 0]\n",
    "ax3.semilogy(\n",
    "    fixed_history[\"step\"], fixed_history[\"pde_loss\"], \"b-\", label=\"PDE\", linewidth=2\n",
    ")\n",
    "ax3.semilogy(\n",
    "    fixed_history[\"step\"], fixed_history[\"bc_loss\"], \"g-\", label=\"BC\", linewidth=2\n",
    ")\n",
    "ax3.semilogy(\n",
    "    fixed_history[\"step\"], fixed_history[\"ic_loss\"], \"r-\", label=\"IC\", linewidth=2\n",
    ")\n",
    "ax3.set_xlabel(\"Training Step\", fontsize=12)\n",
    "ax3.set_ylabel(\"Loss (log scale)\", fontsize=12)\n",
    "ax3.set_title(\"Fixed Weights: Individual Losses\", fontsize=14)\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Weight evolution\n",
    "ax4 = axes[1, 1]\n",
    "ax4.plot(history[\"step\"], history[\"weight_pde\"], \"b-\", label=\"w_PDE\", linewidth=2)\n",
    "ax4.plot(history[\"step\"], history[\"weight_bc\"], \"g-\", label=\"w_BC\", linewidth=2)\n",
    "ax4.plot(history[\"step\"], history[\"weight_ic\"], \"r-\", label=\"w_IC\", linewidth=2)\n",
    "ax4.set_xlabel(\"Training Step\", fontsize=12)\n",
    "ax4.set_ylabel(\"Weight\", fontsize=12)\n",
    "ax4.set_title(\"GradNorm Weight Evolution\", fontsize=14)\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/training_comparison.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "print(f\"  Saved: {OUTPUT_DIR}/training_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7bf8a5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Figure 2: Solution comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Exact solution\n",
    "ax1 = axes[0]\n",
    "im1 = ax1.imshow(\n",
    "    np.array(U_exact),\n",
    "    extent=[0, 1, 0, T_MAX],\n",
    "    origin=\"lower\",\n",
    "    aspect=\"auto\",\n",
    "    cmap=\"viridis\",\n",
    ")\n",
    "ax1.set_xlabel(\"x\", fontsize=12)\n",
    "ax1.set_ylabel(\"t\", fontsize=12)\n",
    "ax1.set_title(\"Exact Solution\", fontsize=14)\n",
    "plt.colorbar(im1, ax=ax1)\n",
    "\n",
    "# GradNorm solution\n",
    "ax2 = axes[1]\n",
    "im2 = ax2.imshow(\n",
    "    np.array(U_gradnorm),\n",
    "    extent=[0, 1, 0, T_MAX],\n",
    "    origin=\"lower\",\n",
    "    aspect=\"auto\",\n",
    "    cmap=\"viridis\",\n",
    ")\n",
    "ax2.set_xlabel(\"x\", fontsize=12)\n",
    "ax2.set_ylabel(\"t\", fontsize=12)\n",
    "ax2.set_title(f\"GradNorm (L2={l2_gradnorm:.2e})\", fontsize=14)\n",
    "plt.colorbar(im2, ax=ax2)\n",
    "\n",
    "# Error\n",
    "ax3 = axes[2]\n",
    "error = np.abs(np.array(U_gradnorm - U_exact))\n",
    "im3 = ax3.imshow(\n",
    "    error, extent=[0, 1, 0, T_MAX], origin=\"lower\", aspect=\"auto\", cmap=\"hot\"\n",
    ")\n",
    "ax3.set_xlabel(\"x\", fontsize=12)\n",
    "ax3.set_ylabel(\"t\", fontsize=12)\n",
    "ax3.set_title(\"Absolute Error\", fontsize=14)\n",
    "plt.colorbar(im3, ax=ax3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/solution.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "print(f\"  Saved: {OUTPUT_DIR}/solution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edd7112",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cb3b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"Final Losses:\")\n",
    "print(\n",
    "    f\"  GradNorm: PDE={history['pde_loss'][-1]:.4e}, \"\n",
    "    f\"BC={history['bc_loss'][-1]:.4e}, IC={history['ic_loss'][-1]:.4e}\"\n",
    ")\n",
    "print(\n",
    "    f\"  Fixed:    PDE={fixed_history['pde_loss'][-1]:.4e}, \"\n",
    "    f\"BC={fixed_history['bc_loss'][-1]:.4e}, IC={fixed_history['ic_loss'][-1]:.4e}\"\n",
    ")\n",
    "print()\n",
    "print(\"Final Weights (GradNorm):\")\n",
    "print(\n",
    "    f\"  w_PDE={history['weight_pde'][-1]:.3f}, \"\n",
    "    f\"w_BC={history['weight_bc'][-1]:.3f}, w_IC={history['weight_ic'][-1]:.3f}\"\n",
    ")\n",
    "print()\n",
    "print(\"Solution Quality:\")\n",
    "print(f\"  GradNorm L2 error: {l2_gradnorm:.6e}\")\n",
    "print(f\"  Fixed L2 error:    {l2_fixed:.6e}\")\n",
    "print(f\"  Improvement:       {(l2_fixed - l2_gradnorm) / l2_fixed * 100:.1f}%\")\n",
    "print()\n",
    "print(\"Key Insights:\")\n",
    "print(\"  1. GradNorm automatically balances loss components\")\n",
    "print(\"  2. Weights adapt based on gradient magnitudes and training rates\")\n",
    "print(\"  3. All components (PDE, BC, IC) decrease together with GradNorm\")\n",
    "print(\"  4. Fixed weights often lead to one component dominating\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf54cc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"GradNorm example completed successfully!\")\n",
    "print(f\"Results saved to: {OUTPUT_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "py:percent,ipynb",
   "notebook_metadata_filter": "kernelspec,language_info,-jupytext.text_representation.jupytext_version,-jupytext.notebook_metadata_filter,-jupytext.cell_metadata_filter"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
