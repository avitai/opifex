{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caaaf3df",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Distributed Data-Parallel Training\n",
    "\n",
    "| Property      | Value                                        |\n",
    "|---------------|----------------------------------------------|\n",
    "| Level         | Intermediate                                 |\n",
    "| Runtime       | ~10 seconds (1 GPU), ~5 seconds (2+ GPUs)   |\n",
    "| Memory        | ~500 MB                                      |\n",
    "| Prerequisites | `source activate.sh`                         |\n",
    "\n",
    "## Overview\n",
    "\n",
    "Train a simple PDE solver network using **SPMD data-parallel training**\n",
    "across all available JAX devices. Opifex wraps the mesh setup and batch\n",
    "sharding behind `DistributedConfig` — just pass it to `TrainingConfig`\n",
    "and the `Trainer` handles the rest.\n",
    "\n",
    "This example demonstrates:\n",
    "- **DistributedConfig**: Declarative mesh topology configuration\n",
    "- **Trainer integration**: Zero-code-change distributed training\n",
    "- **Automatic batch sharding**: Batches are partitioned across devices\n",
    "- **JIT compilation**: Training step is compiled via `nnx.jit`\n",
    "\n",
    "**How it works:**\n",
    "1. `DistributedConfig` describes the device mesh shape and axis names.\n",
    "2. `Trainer.__init__` creates a `DistributedManager` from this config.\n",
    "3. `Trainer.fit` shards each mini-batch across the `\"data\"` mesh axis\n",
    "   before feeding it to the JIT-compiled training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67cf188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "\n",
    "from opifex.core.training.config import TrainingConfig\n",
    "from opifex.core.training.trainer import Trainer\n",
    "from opifex.distributed.config import DistributedConfig\n",
    "\n",
    "\n",
    "num_devices = jax.device_count()\n",
    "print(\"=\" * 60)\n",
    "print(\"Distributed Data-Parallel Training\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"JAX backend:  {jax.default_backend()}\")\n",
    "print(f\"Devices:      {num_devices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ae3c63",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 1: Define the Model\n",
    "\n",
    "A minimal feed-forward network standing in for a PDE solver.\n",
    "Any `nnx.Module` works — the distributed machinery is orthogonal\n",
    "to the model definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c7cbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplePDEModel(nnx.Module):\n",
    "    \"\"\"Toy feed-forward surrogate for a PDE solution operator.\"\"\"\n",
    "\n",
    "    def __init__(self, features: int = 64, rngs: nnx.Rngs | None = None) -> None:\n",
    "        if rngs is None:\n",
    "            rngs = nnx.Rngs(0)\n",
    "        self.layer1 = nnx.Linear(4, features, rngs=rngs)\n",
    "        self.layer2 = nnx.Linear(features, 1, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x: jax.Array) -> jax.Array:\n",
    "        \"\"\"Forward pass through the model.\"\"\"\n",
    "        return self.layer2(nnx.relu(self.layer1(x)))\n",
    "\n",
    "\n",
    "model = SimplePDEModel(features=64, rngs=nnx.Rngs(42))\n",
    "n_params = sum(x.size for x in jax.tree_util.tree_leaves(nnx.state(model, nnx.Param)))\n",
    "print(f\"Model parameters: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10214a98",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 2: Configure Distributed Training\n",
    "\n",
    "`DistributedConfig` is a frozen dataclass describing the mesh topology.\n",
    "Pass `-1` for a mesh axis size to use all available devices on that axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0baf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "distributed_config = DistributedConfig(\n",
    "    mesh_shape=(num_devices,),\n",
    "    mesh_axis_names=(\"data\",),\n",
    "    strategy=\"data\",\n",
    ")\n",
    "print(f\"Mesh shape:   {distributed_config.mesh_shape}\")\n",
    "print(f\"Axis names:   {distributed_config.mesh_axis_names}\")\n",
    "print(f\"Strategy:     {distributed_config.strategy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b25dfa",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 3: Create Trainer with Distributed Config\n",
    "\n",
    "The only change compared to single-device training is passing\n",
    "`distributed_config` into `TrainingConfig`. Everything else —\n",
    "optimizer creation, JIT compilation, gradient computation — stays\n",
    "the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737142f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config = TrainingConfig(\n",
    "    num_epochs=20,\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=32,\n",
    "    verbose=False,\n",
    "    distributed_config=distributed_config,\n",
    ")\n",
    "\n",
    "trainer = Trainer(model, training_config)\n",
    "print(\"Trainer created with distributed config\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6808fcf0",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 4: Generate Data and Train\n",
    "\n",
    "Synthetic regression data: predict the sum-of-squares from 4 input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e075d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "x = jax.random.normal(key, (256, 4))\n",
    "y = jnp.sum(x**2, axis=-1, keepdims=True)\n",
    "\n",
    "print(f\"Training data: x={x.shape}, y={y.shape}\")\n",
    "print()\n",
    "print(\"Training...\")\n",
    "trained_model, metrics = trainer.fit(train_data=(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a062e59c",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b37a3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Devices used:    {num_devices}\")\n",
    "print(f\"  Initial loss:    {metrics['initial_train_loss']:.6f}\")\n",
    "print(f\"  Final loss:      {metrics['final_train_loss']:.6f}\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"Distributed training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c2d1f7",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Summary\n",
    "\n",
    "This example showed how to enable data-parallel training with three lines:\n",
    "\n",
    "1. Create a `DistributedConfig` describing the mesh topology\n",
    "2. Pass it to `TrainingConfig(distributed_config=...)`\n",
    "3. Call `trainer.fit()` as usual\n",
    "\n",
    "The `Trainer` automatically:\n",
    "- Creates a `DistributedManager` to manage the JAX device mesh\n",
    "- Shards each mini-batch across the `\"data\"` axis before the training step\n",
    "- JIT-compiles the training step via `@nnx.jit`\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Explore FSDP by setting `strategy=\"fsdp\"` with a 2D mesh\n",
    "- Add model-parallel sharding with `nnx.with_partitioning()`\n",
    "- See the [distributed module tests](../../tests/distributed/) for more patterns"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "py:percent,ipynb",
   "main_language": "python",
   "notebook_metadata_filter": "kernelspec,language_info,-jupytext.text_representation.jupytext_version,-jupytext.notebook_metadata_filter,-jupytext.cell_metadata_filter"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
