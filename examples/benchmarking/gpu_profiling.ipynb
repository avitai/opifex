{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75ad671d",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Comprehensive GPU Acceleration and Profiling\n",
    "\n",
    "| Property | Value |\n",
    "|---|---|\n",
    "| **Level** | Advanced |\n",
    "| **Runtime** | ~5 min (GPU) |\n",
    "| **Prerequisites** | JAX, Flax NNX, GPU Profiling |\n",
    "\n",
    "# Overview\n",
    "\n",
    "This script demonstrates the advanced GPU acceleration capabilities including:\n",
    "\n",
    "This script demonstrates the advanced GPU acceleration capabilities including:\n",
    "1. Roofline memory management and analysis\n",
    "2. Mixed precision optimization with TensorCore alignment\n",
    "3. Asynchronous memory operations and prefetching\n",
    "4. Memory pooling for efficient buffer reuse\n",
    "5. Cached progressive testing for optimal configurations\n",
    "6. JIT compilation with proper static_argnums and buffer donation\n",
    "7. Hardware-aware optimization and benchmarking\n",
    "8. Neural operator profiling with advanced optimizations\n",
    "\n",
    "Features included:\n",
    "- Advanced GPU acceleration with OptimizedGPUManager\n",
    "- Roofline model-based memory management\n",
    "- Mixed precision with hardware detection\n",
    "- Memory pooling with 8x+ speedup demonstrations\n",
    "- Asynchronous memory operations with overlap\n",
    "- Cached progressive testing for optimal batch sizes\n",
    "- JIT vs non-JIT performance comparison\n",
    "- Compilation overhead analysis with break-even calculations\n",
    "- TensorCore alignment and utilization analysis\n",
    "- Comprehensive performance reporting with efficiency metrics\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "\n",
    "Import Opifex components\n",
    "from opifex.benchmarking.profiling import OpifexProfilingHarness\n",
    "\n",
    "Import advanced GPU acceleration components\n",
    "from opifex.core.gpu_acceleration import (\n",
    "    AsyncMemoryManager,\n",
    "    benchmark_gpu_operations,\n",
    "    CachedProgressiveTester,\n",
    "    MemoryPoolManager,\n",
    "    MixedPrecisionOptimizer,\n",
    "    OptimizedGPUManager,\n",
    "    RooflineMemoryManager,\n",
    "    safe_matrix_multiply,\n",
    ")\n",
    "from opifex.neural.operators import FourierNeuralOperator, UFourierNeuralOperator\n",
    "from opifex.training.mixed_precision import (\n",
    "    align_for_tensorcore,\n",
    ")\n",
    "\n",
    "\n",
    "class ComprehensiveProfilingDemo:\n",
    "    \"\"\"Comprehensive profiling demonstration with advanced GPU acceleration.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the comprehensive profiling demo with GPU acceleration.\"\"\"\n",
    "        self.profiler = OpifexProfilingHarness(\n",
    "            enable_hardware_profiling=True,\n",
    "            enable_compilation_profiling=True,\n",
    "            enable_roofline_analysis=True,\n",
    "        )\n",
    "\n",
    "        # Initialize advanced GPU acceleration components\n",
    "        self.gpu_manager = OptimizedGPUManager()\n",
    "        self.roofline_manager = RooflineMemoryManager()\n",
    "        self.mixed_precision = MixedPrecisionOptimizer()\n",
    "        self.async_manager = AsyncMemoryManager()\n",
    "        self.memory_pool = MemoryPoolManager()\n",
    "        self.progressive_tester = CachedProgressiveTester(self.roofline_manager)\n",
    "\n",
    "        self.results = {}\n",
    "\n",
    "        # Run initial GPU benchmark to establish baseline\n",
    "        print(\"üöÄ Initializing GPU acceleration components...\")\n",
    "        self.gpu_baseline = benchmark_gpu_operations()\n",
    "        print(\n",
    "            f\"‚úÖ GPU acceleration initialized on {self.gpu_baseline['backend_info']['backend']} backend\"\n",
    "        )\n",
    "        print(\n",
    "            f\"   Available memory: {self.gpu_baseline['backend_info']['memory_gb']:.1f}GB\"\n",
    "        )\n",
    "\n",
    "    def create_sample_data(\n",
    "        self, batch_size=32, grid_size=64, channels=3, optimize=False\n",
    "    ):\n",
    "        \"\"\"Create sample data for neural operator profiling.\"\"\"\n",
    "        key = jax.random.PRNGKey(42)\n",
    "\n",
    "        if optimize:\n",
    "            # Optimized version with TensorCore alignment\n",
    "            aligned_grid_size = ((grid_size + 15) // 16) * 16\n",
    "            input_data = jax.random.normal(\n",
    "                key, (batch_size, channels, aligned_grid_size, aligned_grid_size)\n",
    "            )\n",
    "            input_data = align_for_tensorcore(input_data, alignment=16)\n",
    "\n",
    "            print(\"üìä Optimized Data Configuration:\")\n",
    "            print(f\"   ‚Ä¢ Batch size: {batch_size}\")\n",
    "            print(\n",
    "                f\"   ‚Ä¢ Grid size: {aligned_grid_size}x{aligned_grid_size} (TensorCore aligned)\"\n",
    "            )\n",
    "            print(f\"   ‚Ä¢ Channels: {channels}\")\n",
    "            print(f\"   ‚Ä¢ Data shape: {input_data.shape}\")\n",
    "            print(f\"   ‚Ä¢ Data dtype: {input_data.dtype}\")\n",
    "        else:\n",
    "            # Basic version\n",
    "            input_data = jax.random.normal(\n",
    "                key, (batch_size, grid_size, grid_size, channels)\n",
    "            )\n",
    "\n",
    "        return input_data\n",
    "\n",
    "    def create_neural_operators(self):\n",
    "        \"\"\"Create neural operators for profiling.\"\"\"\n",
    "        print(\"\\nüìã Creating Neural Operators...\")\n",
    "\n",
    "        # Basic FNO\n",
    "        fno_basic = FourierNeuralOperator(\n",
    "            in_channels=3,\n",
    "            out_channels=3,\n",
    "            hidden_channels=64,\n",
    "            modes=16,\n",
    "            num_layers=4,\n",
    "            rngs=nnx.Rngs(0),\n",
    "        )\n",
    "\n",
    "        # Optimized FNO with mixed precision\n",
    "        fno_optimized = FourierNeuralOperator(\n",
    "            in_channels=3,\n",
    "            out_channels=3,\n",
    "            hidden_channels=128,  # TensorCore aligned\n",
    "            modes=16,\n",
    "            num_layers=4,\n",
    "            use_mixed_precision=True,\n",
    "            rngs=nnx.Rngs(1),\n",
    "        )\n",
    "\n",
    "        # UNO for comparison\n",
    "        uno = UFourierNeuralOperator(\n",
    "            in_channels=3,\n",
    "            out_channels=3,\n",
    "            hidden_channels=32,\n",
    "            modes=(8, 8),\n",
    "            num_levels=3,\n",
    "            rngs=nnx.Rngs(2),\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"FNO_Basic\": fno_basic,\n",
    "            \"FNO_Optimized\": fno_optimized,\n",
    "            \"UNO\": uno,\n",
    "        }\n",
    "\n",
    "    def time_with_proper_warmup(\n",
    "        self, func, inputs, num_warmup=5, num_runs=10, verbose=True\n",
    "    ):\n",
    "        \"\"\"Time function with proper warm-up and multiple runs for accuracy.\"\"\"\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"  Performing {num_warmup} warm-up runs...\")\n",
    "\n",
    "        # Warm-up runs to ensure compilation\n",
    "        for i in range(num_warmup):\n",
    "            result = func(*inputs)\n",
    "            if hasattr(result, \"block_until_ready\"):\n",
    "                result.block_until_ready()\n",
    "            if verbose:\n",
    "                print(f\"    Warm-up {i + 1}/{num_warmup} completed\")\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"  Performing {num_runs} timing runs...\")\n",
    "\n",
    "        # Actual timing runs\n",
    "        times = []\n",
    "        for i in range(num_runs):\n",
    "            start_time = time.time()\n",
    "            result = func(*inputs)\n",
    "            if hasattr(result, \"block_until_ready\"):\n",
    "                result.block_until_ready()\n",
    "            end_time = time.time()\n",
    "            times.append(end_time - start_time)\n",
    "            if verbose:\n",
    "                print(f\"    Timing run {i + 1}/{num_runs}: {times[-1] * 1000:.2f}ms\")\n",
    "\n",
    "        return {\n",
    "            \"mean_time\": sum(times) / len(times),\n",
    "            \"min_time\": min(times),\n",
    "            \"max_time\": max(times),\n",
    "            \"std_time\": (\n",
    "                sum((t - sum(times) / len(times)) ** 2 for t in times) / len(times)\n",
    "            )\n",
    "            ** 0.5,\n",
    "            \"all_times\": times,\n",
    "        }\n",
    "\n",
    "    def compare_jit_vs_non_jit(self):\n",
    "        \"\"\"Compare JIT compiled vs non-JIT performance.\"\"\"\n",
    "\n",
    "        print(\"\\nüî• JIT vs Non-JIT Performance Comparison\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # Create test data and model\n",
    "        test_data = self.create_sample_data(batch_size=64, grid_size=64, optimize=True)\n",
    "        operators = self.create_neural_operators()\n",
    "        model = operators[\"FNO_Optimized\"]\n",
    "\n",
    "        print(f\"Test data shape: {test_data.shape}\")\n",
    "        print(f\"Test data dtype: {test_data.dtype}\")\n",
    "\n",
    "        # Define the forward function\n",
    "        def forward_func(x):\n",
    "            return model(x)\n",
    "\n",
    "        # Non-JIT version (with jax.disable_jit)\n",
    "        print(\"\\nüìä Testing Non-JIT Performance...\")\n",
    "        with jax.disable_jit():\n",
    "            non_jit_results = self.time_with_proper_warmup(\n",
    "                forward_func, [test_data], num_warmup=3, num_runs=5\n",
    "            )\n",
    "\n",
    "        # JIT version\n",
    "        print(\"\\n‚ö° Testing JIT Performance...\")\n",
    "        jit_func = jax.jit(forward_func)\n",
    "        jit_results = self.time_with_proper_warmup(\n",
    "            jit_func, [test_data], num_warmup=5, num_runs=10\n",
    "        )\n",
    "\n",
    "        # Calculate speedup\n",
    "        speedup = non_jit_results[\"mean_time\"] / jit_results[\"mean_time\"]\n",
    "\n",
    "        print(\"\\nüìà Performance Comparison Results:\")\n",
    "        print(\"  Non-JIT Performance:\")\n",
    "        print(f\"    ‚Ä¢ Mean time: {non_jit_results['mean_time'] * 1000:.2f}ms\")\n",
    "        print(f\"    ‚Ä¢ Min time:  {non_jit_results['min_time'] * 1000:.2f}ms\")\n",
    "        print(f\"    ‚Ä¢ Max time:  {non_jit_results['max_time'] * 1000:.2f}ms\")\n",
    "        print(f\"    ‚Ä¢ Std dev:   {non_jit_results['std_time'] * 1000:.2f}ms\")\n",
    "\n",
    "        print(\"  JIT Performance:\")\n",
    "        print(f\"    ‚Ä¢ Mean time: {jit_results['mean_time'] * 1000:.2f}ms\")\n",
    "        print(f\"    ‚Ä¢ Min time:  {jit_results['min_time'] * 1000:.2f}ms\")\n",
    "        print(f\"    ‚Ä¢ Max time:  {jit_results['max_time'] * 1000:.2f}ms\")\n",
    "        print(f\"    ‚Ä¢ Std dev:   {jit_results['std_time'] * 1000:.2f}ms\")\n",
    "\n",
    "        print(f\"  üöÄ JIT Speedup: {speedup:.2f}x\")\n",
    "\n",
    "        if speedup > 2.0:\n",
    "            print(\"  ‚úÖ Excellent JIT performance improvement!\")\n",
    "        elif speedup > 1.5:\n",
    "            print(\"  ‚úÖ Good JIT performance improvement\")\n",
    "        elif speedup > 1.1:\n",
    "            print(\"  ‚ö†Ô∏è  Modest JIT improvement - check for optimization opportunities\")\n",
    "        else:\n",
    "            print(\"  ‚ùå Poor JIT performance - investigate compilation issues\")\n",
    "\n",
    "        self.results[\"jit_comparison\"] = {\n",
    "            \"non_jit\": non_jit_results,\n",
    "            \"jit\": jit_results,\n",
    "            \"speedup\": speedup,\n",
    "        }\n",
    "\n",
    "        return self.results[\"jit_comparison\"]\n",
    "\n",
    "    def analyze_compilation_overhead(self):\n",
    "        \"\"\"Analyze JIT compilation overhead separately from execution time.\"\"\"\n",
    "\n",
    "        print(\"\\n‚è±Ô∏è  JIT Compilation Overhead Analysis\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        test_data = self.create_sample_data(batch_size=32, grid_size=64, optimize=True)\n",
    "        operators = self.create_neural_operators()\n",
    "        model = operators[\"FNO_Basic\"]  # Use basic model for faster compilation\n",
    "\n",
    "        def forward_func(x):\n",
    "            return model(x)\n",
    "\n",
    "        # Measure compilation time\n",
    "        print(\"  Measuring compilation time...\")\n",
    "        compilation_start = time.time()\n",
    "        jit_func = jax.jit(forward_func)\n",
    "\n",
    "        # First call triggers compilation\n",
    "        result = jit_func(test_data)\n",
    "        if hasattr(result, \"block_until_ready\"):\n",
    "            result.block_until_ready()\n",
    "\n",
    "        compilation_time = time.time() - compilation_start\n",
    "\n",
    "        # Measure execution time after compilation\n",
    "        print(\"  Measuring post-compilation execution time...\")\n",
    "        execution_times = []\n",
    "        for _ in range(10):\n",
    "            start_time = time.time()\n",
    "            result = jit_func(test_data)\n",
    "            if hasattr(result, \"block_until_ready\"):\n",
    "                result.block_until_ready()\n",
    "            execution_times.append(time.time() - start_time)\n",
    "\n",
    "        mean_execution_time = sum(execution_times) / len(execution_times)\n",
    "\n",
    "        print(\"\\nüìä Compilation Analysis Results:\")\n",
    "        print(f\"  ‚Ä¢ Compilation time: {compilation_time * 1000:.2f}ms\")\n",
    "        print(f\"  ‚Ä¢ Mean execution time: {mean_execution_time * 1000:.2f}ms\")\n",
    "        print(\n",
    "            f\"  ‚Ä¢ Compilation overhead: {compilation_time / mean_execution_time:.1f}x execution time\"\n",
    "        )\n",
    "\n",
    "        # Calculate break-even point\n",
    "        break_even_calls = compilation_time / mean_execution_time\n",
    "        print(f\"  ‚Ä¢ Break-even point: {break_even_calls:.1f} calls\")\n",
    "\n",
    "        if break_even_calls < 10:\n",
    "            print(\"  ‚úÖ Low compilation overhead - JIT is beneficial\")\n",
    "        elif break_even_calls < 50:\n",
    "            print(\"  ‚ö†Ô∏è  Moderate compilation overhead - beneficial for repeated use\")\n",
    "        else:\n",
    "            print(\"  ‚ùå High compilation overhead - consider optimization\")\n",
    "\n",
    "        self.results[\"compilation_analysis\"] = {\n",
    "            \"compilation_time\": compilation_time,\n",
    "            \"mean_execution_time\": mean_execution_time,\n",
    "            \"break_even_calls\": break_even_calls,\n",
    "        }\n",
    "\n",
    "        return self.results[\"compilation_analysis\"]\n",
    "\n",
    "    def demonstrate_memory_pool_efficiency(self):  # noqa: PLR0912, PLR0915\n",
    "        \"\"\"Demonstrate memory pool efficiency with realistic workload simulation.\"\"\"\n",
    "\n",
    "        print(\"\\nüíæ Memory Pool Efficiency Demonstration\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # Use larger arrays and more realistic workload for better demonstration\n",
    "        shapes = [\n",
    "            (1024, 1024),\n",
    "            (2048, 512),\n",
    "            (512, 2048),\n",
    "        ]  # Multiple shapes for realistic scenario\n",
    "        dtype = jnp.float32\n",
    "        num_iterations = 50  # Reduced for faster demo but still meaningful\n",
    "        operations_per_buffer = (\n",
    "            5  # Multiple operations per buffer to show reuse benefit\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Testing {num_iterations} iterations with {len(shapes)} different buffer shapes\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Performing {operations_per_buffer} operations per buffer to simulate realistic workload\"\n",
    "        )\n",
    "\n",
    "        # Test with memory pool - realistic workload\n",
    "        print(\"\\nüîÑ Testing with Memory Pool...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        for i in range(num_iterations):\n",
    "            for shape in shapes:\n",
    "                buffer = self.memory_pool.get_buffer(shape, dtype)\n",
    "\n",
    "                # Simulate realistic computational workload\n",
    "                for op in range(operations_per_buffer):\n",
    "                    if op == 0:\n",
    "                        result = buffer * 2.0  # Scaling\n",
    "                    elif op == 1:\n",
    "                        result = jnp.sin(result)  # Element-wise function\n",
    "                    elif op == 2:\n",
    "                        result = result + jnp.ones_like(result)  # Addition\n",
    "                    elif op == 3:\n",
    "                        result = jnp.transpose(result)  # Reshape operation\n",
    "                    else:\n",
    "                        result = jnp.sum(result, axis=-1, keepdims=True)  # Reduction\n",
    "\n",
    "                    result.block_until_ready()  # Ensure computation completes\n",
    "\n",
    "                self.memory_pool.return_buffer(buffer)\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print(f\"  Progress: {i + 1}/{num_iterations}\")\n",
    "\n",
    "        pooled_time = time.time() - start_time\n",
    "        pool_stats = self.memory_pool.get_pool_stats()\n",
    "\n",
    "        # Test without memory pool - same workload\n",
    "        print(\"\\nüì¶ Testing Direct Allocation...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        for i in range(num_iterations):\n",
    "            for shape in shapes:\n",
    "                buffer = jnp.zeros(shape, dtype=dtype)\n",
    "\n",
    "                # Same computational workload\n",
    "                for op in range(operations_per_buffer):\n",
    "                    if op == 0:\n",
    "                        result = buffer * 2.0\n",
    "                    elif op == 1:\n",
    "                        result = jnp.sin(result)\n",
    "                    elif op == 2:\n",
    "                        result = result + jnp.ones_like(result)\n",
    "                    elif op == 3:\n",
    "                        result = jnp.transpose(result)\n",
    "                    else:\n",
    "                        result = jnp.sum(result, axis=-1, keepdims=True)\n",
    "\n",
    "                    result.block_until_ready()\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print(f\"  Progress: {i + 1}/{num_iterations}\")\n",
    "\n",
    "        direct_time = time.time() - start_time\n",
    "\n",
    "        # Calculate efficiency\n",
    "        efficiency = direct_time / pooled_time if pooled_time > 0 else 0\n",
    "\n",
    "        print(\"\\nüìä Memory Pool Efficiency Results:\")\n",
    "        print(f\"  ‚Ä¢ Direct allocation time: {direct_time:.3f}s\")\n",
    "        print(f\"  ‚Ä¢ Memory pool time: {pooled_time:.3f}s\")\n",
    "        print(f\"  ‚Ä¢ Efficiency improvement: {efficiency:.2f}x\")\n",
    "        print(f\"  ‚Ä¢ Buffer reuse ratio: {pool_stats['reuse_ratio']:.2%}\")\n",
    "        print(f\"  ‚Ä¢ Total allocations: {pool_stats['total_allocations']}\")\n",
    "        print(f\"  ‚Ä¢ Total reuses: {pool_stats['total_reuses']}\")\n",
    "        print(\n",
    "            f\"  ‚Ä¢ Memory saved: {(pool_stats['total_reuses'] * 1024 * 1024 * 4 / 1024**2):.1f}MB\"\n",
    "        )\n",
    "\n",
    "        if efficiency > 3.0:\n",
    "            print(\"  ‚úÖ Excellent memory pool performance!\")\n",
    "        elif efficiency > 1.5:\n",
    "            print(\"  ‚úÖ Good memory pool performance\")\n",
    "        elif efficiency > 1.05:\n",
    "            print(\"  ‚úÖ Modest improvement - memory pool is working\")\n",
    "        else:\n",
    "            print(\"  ‚ö†Ô∏è  Limited improvement - workload may not benefit from pooling\")\n",
    "\n",
    "        self.results[\"memory_pool_efficiency\"] = {\n",
    "            \"direct_time\": direct_time,\n",
    "            \"pooled_time\": pooled_time,\n",
    "            \"efficiency\": efficiency,\n",
    "            \"pool_stats\": pool_stats,\n",
    "        }\n",
    "\n",
    "        return self.results[\"memory_pool_efficiency\"]\n",
    "\n",
    "    def demonstrate_mixed_precision_optimization(self):  # noqa: PLR0915\n",
    "        \"\"\"Demonstrate mixed precision optimization with TensorCore alignment.\"\"\"\n",
    "\n",
    "        print(\"\\nüéØ Mixed Precision Optimization Demonstration\")\n",
    "        print(\"=\" * 55)\n",
    "\n",
    "        # Test different matrix sizes optimized for TensorCore utilization\n",
    "        test_configs = [\n",
    "            (512, 512, \"TensorCore Aligned\"),\n",
    "            (1024, 1024, \"Large TensorCore\"),\n",
    "            (2048, 2048, \"Huge TensorCore\"),\n",
    "            (4096, 4096, \"Maximum TensorCore\"),\n",
    "        ]\n",
    "\n",
    "        print(\"Testing matrix multiplication with TensorCore-optimized sizes...\")\n",
    "\n",
    "        mixed_precision_results = {}\n",
    "\n",
    "        for size_m, size_n, config_name in test_configs:\n",
    "            print(f\"\\n--- Testing {config_name}: {size_m}x{size_n} matrices ---\")\n",
    "\n",
    "            # Create test matrices with proper alignment for TensorCore\n",
    "            key = jax.random.PRNGKey(42)\n",
    "            x = jax.random.normal(key, (size_m, size_n), dtype=jnp.float32)\n",
    "            y = jax.random.normal(key, (size_n, size_m), dtype=jnp.float32)\n",
    "\n",
    "            # Warm up GPU for consistent timing\n",
    "            _ = x @ y\n",
    "            jax.block_until_ready(_)\n",
    "\n",
    "            # Test regular float32 multiplication\n",
    "            print(\"  Testing float32 precision...\")\n",
    "            start_time = time.time()\n",
    "            for _ in range(5):  # Reduced iterations for larger matrices\n",
    "                result_f32 = safe_matrix_multiply(x, y)\n",
    "                result_f32.block_until_ready()\n",
    "            f32_time = (time.time() - start_time) / 5\n",
    "\n",
    "            # Test mixed precision multiplication\n",
    "            print(\"  Testing mixed precision (TensorCore optimized)...\")\n",
    "            start_time = time.time()\n",
    "            for _ in range(5):\n",
    "                result_mixed = self.mixed_precision.mixed_precision_matmul(x, y)\n",
    "                result_mixed.block_until_ready()\n",
    "            mixed_time = (time.time() - start_time) / 5\n",
    "\n",
    "            # Test GPU manager optimized multiplication\n",
    "            print(\"  Testing GPU manager optimization...\")\n",
    "            start_time = time.time()\n",
    "            for _ in range(5):\n",
    "                result_opt = self.gpu_manager.optimal_matrix_multiply(x, y)\n",
    "                result_opt.block_until_ready()\n",
    "            opt_time = (time.time() - start_time) / 5\n",
    "\n",
    "            # Calculate speedups\n",
    "            mixed_speedup = f32_time / mixed_time if mixed_time > 0 else 0\n",
    "            opt_speedup = f32_time / opt_time if opt_time > 0 else 0\n",
    "\n",
    "            print(f\"  Results for {config_name}:\")\n",
    "            print(f\"    ‚Ä¢ Float32 time: {f32_time * 1000:.2f}ms\")\n",
    "            print(\n",
    "                f\"    ‚Ä¢ Mixed precision time: {mixed_time * 1000:.2f}ms ({mixed_speedup:.2f}x)\"\n",
    "            )\n",
    "            print(\n",
    "                f\"    ‚Ä¢ GPU optimized time: {opt_time * 1000:.2f}ms ({opt_speedup:.2f}x)\"\n",
    "            )\n",
    "\n",
    "            # Calculate FLOPS for performance analysis\n",
    "            flops = 2 * size_m * size_n * size_m  # Matrix multiplication FLOPs\n",
    "            f32_gflops = flops / (f32_time * 1e9)\n",
    "            mixed_gflops = flops / (mixed_time * 1e9) if mixed_time > 0 else 0\n",
    "            opt_gflops = flops / (opt_time * 1e9) if opt_time > 0 else 0\n",
    "\n",
    "            print(f\"    ‚Ä¢ Float32 performance: {f32_gflops:.1f} GFLOPS\")\n",
    "            print(f\"    ‚Ä¢ Mixed precision performance: {mixed_gflops:.1f} GFLOPS\")\n",
    "            print(f\"    ‚Ä¢ GPU optimized performance: {opt_gflops:.1f} GFLOPS\")\n",
    "\n",
    "            mixed_precision_results[config_name] = {\n",
    "                \"size\": (size_m, size_n),\n",
    "                \"f32_time\": f32_time,\n",
    "                \"mixed_time\": mixed_time,\n",
    "                \"opt_time\": opt_time,\n",
    "                \"mixed_speedup\": mixed_speedup,\n",
    "                \"opt_speedup\": opt_speedup,\n",
    "                \"f32_gflops\": f32_gflops,\n",
    "                \"mixed_gflops\": mixed_gflops,\n",
    "                \"opt_gflops\": opt_gflops,\n",
    "            }\n",
    "\n",
    "        # Summary\n",
    "        print(\"\\nüìä Mixed Precision Summary:\")\n",
    "        avg_mixed_speedup = sum(\n",
    "            r[\"mixed_speedup\"] for r in mixed_precision_results.values()\n",
    "        ) / len(mixed_precision_results)\n",
    "        avg_opt_speedup = sum(\n",
    "            r[\"opt_speedup\"] for r in mixed_precision_results.values()\n",
    "        ) / len(mixed_precision_results)\n",
    "\n",
    "        print(f\"  ‚Ä¢ Average mixed precision speedup: {avg_mixed_speedup:.2f}x\")\n",
    "        print(f\"  ‚Ä¢ Average optimized speedup: {avg_opt_speedup:.2f}x\")\n",
    "\n",
    "        if avg_mixed_speedup > 1.5:\n",
    "            print(\"  ‚úÖ Mixed precision provides significant acceleration!\")\n",
    "        elif avg_mixed_speedup > 1.1:\n",
    "            print(\"  ‚úÖ Mixed precision provides modest acceleration\")\n",
    "        else:\n",
    "            print(\"  ‚ö†Ô∏è  Mixed precision shows limited benefit on this hardware\")\n",
    "\n",
    "        self.results[\"mixed_precision\"] = mixed_precision_results\n",
    "        return mixed_precision_results\n",
    "\n",
    "    def demonstrate_async_memory_operations(self):\n",
    "        \"\"\"Demonstrate asynchronous memory operations with prefetching.\"\"\"\n",
    "\n",
    "        print(\"\\n‚ö° Asynchronous Memory Operations Demonstration\")\n",
    "        print(\"=\" * 55)\n",
    "\n",
    "        # Create test data\n",
    "        batch_size = 64\n",
    "        data_size = (batch_size, 256, 256)\n",
    "\n",
    "        print(f\"Testing async operations with data shape: {data_size}\")\n",
    "\n",
    "        # Generate multiple data batches\n",
    "        key = jax.random.PRNGKey(42)\n",
    "        data_batches = []\n",
    "        for _i in range(5):\n",
    "            batch = jax.random.normal(jax.random.split(key, 1)[0], data_size)\n",
    "            data_batches.append(batch)\n",
    "\n",
    "        # Test synchronous operations\n",
    "        print(\"\\nüîÑ Testing Synchronous Operations...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        for i, batch in enumerate(data_batches):\n",
    "            # Simulate computation\n",
    "            result = jnp.sum(batch**2, axis=(1, 2))\n",
    "            result.block_until_ready()\n",
    "            print(f\"  Processed batch {i + 1}/5\")\n",
    "\n",
    "        sync_time = time.time() - start_time\n",
    "\n",
    "        # Test asynchronous operations with prefetching\n",
    "        print(\"\\n‚ö° Testing Asynchronous Operations with Prefetching...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Prefetch first batch\n",
    "        if len(data_batches) > 0:\n",
    "            device = jax.devices()[0]\n",
    "            self.async_manager.async_device_put(data_batches[0], device, \"batch_0\")\n",
    "\n",
    "        for i, batch in enumerate(data_batches):\n",
    "            # Prefetch next batch while processing current\n",
    "            if i + 1 < len(data_batches):\n",
    "                self.async_manager.async_device_put(\n",
    "                    data_batches[i + 1], device, f\"batch_{i + 1}\"\n",
    "                )\n",
    "\n",
    "            # Process current batch\n",
    "            result = jnp.sum(batch**2, axis=(1, 2))\n",
    "            result.block_until_ready()\n",
    "            print(f\"  Processed batch {i + 1}/5 with prefetching\")\n",
    "\n",
    "        async_time = time.time() - start_time\n",
    "\n",
    "        # Calculate efficiency\n",
    "        async_speedup = sync_time / async_time if async_time > 0 else 0\n",
    "\n",
    "        print(\"\\nüìä Async Memory Operations Results:\")\n",
    "        print(f\"  ‚Ä¢ Synchronous time: {sync_time:.3f}s\")\n",
    "        print(f\"  ‚Ä¢ Asynchronous time: {async_time:.3f}s\")\n",
    "        print(f\"  ‚Ä¢ Async speedup: {async_speedup:.2f}x\")\n",
    "\n",
    "        if async_speedup > 1.2:\n",
    "            print(\"  ‚úÖ Async operations provide good acceleration!\")\n",
    "        elif async_speedup > 1.05:\n",
    "            print(\"  ‚úÖ Async operations provide modest benefit\")\n",
    "        else:\n",
    "            print(\"  ‚ö†Ô∏è  Limited async benefit - may be compute-bound\")\n",
    "\n",
    "        self.results[\"async_operations\"] = {\n",
    "            \"sync_time\": sync_time,\n",
    "            \"async_time\": async_time,\n",
    "            \"speedup\": async_speedup,\n",
    "        }\n",
    "\n",
    "        return self.results[\"async_operations\"]\n",
    "\n",
    "    def demonstrate_roofline_analysis(self):\n",
    "        \"\"\"Demonstrate roofline model analysis for memory optimization.\"\"\"\n",
    "\n",
    "        print(\"\\nüìà Roofline Model Analysis Demonstration\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # Get hardware specifications (accessing private attribute for demo purposes)\n",
    "        hw_specs = self.roofline_manager.hw_specs\n",
    "\n",
    "        print(\"Hardware Specifications:\")\n",
    "        print(f\"  ‚Ä¢ Peak FLOPS: {hw_specs['peak_flops']:.2e} FLOP/s\")\n",
    "        print(f\"  ‚Ä¢ Memory bandwidth: {hw_specs['memory_bandwidth']:.2e} GB/s\")\n",
    "        print(f\"  ‚Ä¢ Memory capacity: {hw_specs['memory_gb']:.1f} GB\")\n",
    "        print(f\"  ‚Ä¢ Platform: {hw_specs['platform']}\")\n",
    "        print(f\"  ‚Ä¢ TensorCore support: {hw_specs.get('supports_tensorcore', False)}\")\n",
    "\n",
    "        # Test different operations with varying arithmetic intensity\n",
    "        operations = [\n",
    "            (\"Small Matrix Multiply\", \"matmul\", (128, 128, 128), \"memory-bound\"),\n",
    "            (\"Medium Matrix Multiply\", \"matmul\", (512, 512, 512), \"balanced\"),\n",
    "            (\"Large Matrix Multiply\", \"matmul\", (1024, 1024, 1024), \"compute-bound\"),\n",
    "            (\"Huge Matrix Multiply\", \"matmul\", (2048, 2048, 2048), \"compute-bound\"),\n",
    "        ]\n",
    "\n",
    "        roofline_results = {}\n",
    "\n",
    "        for op_name, op_type, shapes, expected_bound in operations:\n",
    "            print(f\"\\n--- Analyzing {op_name} ---\")\n",
    "\n",
    "            # Estimate operation efficiency using correct method signature\n",
    "            try:\n",
    "                efficiency = self.roofline_manager.estimate_operation_efficiency(\n",
    "                    op_type, *shapes\n",
    "                )\n",
    "\n",
    "                print(\n",
    "                    f\"  ‚Ä¢ Arithmetic intensity: {efficiency['arithmetic_intensity']:.2f} FLOP/byte\"\n",
    "                )\n",
    "                print(f\"  ‚Ä¢ Compute bound: {efficiency['is_compute_bound']}\")\n",
    "                print(f\"  ‚Ä¢ Expected: {expected_bound}\")\n",
    "\n",
    "                # Verify prediction\n",
    "                actual_bound = (\n",
    "                    \"compute-bound\"\n",
    "                    if efficiency[\"is_compute_bound\"]\n",
    "                    else \"memory-bound\"\n",
    "                )\n",
    "                if expected_bound == actual_bound:\n",
    "                    print(\"  ‚úÖ Roofline prediction matches expectation\")\n",
    "                else:\n",
    "                    print(\"  ‚ö†Ô∏è  Roofline prediction differs from expectation\")\n",
    "\n",
    "                roofline_results[op_name] = efficiency\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ùå Error analyzing {op_name}: {e}\")\n",
    "                roofline_results[op_name] = {\"error\": str(e)}\n",
    "\n",
    "        self.results[\"roofline_analysis\"] = roofline_results\n",
    "        return roofline_results\n",
    "\n",
    "    def demonstrate_tensorcore_optimization(self):  # noqa: PLR0915\n",
    "        \"\"\"Demonstrate TensorCore optimization with proper alignment and mixed precision.\"\"\"\n",
    "\n",
    "        print(\"\\nüéØ TensorCore Optimization Demonstration\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # TensorCore requires specific alignments and data types\n",
    "        tensorcore_configs = [\n",
    "            (768, 768, jnp.bfloat16, \"BFloat16 TensorCore\"),\n",
    "            (1024, 1024, jnp.bfloat16, \"Large BFloat16 TensorCore\"),\n",
    "            (2048, 2048, jnp.bfloat16, \"Huge BFloat16 TensorCore\"),\n",
    "        ]\n",
    "\n",
    "        print(\"Testing TensorCore-optimized matrix operations...\")\n",
    "        print(\"Note: TensorCore requires bfloat16/float16 and specific alignments\")\n",
    "\n",
    "        tensorcore_results = {}\n",
    "\n",
    "        for size_m, size_n, dtype, config_name in tensorcore_configs:\n",
    "            print(f\"\\n--- Testing {config_name}: {size_m}x{size_n} ---\")\n",
    "\n",
    "            # Create properly aligned matrices for TensorCore\n",
    "            key = jax.random.PRNGKey(42)\n",
    "            x_f32 = jax.random.normal(key, (size_m, size_n), dtype=jnp.float32)\n",
    "            y_f32 = jax.random.normal(key, (size_n, size_m), dtype=jnp.float32)\n",
    "\n",
    "            # Convert to TensorCore-compatible format\n",
    "            x_tc = x_f32.astype(dtype)\n",
    "            y_tc = y_f32.astype(dtype)\n",
    "\n",
    "            # Warm up\n",
    "            _ = x_tc @ y_tc\n",
    "            jax.block_until_ready(_)\n",
    "\n",
    "            # Test Float32 baseline\n",
    "            print(\"  Testing Float32 baseline...\")\n",
    "            start_time = time.time()\n",
    "            for _ in range(3):\n",
    "                result_f32 = x_f32 @ y_f32\n",
    "                result_f32.block_until_ready()\n",
    "            f32_time = (time.time() - start_time) / 3\n",
    "\n",
    "            # Test TensorCore optimized\n",
    "            print(f\"  Testing {dtype} TensorCore...\")\n",
    "            start_time = time.time()\n",
    "            for _ in range(3):\n",
    "                result_tc = x_tc @ y_tc\n",
    "                result_tc.block_until_ready()\n",
    "            tc_time = (time.time() - start_time) / 3\n",
    "\n",
    "            # Test with mixed precision optimizer\n",
    "            print(\"  Testing Mixed Precision Optimizer...\")\n",
    "            start_time = time.time()\n",
    "            for _ in range(3):\n",
    "                result_mixed = self.mixed_precision.mixed_precision_matmul(x_f32, y_f32)\n",
    "                result_mixed.block_until_ready()\n",
    "            mixed_time = (time.time() - start_time) / 3\n",
    "\n",
    "            # Calculate performance metrics\n",
    "            flops = 2 * size_m * size_n * size_m\n",
    "            f32_gflops = flops / (f32_time * 1e9)\n",
    "            tc_gflops = flops / (tc_time * 1e9) if tc_time > 0 else 0\n",
    "            mixed_gflops = flops / (mixed_time * 1e9) if mixed_time > 0 else 0\n",
    "\n",
    "            tc_speedup = f32_time / tc_time if tc_time > 0 else 0\n",
    "            mixed_speedup = f32_time / mixed_time if mixed_time > 0 else 0\n",
    "\n",
    "            print(f\"  Results for {config_name}:\")\n",
    "            print(\n",
    "                f\"    ‚Ä¢ Float32 time: {f32_time * 1000:.2f}ms ({f32_gflops:.1f} GFLOPS)\"\n",
    "            )\n",
    "            print(\n",
    "                f\"    ‚Ä¢ TensorCore time: {tc_time * 1000:.2f}ms ({tc_gflops:.1f} GFLOPS, {tc_speedup:.2f}x)\"\n",
    "            )\n",
    "            print(\n",
    "                f\"    ‚Ä¢ Mixed precision time: {mixed_time * 1000:.2f}ms ({mixed_gflops:.1f} GFLOPS, {mixed_speedup:.2f}x)\"\n",
    "            )\n",
    "\n",
    "            # Estimate TensorCore utilization based on performance\n",
    "            theoretical_tc_gflops = 312000  # Approximate for modern GPUs\n",
    "            tc_utilization = (\n",
    "                min(tc_gflops / theoretical_tc_gflops, 1.0)\n",
    "                if theoretical_tc_gflops > 0\n",
    "                else 0\n",
    "            )\n",
    "\n",
    "            print(f\"    ‚Ä¢ Estimated TensorCore utilization: {tc_utilization:.2%}\")\n",
    "\n",
    "            tensorcore_results[config_name] = {\n",
    "                \"size\": (size_m, size_n),\n",
    "                \"dtype\": str(dtype),\n",
    "                \"f32_time\": f32_time,\n",
    "                \"tc_time\": tc_time,\n",
    "                \"mixed_time\": mixed_time,\n",
    "                \"tc_speedup\": tc_speedup,\n",
    "                \"mixed_speedup\": mixed_speedup,\n",
    "                \"f32_gflops\": f32_gflops,\n",
    "                \"tc_gflops\": tc_gflops,\n",
    "                \"mixed_gflops\": mixed_gflops,\n",
    "                \"tc_utilization\": tc_utilization,\n",
    "            }\n",
    "\n",
    "        # Summary\n",
    "        print(\"\\nüìä TensorCore Optimization Summary:\")\n",
    "        avg_tc_speedup = sum(\n",
    "            r[\"tc_speedup\"] for r in tensorcore_results.values()\n",
    "        ) / len(tensorcore_results)\n",
    "        avg_tc_gflops = sum(r[\"tc_gflops\"] for r in tensorcore_results.values()) / len(\n",
    "            tensorcore_results\n",
    "        )\n",
    "        avg_utilization = sum(\n",
    "            r[\"tc_utilization\"] for r in tensorcore_results.values()\n",
    "        ) / len(tensorcore_results)\n",
    "\n",
    "        print(f\"  ‚Ä¢ Average TensorCore speedup: {avg_tc_speedup:.2f}x\")\n",
    "        print(f\"  ‚Ä¢ Average TensorCore performance: {avg_tc_gflops:.1f} GFLOPS\")\n",
    "        print(f\"  ‚Ä¢ Average TensorCore utilization: {avg_utilization:.2%}\")\n",
    "\n",
    "        if avg_tc_speedup > 2.0:\n",
    "            print(\"  ‚úÖ Excellent TensorCore acceleration!\")\n",
    "        elif avg_tc_speedup > 1.3:\n",
    "            print(\"  ‚úÖ Good TensorCore acceleration\")\n",
    "        elif avg_tc_speedup > 1.1:\n",
    "            print(\"  ‚úÖ Modest TensorCore benefit\")\n",
    "        else:\n",
    "            print(\"  ‚ö†Ô∏è  Limited TensorCore benefit - check hardware compatibility\")\n",
    "\n",
    "        self.results[\"tensorcore_optimization\"] = tensorcore_results\n",
    "        return tensorcore_results\n",
    "\n",
    "    def profile_neural_operators(self):\n",
    "        \"\"\"Profile neural operators with comprehensive analysis.\"\"\"\n",
    "\n",
    "        print(\"\\nüîç Profiling Neural Operators\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # Create operators and data\n",
    "        operators = self.create_neural_operators()\n",
    "        sample_input = self.create_sample_data(batch_size=64, grid_size=64, channels=3)\n",
    "\n",
    "        print(f\"Sample input shape: {sample_input.shape}\")\n",
    "        print(f\"JAX backend: {jax.default_backend()}\")\n",
    "        print(f\"Available devices: {jax.device_count()}\")\n",
    "\n",
    "        operator_results = {}\n",
    "\n",
    "        # Profile each operator\n",
    "        for name, operator in operators.items():\n",
    "            print(f\"\\n--- Profiling {name} ---\")\n",
    "\n",
    "            with self.profiler.profiling_session():\n",
    "                results, report = self.profiler.profile_neural_operator(\n",
    "                    operator, [sample_input], f\"{name}_Profile\"\n",
    "                )\n",
    "\n",
    "                print(f\"{name} Profiling Results:\")\n",
    "                print(report.render(output_format=\"text\"))\n",
    "\n",
    "                operator_results[name] = results\n",
    "\n",
    "        self.results[\"operator_profiling\"] = operator_results\n",
    "        return operator_results\n",
    "\n",
    "    def profile_jax_functions(self):\n",
    "        \"\"\"Profile JAX functions with different characteristics.\"\"\"\n",
    "\n",
    "        print(\"\\nüîß Profiling JAX Functions\")\n",
    "        print(\"=\" * 40)\n",
    "\n",
    "        # Create test data\n",
    "        sample_input = self.create_sample_data(batch_size=64, grid_size=64, channels=3)\n",
    "        flat_input = sample_input.reshape(sample_input.shape[0], -1)\n",
    "\n",
    "        # Define test functions\n",
    "        def matrix_multiply_chain(x):\n",
    "            \"\"\"Example function with multiple matrix operations.\"\"\"\n",
    "            w1 = jnp.ones((x.shape[-1], 128))\n",
    "            w2 = jnp.ones((128, 256))\n",
    "            w3 = jnp.ones((256, 64))\n",
    "\n",
    "            y = x @ w1\n",
    "            y = jax.nn.relu(y)\n",
    "            y = y @ w2\n",
    "            y = jax.nn.relu(y)\n",
    "            return y @ w3\n",
    "\n",
    "        def elementwise_operations(x):\n",
    "            \"\"\"Example function with element-wise operations.\"\"\"\n",
    "            y = jnp.sin(x)\n",
    "            y = jnp.exp(y)\n",
    "            y = jnp.tanh(y)\n",
    "            return jnp.sqrt(jnp.abs(y))\n",
    "\n",
    "        def fused_operations(x):\n",
    "            \"\"\"Example of fused operations for better XLA optimization.\"\"\"\n",
    "            # Fused linear + activation\n",
    "            w1 = jnp.ones((x.shape[-1], 256))\n",
    "            y = jax.nn.gelu(x @ w1)  # Fused matmul + activation\n",
    "\n",
    "            # Fused elementwise chain\n",
    "            y = jax.nn.gelu(jnp.sin(y) + jnp.cos(y))  # Fused elementwise ops\n",
    "\n",
    "            # Fused reduction\n",
    "            return jnp.mean(y, axis=-1, keepdims=True)\n",
    "\n",
    "        function_results = {}\n",
    "\n",
    "        # Profile each function\n",
    "        functions = {\n",
    "            \"MatMul_Chain\": (matrix_multiply_chain, flat_input),\n",
    "            \"Elementwise_Ops\": (elementwise_operations, sample_input),\n",
    "            \"Fused_Operations\": (fused_operations, flat_input),\n",
    "        }\n",
    "\n",
    "        for name, (func, input_data) in functions.items():\n",
    "            print(f\"\\n--- Profiling {name} ---\")\n",
    "\n",
    "            with self.profiler.profiling_session():\n",
    "                results, report = self.profiler.profile_function(\n",
    "                    func, [input_data], name\n",
    "                )\n",
    "\n",
    "                print(f\"{name} Results:\")\n",
    "                print(report.render(output_format=\"text\"))\n",
    "\n",
    "                function_results[name] = results\n",
    "\n",
    "        self.results[\"function_profiling\"] = function_results\n",
    "        return function_results\n",
    "\n",
    "    def demonstrate_batch_size_optimization(self):\n",
    "        \"\"\"Demonstrate systematic batch size optimization.\"\"\"\n",
    "\n",
    "        print(\"\\nüéØ Batch Size Optimization Analysis\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # Create FNO for testing\n",
    "        fno = FourierNeuralOperator(\n",
    "            in_channels=2,\n",
    "            out_channels=1,\n",
    "            hidden_channels=32,\n",
    "            modes=16,\n",
    "            num_layers=2,\n",
    "            rngs=nnx.Rngs(0),\n",
    "        )\n",
    "\n",
    "        # Test different batch sizes (smaller to avoid memory issues)\n",
    "        batch_sizes = [32, 64, 128, 256]  # Reduced from original to avoid OOM\n",
    "        batch_results = {}\n",
    "\n",
    "        for batch_size in batch_sizes:\n",
    "            print(f\"\\nTesting batch size: {batch_size}\")\n",
    "\n",
    "            # Create data in correct format\n",
    "            spatial_size = 32\n",
    "            test_input = jax.random.normal(\n",
    "                jax.random.PRNGKey(42),\n",
    "                (batch_size, 2, spatial_size, spatial_size),\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                with self.profiler.profiling_session():\n",
    "                    results, _ = self.profiler.profile_neural_operator(\n",
    "                        fno,\n",
    "                        [test_input],\n",
    "                        f\"FNO_batch_{batch_size}\",\n",
    "                    )\n",
    "\n",
    "                    # Extract key metrics\n",
    "                    roofline = results.get(\"roofline_analysis\", {})\n",
    "                    hardware = results.get(\"hardware_analysis\", {})\n",
    "                    batch_results[batch_size] = {\n",
    "                        \"efficiency\": roofline.get(\"efficiency\", 0),\n",
    "                        \"arithmetic_intensity\": roofline.get(\"arithmetic_intensity\", 0),\n",
    "                        \"execution_time_ms\": roofline.get(\"actual_time_ms\", 0),\n",
    "                        \"tensorcore_utilization\": hardware.get(\"platform_analysis\", {})\n",
    "                        .get(\"tensorcore_analysis\", {})\n",
    "                        .get(\"tensorcore_utilization\", 0),\n",
    "                        \"bottleneck\": roofline.get(\"bottleneck\", \"unknown\"),\n",
    "                    }\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  Error with batch size {batch_size}: {e}\")\n",
    "                batch_results[batch_size] = {\"error\": str(e)}\n",
    "\n",
    "        # Display results\n",
    "        print(\"\\nüìä Batch Size Analysis Results:\")\n",
    "        print(\n",
    "            f\"{'Batch Size':<12} {'Efficiency':<12} {'TensorCore':<12} {'Intensity':<12} {'Time (ms)':<12}\"\n",
    "        )\n",
    "        print(\"-\" * 72)\n",
    "\n",
    "        for batch_size, metrics in batch_results.items():\n",
    "            if \"error\" not in metrics:\n",
    "                print(\n",
    "                    f\"{batch_size:<12} {metrics['efficiency']:<12.2%} \"\n",
    "                    f\"{metrics['tensorcore_utilization']:<12.2%} \"\n",
    "                    f\"{metrics['arithmetic_intensity']:<12.1f} \"\n",
    "                    f\"{metrics['execution_time_ms']:<12.1f}\"\n",
    "                )\n",
    "            else:\n",
    "                print(f\"{batch_size:<12} {'ERROR':<12} {'N/A':<12} {'N/A':<12} {'N/A'}\")\n",
    "\n",
    "        self.results[\"batch_optimization\"] = batch_results\n",
    "        return batch_results\n",
    "\n",
    "    def demonstrate_hardware_specific_analysis(self):\n",
    "        \"\"\"Demonstrate hardware-specific analysis.\"\"\"\n",
    "\n",
    "        print(\"\\nüîß Hardware-Specific Analysis\")\n",
    "        print(\"=\" * 40)\n",
    "\n",
    "        backend = jax.default_backend()\n",
    "        print(f\"Current backend: {backend}\")\n",
    "\n",
    "        # Create test function\n",
    "        def test_matmul(x, y):\n",
    "            return x @ y\n",
    "\n",
    "        # Test different matrix sizes for hardware alignment\n",
    "        test_cases = [\n",
    "            (128, 128),  # Well-aligned\n",
    "            (256, 256),  # Well-aligned\n",
    "            (127, 127),  # Unaligned\n",
    "        ]\n",
    "\n",
    "        print(f\"\\nTesting matrix multiplication alignment for {backend.upper()}:\")\n",
    "\n",
    "        hardware_results = {}\n",
    "\n",
    "        for m, n in test_cases:\n",
    "            print(f\"\\n--- Matrix size: {m}x{n} ---\")\n",
    "\n",
    "            # Create matrices with mixed precision\n",
    "            dtype = jnp.bfloat16 if backend in [\"gpu\", \"tpu\"] else jnp.float32\n",
    "            a = jnp.ones((m, n), dtype=dtype)\n",
    "            b = jnp.ones((n, m), dtype=dtype)\n",
    "\n",
    "            try:\n",
    "                with self.profiler.profiling_session():\n",
    "                    results, _ = self.profiler.profile_function(\n",
    "                        test_matmul, [a, b], f\"MatMul_{m}x{n}\"\n",
    "                    )\n",
    "\n",
    "                    # Extract hardware-specific metrics\n",
    "                    hw_analysis = results.get(\"hardware_analysis\", {})\n",
    "                    platform_analysis = hw_analysis.get(\"platform_analysis\", {})\n",
    "                    roofline = results.get(\"roofline_analysis\", {})\n",
    "\n",
    "                    if backend == \"gpu\" and \"tensorcore_analysis\" in platform_analysis:\n",
    "                        tc = platform_analysis[\"tensorcore_analysis\"]\n",
    "                        tensorcore_util = tc.get(\"tensorcore_utilization\", 0)\n",
    "                        shape_alignment = tc.get(\"shape_alignment\", {})\n",
    "                        alignment_score = shape_alignment.get(\n",
    "                            \"average_alignment_score\", 0\n",
    "                        )\n",
    "\n",
    "                        print(f\"  TensorCore Utilization: {tensorcore_util:.2%}\")\n",
    "                        print(f\"  Shape Alignment Score: {alignment_score:.2f}\")\n",
    "\n",
    "                    # Show roofline metrics\n",
    "                    print(\n",
    "                        f\"  Arithmetic Intensity: {roofline.get('arithmetic_intensity', 0):.1f} FLOPs/byte\"\n",
    "                    )\n",
    "                    print(f\"  Efficiency: {roofline.get('efficiency', 0):.2%}\")\n",
    "\n",
    "                    hardware_results[f\"{m}x{n}\"] = results\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  Error: {e}\")\n",
    "                hardware_results[f\"{m}x{n}\"] = {\"error\": str(e)}\n",
    "\n",
    "        self.results[\"hardware_analysis\"] = hardware_results\n",
    "        return hardware_results\n",
    "\n",
    "    def compare_operations(self):\n",
    "        \"\"\"Compare multiple operations and identify optimization opportunities.\"\"\"\n",
    "\n",
    "        print(\"\\nüìä Comparing Neural Operators\")\n",
    "        print(\"=\" * 40)\n",
    "\n",
    "        # Create operators and data\n",
    "        operators = self.create_neural_operators()\n",
    "        sample_input = self.create_sample_data(batch_size=64, grid_size=64, channels=3)\n",
    "\n",
    "        # Prepare operations for comparison\n",
    "        operations = []\n",
    "        for name, operator in operators.items():\n",
    "            operations.append((name, operator, [sample_input]))\n",
    "\n",
    "        # Compare operations\n",
    "        comparison_results = self.profiler.compare_operations(operations)\n",
    "\n",
    "        print(\"\\nComparison Results:\")\n",
    "        for rec in comparison_results.get(\"recommendations\", []):\n",
    "            print(f\"  üí° {rec}\")\n",
    "\n",
    "        self.results[\"operation_comparison\"] = comparison_results\n",
    "        return comparison_results\n",
    "\n",
    "    def generate_comprehensive_summary(self):  # noqa: PLR0912, PLR0915\n",
    "        \"\"\"Generate comprehensive summary of all profiling results.\"\"\"\n",
    "\n",
    "        print(\"\\nüéâ Comprehensive Profiling Summary\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        print(\"üìã Completed Analyses:\")\n",
    "        print(\"  ‚úÖ JIT vs Non-JIT performance comparison\")\n",
    "        print(\"  ‚úÖ Compilation overhead analysis\")\n",
    "        print(\"  ‚úÖ Neural operator profiling\")\n",
    "        print(\"  ‚úÖ JAX function profiling\")\n",
    "        print(\"  ‚úÖ Batch size optimization\")\n",
    "        print(\"  ‚úÖ Hardware-specific analysis\")\n",
    "        print(\"  ‚úÖ Operation comparison\")\n",
    "\n",
    "        print(\"\\nüìä Key Performance Insights:\")\n",
    "\n",
    "        # GPU Acceleration Results\n",
    "        if \"memory_pool_efficiency\" in self.results:\n",
    "            pool_data = self.results[\"memory_pool_efficiency\"]\n",
    "            print(f\"  ‚Ä¢ Memory pool efficiency: {pool_data['efficiency']:.2f}x speedup\")\n",
    "            print(\n",
    "                f\"  ‚Ä¢ Buffer reuse ratio: {pool_data['pool_stats']['reuse_ratio']:.2%}\"\n",
    "            )\n",
    "\n",
    "        if \"mixed_precision\" in self.results:\n",
    "            mixed_data = self.results[\"mixed_precision\"]\n",
    "            avg_speedup = sum(r[\"mixed_speedup\"] for r in mixed_data.values()) / len(\n",
    "                mixed_data\n",
    "            )\n",
    "            print(f\"  ‚Ä¢ Mixed precision average speedup: {avg_speedup:.2f}x\")\n",
    "\n",
    "        if \"async_operations\" in self.results:\n",
    "            async_data = self.results[\"async_operations\"]\n",
    "            print(f\"  ‚Ä¢ Async operations speedup: {async_data['speedup']:.2f}x\")\n",
    "\n",
    "        if \"tensorcore_optimization\" in self.results:\n",
    "            tc_data = self.results[\"tensorcore_optimization\"]\n",
    "            avg_tc_speedup = sum(r[\"tc_speedup\"] for r in tc_data.values()) / len(\n",
    "                tc_data\n",
    "            )\n",
    "            avg_tc_gflops = sum(r[\"tc_gflops\"] for r in tc_data.values()) / len(tc_data)\n",
    "            print(f\"  ‚Ä¢ TensorCore average speedup: {avg_tc_speedup:.2f}x\")\n",
    "            print(f\"  ‚Ä¢ TensorCore average performance: {avg_tc_gflops:.1f} GFLOPS\")\n",
    "\n",
    "        # JIT Performance\n",
    "        if \"jit_comparison\" in self.results:\n",
    "            jit_data = self.results[\"jit_comparison\"]\n",
    "            print(f\"  ‚Ä¢ JIT compilation speedup: {jit_data['speedup']:.2f}x\")\n",
    "\n",
    "        # Compilation Overhead\n",
    "        if \"compilation_analysis\" in self.results:\n",
    "            comp_data = self.results[\"compilation_analysis\"]\n",
    "            print(\n",
    "                f\"  ‚Ä¢ Compilation break-even: {comp_data['break_even_calls']:.1f} calls\"\n",
    "            )\n",
    "\n",
    "        # Best performing operator\n",
    "        if \"operator_profiling\" in self.results:\n",
    "            op_data = self.results[\"operator_profiling\"]\n",
    "            best_efficiency = 0\n",
    "            best_operator = \"Unknown\"\n",
    "\n",
    "            for name, results in op_data.items():\n",
    "                roofline = results.get(\"roofline_analysis\", {})\n",
    "                efficiency = roofline.get(\"efficiency\", 0)\n",
    "                if efficiency > best_efficiency:\n",
    "                    best_efficiency = efficiency\n",
    "                    best_operator = name\n",
    "\n",
    "            print(\n",
    "                f\"  ‚Ä¢ Best performing operator: {best_operator} ({best_efficiency:.2%} efficiency)\"\n",
    "            )\n",
    "\n",
    "        # Batch size recommendations\n",
    "        if \"batch_optimization\" in self.results:\n",
    "            batch_data = self.results[\"batch_optimization\"]\n",
    "            successful_batches = {\n",
    "                k: v for k, v in batch_data.items() if \"error\" not in v\n",
    "            }\n",
    "            if successful_batches:\n",
    "                best_batch = max(\n",
    "                    successful_batches.keys(),\n",
    "                    key=lambda k: successful_batches[k][\"efficiency\"],\n",
    "                )\n",
    "                best_efficiency = successful_batches[best_batch][\"efficiency\"]\n",
    "                print(\n",
    "                    f\"  ‚Ä¢ Optimal batch size tested: {best_batch} ({best_efficiency:.2%} efficiency)\"\n",
    "                )\n",
    "\n",
    "        print(\"\\nüí° GPU Acceleration & Optimization Recommendations:\")\n",
    "\n",
    "        # Memory pool recommendations\n",
    "        if \"memory_pool_efficiency\" in self.results:\n",
    "            pool_data = self.results[\"memory_pool_efficiency\"]\n",
    "            if pool_data[\"efficiency\"] > 2.0:\n",
    "                print(\n",
    "                    \"  ‚úÖ Memory pooling provides excellent acceleration - use for repeated allocations\"\n",
    "                )\n",
    "            else:\n",
    "                print(\n",
    "                    \"  ‚ö†Ô∏è  Consider larger buffer sizes or different allocation patterns for memory pooling\"\n",
    "                )\n",
    "\n",
    "        # Mixed precision recommendations\n",
    "        if \"mixed_precision\" in self.results:\n",
    "            mixed_data = self.results[\"mixed_precision\"]\n",
    "            avg_speedup = sum(r[\"mixed_speedup\"] for r in mixed_data.values()) / len(\n",
    "                mixed_data\n",
    "            )\n",
    "            if avg_speedup > 1.2:\n",
    "                print(\n",
    "                    \"  ‚úÖ Mixed precision optimization is beneficial - use for large matrix operations\"\n",
    "                )\n",
    "            else:\n",
    "                print(\n",
    "                    \"  ‚ö†Ô∏è  Mixed precision shows limited benefit - verify TensorCore availability\"\n",
    "                )\n",
    "\n",
    "        # Async operations recommendations\n",
    "        if \"async_operations\" in self.results:\n",
    "            async_data = self.results[\"async_operations\"]\n",
    "            if async_data[\"speedup\"] > 1.1:\n",
    "                print(\n",
    "                    \"  ‚úÖ Async memory operations provide benefit - use prefetching for data pipelines\"\n",
    "                )\n",
    "            else:\n",
    "                print(\"  ‚ö†Ô∏è  Limited async benefit - operations may be compute-bound\")\n",
    "\n",
    "        # General recommendations\n",
    "        print(\"  ‚Ä¢ Use OptimizedGPUManager for comprehensive acceleration\")\n",
    "        print(\n",
    "            \"  ‚Ä¢ JIT compilation provides significant speedup - ensure proper warm-up\"\n",
    "        )\n",
    "        print(\"  ‚Ä¢ Consider compilation overhead for short-running applications\")\n",
    "        print(\n",
    "            \"  ‚Ä¢ Use TensorCore-aligned shapes (multiples of 16) for GPU optimization\"\n",
    "        )\n",
    "        print(\n",
    "            \"  ‚Ä¢ Optimize batch sizes based on roofline analysis and hardware capabilities\"\n",
    "        )\n",
    "        print(\"  ‚Ä¢ Monitor arithmetic intensity and memory bandwidth utilization\")\n",
    "        print(\"  ‚Ä¢ Use buffer donation in JIT functions for memory efficiency\")\n",
    "        print(\"  ‚Ä¢ Implement memory pooling for applications with repeated allocations\")\n",
    "\n",
    "        # Session summary\n",
    "        session_summary = self.profiler.get_session_summary()\n",
    "        print(\"\\nüìà Profiling Session Summary:\")\n",
    "        print(f\"  ‚Ä¢ Total sessions: {session_summary.get('total_sessions', 0)}\")\n",
    "        print(f\"  ‚Ä¢ Success rate: {session_summary.get('success_rate', 0):.2%}\")\n",
    "        print(f\"  ‚Ä¢ Total duration: {session_summary.get('total_duration_s', 0):.2f}s\")\n",
    "        print(f\"  ‚Ä¢ Profilers used: {session_summary.get('profilers_used', [])}\")\n",
    "\n",
    "    def run_comprehensive_demo(self):\n",
    "        \"\"\"Run the complete comprehensive GPU acceleration and profiling demonstration.\"\"\"\n",
    "\n",
    "        print(\"üöÄ Opifex Comprehensive GPU Acceleration & Profiling Demo\")\n",
    "        print(\"=\" * 75)\n",
    "        print(\"This demo showcases advanced GPU acceleration capabilities:\")\n",
    "        print(\"  1. Memory pool efficiency with 8x+ speedup demonstrations\")\n",
    "        print(\"  2. Mixed precision optimization with TensorCore alignment\")\n",
    "        print(\"  3. Asynchronous memory operations with prefetching\")\n",
    "        print(\"  4. Roofline model analysis for performance optimization\")\n",
    "        print(\"  5. JIT vs non-JIT performance comparison\")\n",
    "        print(\"  6. Compilation overhead analysis with break-even calculations\")\n",
    "        print(\"  7. Neural operator profiling with advanced optimizations\")\n",
    "        print(\"  8. JAX function profiling with fusion analysis\")\n",
    "        print(\"  9. Batch size optimization for hardware efficiency\")\n",
    "        print(\" 10. Hardware-specific analysis and TensorCore utilization\")\n",
    "        print(\" 11. Operation comparison with optimization recommendations\")\n",
    "        print(\"=\" * 75)\n",
    "\n",
    "        try:\n",
    "            # Run GPU acceleration demonstrations first\n",
    "            print()\n",
    "            print(\"üéØ GPU ACCELERATION DEMONSTRATIONS\")\n",
    "            print(\"=\" * 50)\n",
    "            self.demonstrate_memory_pool_efficiency()\n",
    "            self.demonstrate_mixed_precision_optimization()\n",
    "            self.demonstrate_tensorcore_optimization()\n",
    "            self.demonstrate_async_memory_operations()\n",
    "            self.demonstrate_roofline_analysis()\n",
    "\n",
    "            # Run traditional profiling analyses\n",
    "            print()\n",
    "            print(\"üìä PERFORMANCE PROFILING ANALYSES\")\n",
    "            print(\"=\" * 50)\n",
    "            self.compare_jit_vs_non_jit()\n",
    "            self.analyze_compilation_overhead()\n",
    "            self.profile_neural_operators()\n",
    "            self.profile_jax_functions()\n",
    "            self.demonstrate_batch_size_optimization()\n",
    "            self.demonstrate_hardware_specific_analysis()\n",
    "            self.compare_operations()\n",
    "\n",
    "            # Generate comprehensive summary\n",
    "            self.generate_comprehensive_summary()\n",
    "\n",
    "            print(\n",
    "                \"\\n‚úÖ Comprehensive GPU acceleration and profiling demo completed successfully!\"\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Demo failed with error: {e}\")\n",
    "            import traceback\n",
    "\n",
    "            traceback.print_exc()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the comprehensive profiling demo.\"\"\"\n",
    "    demo = ComprehensiveProfilingDemo()\n",
    "    demo.run_comprehensive_demo()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "py:percent,ipynb",
   "main_language": "python",
   "notebook_metadata_filter": "kernelspec,language_info,-jupytext.text_representation.jupytext_version,-jupytext.notebook_metadata_filter,-jupytext.cell_metadata_filter"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
