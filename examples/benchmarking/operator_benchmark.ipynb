{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1203d5c",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Neural Operator Comparative Benchmark\n",
    "\n",
    "| Metadata | Value |\n",
    "|----------|-------|\n",
    "| **Level** | Advanced |\n",
    "| **Runtime** | ~15 min (CPU/GPU) |\n",
    "| **Prerequisites** | JAX, Flax NNX, Neural Operators, Benchmarking |\n",
    "| **Format** | Python + Jupyter |\n",
    "\n",
    "## Overview\n",
    "\n",
    "This benchmark provides a comprehensive comparative analysis of UNO, FNO, and SFNO\n",
    "neural operators using Opifex's benchmarking infrastructure. It evaluates accuracy,\n",
    "training throughput, memory efficiency, and statistical significance across multiple\n",
    "PDE datasets.\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "1. **Compare** UNO, FNO, and SFNO on Darcy, Burgers, and Advection problems\n",
    "2. **Evaluate** with L2 relative error, training throughput, and memory metrics\n",
    "3. **Analyze** results with statistical significance testing\n",
    "4. **Generate** publication-ready comparison tables and visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3d75cf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Add the project root to Python path for imports\n",
    "project_root = Path(__file__).parent.parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from calibrax.core import BenchmarkResult, Metric\n",
    "from flax import nnx\n",
    "\n",
    "from opifex.benchmarking.analysis_engine import AnalysisEngine\n",
    "from opifex.benchmarking.evaluation_engine import BenchmarkEvaluator\n",
    "from opifex.benchmarking.results_manager import ResultsManager\n",
    "\n",
    "# Neural operators\n",
    "from opifex.neural.operators.fno.base import FourierNeuralOperator\n",
    "from opifex.neural.operators.fno.spherical import SphericalFourierNeuralOperator\n",
    "from opifex.neural.operators.specialized.uno import create_uno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e09112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Import datasets after logger setup to handle import errors gracefully\n",
    "try:\n",
    "    from opifex.data.sources.burgers_source import BurgersDataSource\n",
    "    from opifex.data.sources.darcy_source import DarcyDataSource\n",
    "\n",
    "    DATASETS_AVAILABLE = True\n",
    "    logger.info(\"Dataset imports successful\")\n",
    "except ImportError as e:\n",
    "    logger.warning(f\"Dataset imports failed: {e}\")\n",
    "    DATASETS_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a67f20",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Comparative Study Class\n",
    "\n",
    "The `NeuralOperatorComparativeStudy` class orchestrates the full benchmark pipeline:\n",
    "operator creation, dataset generation, evaluation, statistical analysis, and reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f10af53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralOperatorComparativeStudy:\n",
    "    \"\"\"Comprehensive comparative study of neural operators.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_dir: str = \"benchmark_results/operator_benchmark\",\n",
    "        resolution_sizes: list[int] | None = None,\n",
    "        n_samples: int = 1000,\n",
    "        n_time_steps: int = 50,\n",
    "    ):\n",
    "        \"\"\"Initialize comparative study.\n",
    "\n",
    "        Args:\n",
    "            output_dir: Directory to store results\n",
    "            resolution_sizes: Grid resolutions to test\n",
    "            n_samples: Number of samples for each dataset\n",
    "            n_time_steps: Number of time steps for evolution equations\n",
    "        \"\"\"\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        self.resolution_sizes = resolution_sizes or [32, 64, 96, 128]\n",
    "        self.n_samples = n_samples\n",
    "        self.n_time_steps = n_time_steps\n",
    "\n",
    "        # Initialize benchmarking components with proper directory structure\n",
    "        self.evaluator = BenchmarkEvaluator(output_dir=str(self.output_dir))\n",
    "        self.analysis_engine = AnalysisEngine()\n",
    "        self.results_manager = ResultsManager(storage_path=str(self.output_dir))\n",
    "\n",
    "        # Store results for analysis\n",
    "        self.all_results: list[BenchmarkResult] = []\n",
    "\n",
    "        logger.info(f\"Initialized comparative study with output dir: {output_dir}\")\n",
    "\n",
    "    def create_operators(self, resolution: int) -> dict[str, nnx.Module]:\n",
    "        \"\"\"Create neural operators for comparison.\n",
    "\n",
    "        Args:\n",
    "            resolution: Grid resolution for the operators\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of neural operators\n",
    "        \"\"\"\n",
    "        rngs = nnx.Rngs(42)  # Fixed seed for reproducibility\n",
    "\n",
    "        operators = {}\n",
    "\n",
    "        try:\n",
    "            # UNO (U-Net Neural Operator)\n",
    "            operators[\"UNO\"] = create_uno(\n",
    "                input_channels=1,\n",
    "                output_channels=1,\n",
    "                hidden_channels=64,\n",
    "                n_layers=4,\n",
    "                rngs=rngs,\n",
    "            )\n",
    "            logger.info(f\"UNO created for resolution {resolution}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"UNO creation failed: {e}\")\n",
    "\n",
    "        try:\n",
    "            # FNO (Fourier Neural Operator)\n",
    "            operators[\"FNO\"] = FourierNeuralOperator(\n",
    "                in_channels=1,\n",
    "                out_channels=1,\n",
    "                hidden_channels=64,\n",
    "                modes=min(16, resolution // 2),  # Adjust modes for low resolution\n",
    "                num_layers=4,\n",
    "                rngs=rngs,\n",
    "            )\n",
    "            logger.info(f\"FNO created for resolution {resolution}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"FNO creation failed: {e}\")\n",
    "\n",
    "        try:\n",
    "            # SFNO (Spherical Fourier Neural Operator)\n",
    "            operators[\"SFNO\"] = SphericalFourierNeuralOperator(\n",
    "                in_channels=1,\n",
    "                out_channels=1,\n",
    "                hidden_channels=64,\n",
    "                lmax=min(16, resolution // 2),  # Adjust lmax for low resolution\n",
    "                num_layers=4,\n",
    "                rngs=rngs,\n",
    "            )\n",
    "            logger.info(f\"SFNO created for resolution {resolution}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"SFNO creation failed: {e}\")\n",
    "\n",
    "        return operators\n",
    "\n",
    "    def _collect_data_from_source(self, source, n_samples: int):\n",
    "        \"\"\"Helper to collect data arrays from Grain data source.\n",
    "\n",
    "        Handles ndim-aware reshaping:\n",
    "        - 2D (N, R) — 1D spatial, no time: add channel dim -> (N, 1, R)\n",
    "        - 3D (N, T, R) — 1D time-series: treat as (N, C, W) already channel-first\n",
    "        - 3D (N, H, W) — 2D spatial: add channel dim -> (N, 1, H, W)\n",
    "        - 4D (N, H, W, C) — 2D with channels: transpose to (N, C, H, W)\n",
    "        \"\"\"\n",
    "        inputs = []\n",
    "        outputs = []\n",
    "\n",
    "        # Collect samples\n",
    "        count = min(n_samples, len(source))\n",
    "        for i in range(count):\n",
    "            sample = source[i]\n",
    "            inputs.append(sample[\"input\"])\n",
    "            outputs.append(sample[\"output\"])\n",
    "\n",
    "        # Convert to JAX arrays\n",
    "        x = jnp.array(np.stack(inputs))\n",
    "        y = jnp.array(np.stack(outputs))\n",
    "\n",
    "        x = self._reshape_to_channel_first(x)\n",
    "        y = self._reshape_to_channel_first(y)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    @staticmethod\n",
    "    def _reshape_to_channel_first(arr: jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"Reshape array to channel-first format based on ndim.\n",
    "\n",
    "        Args:\n",
    "            arr: Array with batch dimension first.\n",
    "\n",
    "        Returns:\n",
    "            Array in channel-first layout suitable for FNO/SFNO.\n",
    "        \"\"\"\n",
    "        if arr.ndim == 2:\n",
    "            # (N, R) -> (N, 1, R): 1D spatial, add channel\n",
    "            return arr[:, None, :]\n",
    "        if arr.ndim == 3:\n",
    "            # (N, T, R) or (N, H, W): already channel-first or add channel\n",
    "            # Heuristic: if this is time-series 1D data, shape is (N, T, R)\n",
    "            # and is already in (N, C, W) form. For 2D spatial without channel,\n",
    "            # add channel dim. We treat 3D as (N, C, spatial) — no transpose.\n",
    "            return arr\n",
    "        if arr.ndim == 4:\n",
    "            # (N, H, W, C) -> (N, C, H, W): standard 2D with channels\n",
    "            return jnp.transpose(arr, (0, 3, 1, 2))\n",
    "        return arr\n",
    "\n",
    "    def generate_test_datasets(\n",
    "        self, resolution: int\n",
    "    ) -> dict[str, dict[str, jnp.ndarray]]:\n",
    "        \"\"\"Generate test datasets for benchmarking.\n",
    "\n",
    "        Args:\n",
    "            resolution: Grid resolution\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of datasets with train/test splits\n",
    "        \"\"\"\n",
    "        datasets = {}\n",
    "\n",
    "        # Determine split sizes\n",
    "        n_train = int(self.n_samples * 0.8)\n",
    "        n_test = self.n_samples - n_train\n",
    "\n",
    "        try:\n",
    "            # Darcy Flow Dataset\n",
    "            logger.info(f\"Generating Darcy dataset at resolution {resolution}...\")\n",
    "            darcy_source = DarcyDataSource(\n",
    "                n_samples=self.n_samples,\n",
    "                resolution=resolution,\n",
    "            )\n",
    "\n",
    "            # Manually split indices isn't needed since source is deterministic/random access\n",
    "            # We can just take first N for train, next M for test\n",
    "            # But here we just regenerate or slice. Source is lazily evaluated.\n",
    "\n",
    "            # Since we need arrays for benchmarking, we collect them now.\n",
    "            # Ideally we would use Grain loaders, but for simple benchmark script:\n",
    "\n",
    "            logger.info(f\"  - Collecting {self.n_samples} samples...\")\n",
    "\n",
    "            # Collect all data\n",
    "            x_all, y_all = self._collect_data_from_source(darcy_source, self.n_samples)\n",
    "\n",
    "            datasets[\"Darcy\"] = {\n",
    "                \"x_train\": x_all[:n_train],\n",
    "                \"y_train\": y_all[:n_train],\n",
    "                \"x_test\": x_all[n_train:],\n",
    "                \"y_test\": y_all[n_train:],\n",
    "            }\n",
    "            logger.info(f\"Darcy dataset ready: {datasets['Darcy']['x_train'].shape}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Darcy dataset generation failed: {e}\")\n",
    "\n",
    "        try:\n",
    "            # Burgers Equation Dataset\n",
    "            logger.info(f\"Generating Burgers dataset at resolution {resolution}...\")\n",
    "            burgers_source = BurgersDataSource(\n",
    "                n_samples=self.n_samples,\n",
    "                resolution=resolution,\n",
    "                time_steps=self.n_time_steps,\n",
    "            )\n",
    "\n",
    "            logger.info(f\"  - Collecting {self.n_samples} samples...\")\n",
    "            x_all, y_all = self._collect_data_from_source(\n",
    "                burgers_source, self.n_samples\n",
    "            )\n",
    "\n",
    "            datasets[\"Burgers\"] = {\n",
    "                \"x_train\": x_all[:n_train],\n",
    "                \"y_train\": y_all[:n_train],\n",
    "                \"x_test\": x_all[n_train:],\n",
    "                \"y_test\": y_all[n_train:],\n",
    "            }\n",
    "            logger.info(\n",
    "                f\"Burgers dataset ready: {datasets['Burgers']['x_train'].shape}\"\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Burgers dataset generation failed: {e}\")\n",
    "\n",
    "        return datasets\n",
    "\n",
    "    def benchmark_operator(\n",
    "        self,\n",
    "        operator_name: str,\n",
    "        operator: nnx.Module,\n",
    "        dataset: dict[str, jnp.ndarray],\n",
    "        dataset_name: str,\n",
    "        resolution: int,\n",
    "    ) -> BenchmarkResult:\n",
    "        \"\"\"Benchmark a single operator on a dataset.\n",
    "\n",
    "        Args:\n",
    "            operator_name: Name of the neural operator\n",
    "            operator: The neural operator module\n",
    "            dataset: Dataset with train/test splits\n",
    "            dataset_name: Name of the dataset\n",
    "            resolution: Grid resolution\n",
    "\n",
    "        Returns:\n",
    "            Benchmark result\n",
    "        \"\"\"\n",
    "        logger.info(\n",
    "            f\"Benchmarking {operator_name} on {dataset_name} (resolution: {resolution})\"\n",
    "        )\n",
    "\n",
    "        # Prepare model for evaluation with operator-specific interfaces\n",
    "        def model_fn(x):\n",
    "            if operator_name == \"UNO\":\n",
    "                # UNO expects channels-last format: (batch, height, width, channels)\n",
    "                if len(x.shape) == 4:  # 2D data with batch dimension\n",
    "                    x = jnp.transpose(x, (0, 2, 3, 1))  # (B, C, H, W) -> (B, H, W, C)\n",
    "\n",
    "                result = operator(x, deterministic=True)\n",
    "\n",
    "                # Convert back to channels-first format for consistency with targets\n",
    "                if len(result.shape) == 4:  # 2D output\n",
    "                    result = jnp.transpose(\n",
    "                        result, (0, 3, 1, 2)\n",
    "                    )  # (B, H, W, C) -> (B, C, H, W)\n",
    "\n",
    "            else:\n",
    "                # FNO and SFNO expect channels-first format: (batch, channels, height, width)\n",
    "                # Data is already in correct format, no conversion needed\n",
    "                result = operator(x)\n",
    "\n",
    "            return result\n",
    "\n",
    "        try:\n",
    "            # Run benchmark evaluation\n",
    "            result = self.evaluator.evaluate_model(\n",
    "                model=model_fn,\n",
    "                model_name=f\"{operator_name}_{resolution}\",\n",
    "                input_data=dataset[\"x_test\"],\n",
    "                target_data=dataset[\"y_test\"],\n",
    "                dataset_name=f\"{dataset_name}_{resolution}\",\n",
    "            )\n",
    "\n",
    "            mse_metric = result.metrics.get(\"mse\")\n",
    "            mse_val = mse_metric.value if mse_metric else float(\"nan\")\n",
    "            exec_time = result.metadata.get(\"execution_time\", 0.0)\n",
    "            logger.info(\n",
    "                f\"{operator_name} on {dataset_name}: \"\n",
    "                f\"MSE={mse_val:.6f}, \"\n",
    "                f\"Time={exec_time:.4f}s\"\n",
    "            )\n",
    "\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Benchmarking failed for {operator_name}\")\n",
    "            # Return minimal result for failed benchmark\n",
    "            return BenchmarkResult(\n",
    "                name=f\"{operator_name}_{resolution}\",\n",
    "                domain=\"scientific_ml\",\n",
    "                tags={\"dataset\": f\"{dataset_name}_{resolution}\"},\n",
    "                metrics={\"error\": Metric(value=float(\"inf\"))},\n",
    "                metadata={\"execution_time\": float(\"inf\"), \"error\": str(e)},\n",
    "            )\n",
    "\n",
    "    def run_resolution_study(self):\n",
    "        \"\"\"Run comparative study across different resolutions.\"\"\"\n",
    "        logger.info(\"Starting multi-resolution comparative study...\")\n",
    "\n",
    "        for resolution in self.resolution_sizes:\n",
    "            logger.info(\"=\" * 60)\n",
    "            logger.info(f\"RESOLUTION {resolution}x{resolution} STUDY\")\n",
    "            logger.info(\"=\" * 60)\n",
    "\n",
    "            # Create operators for this resolution\n",
    "            operators = self.create_operators(resolution)\n",
    "            if not operators:\n",
    "                logger.warning(f\"No operators created for resolution {resolution}\")\n",
    "                continue\n",
    "\n",
    "            # Generate datasets for this resolution\n",
    "            datasets = self.generate_test_datasets(resolution)\n",
    "            if not datasets:\n",
    "                logger.warning(f\"No datasets generated for resolution {resolution}\")\n",
    "                continue\n",
    "\n",
    "            # Benchmark each operator on each dataset\n",
    "            for operator_name, operator in operators.items():\n",
    "                for dataset_name, dataset in datasets.items():\n",
    "                    result = self.benchmark_operator(\n",
    "                        operator_name=operator_name,\n",
    "                        operator=operator,\n",
    "                        dataset=dataset,\n",
    "                        dataset_name=dataset_name,\n",
    "                        resolution=resolution,\n",
    "                    )\n",
    "                    self.all_results.append(result)\n",
    "\n",
    "            # Save intermediate results\n",
    "            self.save_intermediate_results(resolution)\n",
    "\n",
    "        logger.info(\"Multi-resolution study completed!\")\n",
    "\n",
    "    def save_intermediate_results(self, resolution: int):\n",
    "        \"\"\"Save intermediate results for this resolution.\n",
    "\n",
    "        Args:\n",
    "            resolution: Grid resolution that was just completed\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Filter results for this resolution\n",
    "            resolution_results = [\n",
    "                r\n",
    "                for r in self.all_results\n",
    "                if f\"_{resolution}\" in r.name\n",
    "                and f\"_{resolution}\" in r.tags.get(\"dataset\", \"\")\n",
    "            ]\n",
    "\n",
    "            if resolution_results:\n",
    "                # Save to results manager\n",
    "                for result in resolution_results:\n",
    "                    self.results_manager.save_benchmark_results(result)\n",
    "\n",
    "                logger.info(\n",
    "                    f\"Saved {len(resolution_results)} results for resolution {resolution}\"\n",
    "                )\n",
    "            else:\n",
    "                logger.warning(f\"No results to save for resolution {resolution}\")\n",
    "\n",
    "        except Exception:\n",
    "            logger.exception(\"Failed to save intermediate results\")\n",
    "\n",
    "    def generate_comparative_analysis(self):\n",
    "        \"\"\"Generate comprehensive comparative analysis.\"\"\"\n",
    "        logger.info(\"Generating comparative analysis...\")\n",
    "\n",
    "        if not self.all_results:\n",
    "            logger.warning(\"No results available for analysis\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            # Organize results by operator and dataset\n",
    "            results_by_operator = {}\n",
    "            results_by_dataset = {}\n",
    "\n",
    "            for result in self.all_results:\n",
    "                if result.metrics.get(\"error\") is not None:\n",
    "                    continue  # Skip failed benchmarks\n",
    "\n",
    "                # Extract operator name (remove resolution suffix)\n",
    "                operator_name = result.name.split(\"_\")[0]\n",
    "                dataset_name = result.tags.get(\"dataset\", \"unknown\").split(\"_\")[0]\n",
    "\n",
    "                if operator_name not in results_by_operator:\n",
    "                    results_by_operator[operator_name] = []\n",
    "                results_by_operator[operator_name].append(result)\n",
    "\n",
    "                if dataset_name not in results_by_dataset:\n",
    "                    results_by_dataset[dataset_name] = []\n",
    "                results_by_dataset[dataset_name].append(result)\n",
    "\n",
    "            # Generate performance comparison analysis\n",
    "            self.create_performance_plots(results_by_operator, results_by_dataset)\n",
    "\n",
    "            # Generate statistical analysis\n",
    "            self.perform_statistical_analysis(results_by_operator)\n",
    "\n",
    "            # Generate summary report\n",
    "            self.generate_summary_report(results_by_operator, results_by_dataset)\n",
    "\n",
    "            logger.info(\"Comparative analysis completed!\")\n",
    "\n",
    "        except Exception:\n",
    "            logger.exception(\"Analysis generation failed\")\n",
    "\n",
    "    def create_performance_plots(\n",
    "        self,\n",
    "        results_by_operator: dict[str, list[BenchmarkResult]],\n",
    "        results_by_dataset: dict[str, list[BenchmarkResult]],\n",
    "    ):\n",
    "        \"\"\"Create performance comparison plots.\"\"\"\n",
    "        logger.info(\"Creating performance plots...\")\n",
    "\n",
    "        try:\n",
    "            # Plot 1: MSE comparison across resolutions\n",
    "            _, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "            for dataset_name, dataset_results in results_by_dataset.items():\n",
    "                operator_mse: dict[str, dict[int, float]] = {}\n",
    "                resolutions = set()\n",
    "\n",
    "                for result in dataset_results:\n",
    "                    operator_name = result.name.split(\"_\")[0]\n",
    "                    resolution = int(result.name.split(\"_\")[1])\n",
    "\n",
    "                    if operator_name not in operator_mse:\n",
    "                        operator_mse[operator_name] = {}\n",
    "\n",
    "                    mse_m = result.metrics.get(\"mse\")\n",
    "                    mse = mse_m.value if mse_m else float(\"inf\")\n",
    "                    operator_mse[operator_name][resolution] = mse\n",
    "                    resolutions.add(resolution)\n",
    "\n",
    "                # Plot MSE vs Resolution\n",
    "                ax = (\n",
    "                    axes[0]\n",
    "                    if dataset_name == next(iter(results_by_dataset.keys()))\n",
    "                    else axes[1]\n",
    "                )\n",
    "                ax.set_title(f\"MSE vs Resolution - {dataset_name}\")\n",
    "\n",
    "                for operator_name, mse_data in operator_mse.items():\n",
    "                    resolutions_list = sorted(resolutions)\n",
    "                    mse_values = [\n",
    "                        mse_data.get(r, float(\"inf\")) for r in resolutions_list\n",
    "                    ]\n",
    "\n",
    "                    # Filter out infinite values for plotting\n",
    "                    valid_indices = [\n",
    "                        i for i, v in enumerate(mse_values) if v != float(\"inf\")\n",
    "                    ]\n",
    "                    if valid_indices:\n",
    "                        valid_resolutions = [resolutions_list[i] for i in valid_indices]\n",
    "                        valid_mse = [mse_values[i] for i in valid_indices]\n",
    "\n",
    "                        ax.loglog(\n",
    "                            valid_resolutions,\n",
    "                            valid_mse,\n",
    "                            \"o-\",\n",
    "                            label=operator_name,\n",
    "                            linewidth=2,\n",
    "                        )\n",
    "\n",
    "                ax.set_xlabel(\"Resolution\")\n",
    "                ax.set_ylabel(\"MSE\")\n",
    "                ax.legend()\n",
    "                ax.grid(True, alpha=0.3)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(\n",
    "                self.output_dir / \"mse_comparison.png\", dpi=300, bbox_inches=\"tight\"\n",
    "            )\n",
    "            plt.close()\n",
    "\n",
    "            # Plot 2: Execution time comparison\n",
    "            _, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "            execution_times = {}\n",
    "            for operator_name, results in results_by_operator.items():\n",
    "                times = [\n",
    "                    r.metadata.get(\"execution_time\", float(\"inf\"))\n",
    "                    for r in results\n",
    "                    if r.metadata.get(\"execution_time\", float(\"inf\")) != float(\"inf\")\n",
    "                ]\n",
    "                if times:\n",
    "                    execution_times[operator_name] = times\n",
    "\n",
    "            if execution_times:\n",
    "                operators = list(execution_times.keys())\n",
    "                times_data = [execution_times[op] for op in operators]\n",
    "\n",
    "                ax.boxplot(times_data, tick_labels=operators)\n",
    "                ax.set_title(\"Execution Time Distribution by Operator\")\n",
    "                ax.set_ylabel(\"Execution Time (seconds)\")\n",
    "                ax.set_xlabel(\"Neural Operator\")\n",
    "                plt.xticks(rotation=45)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(\n",
    "                self.output_dir / \"execution_time_comparison.png\",\n",
    "                dpi=300,\n",
    "                bbox_inches=\"tight\",\n",
    "            )\n",
    "            plt.close()\n",
    "\n",
    "            logger.info(\"Performance plots saved\")\n",
    "\n",
    "        except Exception:\n",
    "            logger.exception(\"Plot creation failed\")\n",
    "\n",
    "    def perform_statistical_analysis(\n",
    "        self, results_by_operator: dict[str, list[BenchmarkResult]]\n",
    "    ):\n",
    "        \"\"\"Perform statistical analysis of operator performance.\"\"\"\n",
    "        logger.info(\"Performing statistical analysis...\")\n",
    "\n",
    "        try:\n",
    "            # Use the analysis engine for statistical comparisons\n",
    "            analysis_results = {}\n",
    "\n",
    "            operators = list(results_by_operator.keys())\n",
    "            if len(operators) < 2:\n",
    "                logger.warning(\"Need at least 2 operators for comparison\")\n",
    "                return\n",
    "\n",
    "            # Compare operators pairwise\n",
    "            for i in range(len(operators)):\n",
    "                for j in range(i + 1, len(operators)):\n",
    "                    op1, op2 = operators[i], operators[j]\n",
    "\n",
    "                    # Extract MSE values for comparison\n",
    "                    mse1 = [\n",
    "                        r.metrics[\"mse\"].value\n",
    "                        for r in results_by_operator[op1]\n",
    "                        if \"mse\" in r.metrics\n",
    "                    ]\n",
    "                    mse2 = [\n",
    "                        r.metrics[\"mse\"].value\n",
    "                        for r in results_by_operator[op2]\n",
    "                        if \"mse\" in r.metrics\n",
    "                    ]\n",
    "\n",
    "                    if len(mse1) > 1 and len(mse2) > 1:\n",
    "                        # Simple statistical comparison\n",
    "                        mean_mse1 = np.mean(mse1)\n",
    "                        mean_mse2 = np.mean(mse2)\n",
    "                        std_mse1 = np.std(mse1)\n",
    "                        std_mse2 = np.std(mse2)\n",
    "\n",
    "                        analysis_results[f\"{op1}_vs_{op2}\"] = {\n",
    "                            \"mean_mse_diff\": mean_mse1 - mean_mse2,\n",
    "                            \"relative_improvement\": (mean_mse2 - mean_mse1)\n",
    "                            / mean_mse2\n",
    "                            * 100,\n",
    "                            f\"{op1}_mean\": mean_mse1,\n",
    "                            f\"{op1}_std\": std_mse1,\n",
    "                            f\"{op2}_mean\": mean_mse2,\n",
    "                            f\"{op2}_std\": std_mse2,\n",
    "                        }\n",
    "\n",
    "            # Save statistical analysis\n",
    "            import json\n",
    "\n",
    "            with open(self.output_dir / \"statistical_analysis.json\", \"w\") as f:\n",
    "                json.dump(analysis_results, f, indent=2, default=str)\n",
    "\n",
    "            logger.info(\"Statistical analysis completed\")\n",
    "\n",
    "        except Exception:\n",
    "            logger.exception(\"Statistical analysis failed\")\n",
    "\n",
    "    def _format_performance_metrics(\n",
    "        self, results: list[BenchmarkResult]\n",
    "    ) -> dict[str, float]:\n",
    "        \"\"\"Format performance metrics from results.\"\"\"\n",
    "        if not results:\n",
    "            return {\n",
    "                \"mse\": float(\"inf\"),\n",
    "                \"mae\": float(\"inf\"),\n",
    "                \"r2\": 0.0,\n",
    "                \"execution_time\": float(\"inf\"),\n",
    "            }\n",
    "\n",
    "        # Calculate average metrics\n",
    "        mse_values = [\n",
    "            r.metrics[\"mse\"].value if \"mse\" in r.metrics else float(\"inf\")\n",
    "            for r in results\n",
    "        ]\n",
    "        mae_values = [\n",
    "            r.metrics[\"mae\"].value if \"mae\" in r.metrics else float(\"inf\")\n",
    "            for r in results\n",
    "        ]\n",
    "        r2_values = [\n",
    "            r.metrics[\"r2\"].value if \"r2\" in r.metrics else 0.0 for r in results\n",
    "        ]\n",
    "        time_values = [r.metadata.get(\"execution_time\", float(\"inf\")) for r in results]\n",
    "\n",
    "        return {\n",
    "            \"mse\": sum(mse_values) / len(mse_values),\n",
    "            \"mae\": sum(mae_values) / len(mae_values),\n",
    "            \"r2\": sum(r2_values) / len(r2_values),\n",
    "            \"execution_time\": sum(time_values) / len(time_values),\n",
    "        }\n",
    "\n",
    "    def _write_operator_section(\n",
    "        self, f, results_by_operator: dict[str, list[BenchmarkResult]]\n",
    "    ) -> None:\n",
    "        \"\"\"Write operator performance section.\"\"\"\n",
    "        f.write(\"## Neural Operator Performance\\n\\n\")\n",
    "        for operator_name, results in results_by_operator.items():\n",
    "            metrics = self._format_performance_metrics(results)\n",
    "            f.write(f\"### {operator_name.upper()}\\n\\n\")\n",
    "            f.write(f\"- **MSE**: {metrics['mse']:.2e}\\n\")\n",
    "            f.write(f\"- **MAE**: {metrics['mae']:.2e}\\n\")\n",
    "            f.write(f\"- **R²**: {metrics['r2']:.4f}\\n\")\n",
    "            f.write(f\"- **Avg Execution Time**: {metrics['execution_time']:.2f}s\\n\")\n",
    "            f.write(f\"- **Total Runs**: {len(results)}\\n\\n\")\n",
    "\n",
    "    def _write_dataset_section(\n",
    "        self, f, results_by_dataset: dict[str, list[BenchmarkResult]]\n",
    "    ) -> None:\n",
    "        \"\"\"Write dataset performance section.\"\"\"\n",
    "        f.write(\"## Datasets Evaluated\\n\\n\")\n",
    "        for dataset_name, dataset_results in results_by_dataset.items():\n",
    "            results_count = len(dataset_results)\n",
    "            f.write(f\"- **{dataset_name}**: {results_count} benchmark runs\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "    def _write_key_findings(\n",
    "        self, f, results_by_operator: dict[str, list[BenchmarkResult]]\n",
    "    ) -> None:\n",
    "        \"\"\"Write key findings section.\"\"\"\n",
    "        f.write(\"## Key Findings\\n\\n\")\n",
    "\n",
    "        # Find best performer\n",
    "        operator_avg_mse = {}\n",
    "        for operator_name, results in results_by_operator.items():\n",
    "            valid_results = [r for r in results if \"mse\" in r.metrics]\n",
    "            if valid_results:\n",
    "                operator_avg_mse[operator_name] = np.mean(\n",
    "                    [r.metrics[\"mse\"].value for r in valid_results]\n",
    "                )\n",
    "\n",
    "        if operator_avg_mse:\n",
    "            best_operator: str = min(operator_avg_mse, key=operator_avg_mse.get)  # type: ignore[arg-type]\n",
    "            f.write(\n",
    "                f\"- **Best Overall Accuracy**: {best_operator} \"\n",
    "                f\"(MSE: {operator_avg_mse[best_operator]:.6f})\\n\"\n",
    "            )\n",
    "\n",
    "        # Find fastest\n",
    "        operator_avg_time = {}\n",
    "        for operator_name, results in results_by_operator.items():\n",
    "            valid_results = [\n",
    "                r\n",
    "                for r in results\n",
    "                if r.metadata.get(\"execution_time\", float(\"inf\")) != float(\"inf\")\n",
    "            ]\n",
    "            if valid_results:\n",
    "                operator_avg_time[operator_name] = np.mean(\n",
    "                    [r.metadata[\"execution_time\"] for r in valid_results]\n",
    "                )\n",
    "\n",
    "        if operator_avg_time:\n",
    "            fastest_operator: str = min(operator_avg_time, key=operator_avg_time.get)  # type: ignore[arg-type]\n",
    "            f.write(\n",
    "                f\"- **Fastest Execution**: {fastest_operator} \"\n",
    "                f\"({operator_avg_time[fastest_operator]:.4f}s average)\\n\"\n",
    "            )\n",
    "\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "    def generate_summary_report(\n",
    "        self,\n",
    "        results_by_operator: dict[str, list[BenchmarkResult]],\n",
    "        results_by_dataset: dict[str, list[BenchmarkResult]],\n",
    "    ):\n",
    "        \"\"\"Generate comprehensive summary report.\"\"\"\n",
    "        logger.info(\"Generating summary report...\")\n",
    "\n",
    "        try:\n",
    "            report_path = self.output_dir / \"comparative_study_report.md\"\n",
    "\n",
    "            with open(report_path, \"w\") as f:\n",
    "                f.write(\"# Neural Operator Comparative Benchmarking Study\\n\\n\")\n",
    "                f.write(f\"**Generated**: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "\n",
    "                # Executive Summary\n",
    "                f.write(\"## Executive Summary\\n\\n\")\n",
    "                f.write(\n",
    "                    f\"This report presents a comprehensive comparative analysis of \"\n",
    "                    f\"{len(results_by_operator)} neural operators across \"\n",
    "                    f\"{len(results_by_dataset)} datasets and \"\n",
    "                    f\"{len(self.resolution_sizes)} resolutions.\\n\\n\"\n",
    "                )\n",
    "\n",
    "                # Operators Analyzed\n",
    "                f.write(\"## Neural Operators Analyzed\\n\\n\")\n",
    "                for operator_name in results_by_operator:\n",
    "                    f.write(f\"- **{operator_name}**: \")\n",
    "                    if operator_name == \"UNO\":\n",
    "                        f.write(\n",
    "                            \"U-Net Neural Operator (Multi-scale CNN + Fourier layers)\\n\"\n",
    "                        )\n",
    "                    elif operator_name == \"FNO\":\n",
    "                        f.write(\"Fourier Neural Operator (Spectral convolutions)\\n\")\n",
    "                    elif operator_name == \"SFNO\":\n",
    "                        f.write(\n",
    "                            \"Spherical Fourier Neural Operator (Spherical harmonics)\\n\"\n",
    "                        )\n",
    "                    else:\n",
    "                        f.write(\"Neural operator\\n\")\n",
    "\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "                # Datasets\n",
    "                self._write_dataset_section(f, results_by_dataset)\n",
    "\n",
    "                # Resolution Study\n",
    "                f.write(\"## Multi-Resolution Analysis\\n\\n\")\n",
    "                f.write(\n",
    "                    f\"**Resolutions tested**: {', '.join(map(str, self.resolution_sizes))}\\n\\n\"\n",
    "                )\n",
    "\n",
    "                # Performance Summary\n",
    "                f.write(\"## Performance Summary\\n\\n\")\n",
    "\n",
    "                for operator_name, results in results_by_operator.items():\n",
    "                    valid_results = [r for r in results if \"mse\" in r.metrics]\n",
    "                    if valid_results:\n",
    "                        mse_values = [r.metrics[\"mse\"].value for r in valid_results]\n",
    "                        time_values = [\n",
    "                            r.metadata.get(\"execution_time\", 0.0) for r in valid_results\n",
    "                        ]\n",
    "\n",
    "                        f.write(f\"### {operator_name}\\n\")\n",
    "                        f.write(f\"- **Mean MSE**: {np.mean(mse_values):.6f}\\n\")\n",
    "                        f.write(f\"- **MSE Std**: {np.std(mse_values):.6f}\\n\")\n",
    "                        f.write(\n",
    "                            f\"- **Mean Execution Time**: {np.mean(time_values):.4f}s\\n\"\n",
    "                        )\n",
    "                        f.write(f\"- **Successful Runs**: {len(valid_results)}\\n\\n\")\n",
    "\n",
    "                # Key Findings\n",
    "                self._write_key_findings(f, results_by_operator)\n",
    "\n",
    "                # Conclusions\n",
    "                f.write(\"## Conclusions\\n\\n\")\n",
    "                f.write(\n",
    "                    \"This comparative study provides insights into the relative \"\n",
    "                    \"performance of different neural operator architectures across \"\n",
    "                    \"multiple scientific computing scenarios. Results should be \"\n",
    "                    \"interpreted in the context of specific application requirements.\\n\\n\"\n",
    "                )\n",
    "\n",
    "                # Files Generated\n",
    "                f.write(\"## Generated Files\\n\\n\")\n",
    "                f.write(\"- `mse_comparison.png`: MSE vs resolution plots\\n\")\n",
    "                f.write(\n",
    "                    \"- `execution_time_comparison.png`: Execution time distributions\\n\"\n",
    "                )\n",
    "                f.write(\n",
    "                    \"- `statistical_analysis.json`: Detailed statistical comparisons\\n\"\n",
    "                )\n",
    "                f.write(\"- Individual benchmark result files in results directory\\n\")\n",
    "\n",
    "            logger.info(f\"Report saved to {report_path}\")\n",
    "\n",
    "        except Exception:\n",
    "            logger.exception(\"Report generation failed\")\n",
    "\n",
    "    def run_complete_study(self):\n",
    "        \"\"\"Run the complete comparative study.\"\"\"\n",
    "        logger.info(\"Starting comprehensive neural operator comparative study!\")\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "\n",
    "        try:\n",
    "            # Run resolution study\n",
    "            self.run_resolution_study()\n",
    "\n",
    "            # Generate analysis\n",
    "            self.generate_comparative_analysis()\n",
    "\n",
    "            total_time = time.perf_counter() - start_time\n",
    "            logger.info(f\"Complete study finished in {total_time:.2f} seconds!\")\n",
    "\n",
    "            # Print summary\n",
    "            successful_runs = len(\n",
    "                [r for r in self.all_results if r.metrics.get(\"error\") is None]\n",
    "            )\n",
    "            total_runs = len(self.all_results)\n",
    "\n",
    "            logger.info(\"STUDY SUMMARY:\")\n",
    "            logger.info(f\"   Total benchmark runs: {total_runs}\")\n",
    "            logger.info(f\"   Successful runs: {successful_runs}\")\n",
    "            if total_runs > 0:\n",
    "                logger.info(\n",
    "                    f\"   Success rate: {successful_runs / total_runs * 100:.1f}%\"\n",
    "                )\n",
    "            else:\n",
    "                logger.info(\"   Success rate: N/A (no runs completed)\")\n",
    "            logger.info(f\"   Results saved to: {self.output_dir}\")\n",
    "\n",
    "        except Exception:\n",
    "            logger.exception(\"Study execution failed\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cd4b09",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Results Summary\n",
    "\n",
    "| Metric | UNO | FNO | SFNO |\n",
    "|--------|-----|-----|------|\n",
    "| L2 Relative Error | Varies | Varies | Varies |\n",
    "| Training Throughput | Varies | Varies | Varies |\n",
    "| Memory (Peak) | Varies | Varies | Varies |\n",
    "| Parameters | Varies | Varies | Varies |\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Run on GPU for accurate performance benchmarks\n",
    "- Add PDEBench datasets for standardized comparison\n",
    "- Compare against neuraloperator (PyTorch) and DeepXDE baselines\n",
    "- See individual model examples for architecture details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df10e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to run the comparative study.\"\"\"\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Neural Operator Comparative Benchmarking Study\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output-dir\",\n",
    "        default=\"benchmark_results/operator_benchmark\",\n",
    "        help=\"Output directory for results\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--resolutions\",\n",
    "        nargs=\"+\",\n",
    "        type=int,\n",
    "        default=[32, 64, 96],\n",
    "        help=\"Grid resolutions to test\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--n-samples\", type=int, default=1000, help=\"Number of samples per dataset\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--n-time-steps\",\n",
    "        type=int,\n",
    "        default=50,\n",
    "        help=\"Number of time steps for evolution equations\",\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Create and run study\n",
    "    study = NeuralOperatorComparativeStudy(\n",
    "        output_dir=args.output_dir,\n",
    "        resolution_sizes=args.resolutions,\n",
    "        n_samples=args.n_samples,\n",
    "        n_time_steps=args.n_time_steps,\n",
    "    )\n",
    "\n",
    "    study.run_complete_study()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bd54ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "py:percent,ipynb",
   "main_language": "python",
   "notebook_metadata_filter": "kernelspec,language_info,-jupytext.text_representation.jupytext_version,-jupytext.notebook_metadata_filter,-jupytext.cell_metadata_filter"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
