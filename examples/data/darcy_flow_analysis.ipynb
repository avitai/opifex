{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa5f527",
   "metadata": {},
   "outputs": [],
   "source": [
    "!/usr/bin/env python3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f895c371",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Darcy Flow Dataset Analysis\n",
    "\n",
    "| Metadata | Value |\n",
    "|----------|-------|\n",
    "| **Level** | Intermediate |\n",
    "| **Runtime** | ~2 min (CPU) |\n",
    "| **Prerequisites** | JAX, NumPy, Darcy Flow basics |\n",
    "| **Format** | Python + Jupyter |\n",
    "\n",
    "## Overview\n",
    "\n",
    "Darcy flow describes fluid flow through porous media, governed by an elliptic PDE.\n",
    "This example provides comprehensive analysis of Darcy flow datasets generated by\n",
    "the Opifex framework, including field statistics, spatial gradient analysis,\n",
    "resolution scaling, and data quality metrics.\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "1. **Generate** Darcy flow datasets with `DarcyDataSource` at multiple resolutions\n",
    "2. **Analyze** field statistics (mean, std, dynamic range) for permeability and pressure\n",
    "3. **Compute** spatial gradient correlations between input and output fields\n",
    "4. **Evaluate** resolution scaling performance (samples/second, time scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6559e06",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "from typing import Any\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Opifex Framework imports\n",
    "from opifex.data.sources import DarcyDataSource"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3892ed98",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Data Processing Utilities\n",
    "\n",
    "Helper functions for field normalization and grid generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8ba4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_field(field: jax.Array) -> jax.Array:\n",
    "    \"\"\"Normalize a field to zero mean and unit variance.\"\"\"\n",
    "    mean = jnp.mean(field)\n",
    "    std = jnp.std(field)\n",
    "    return (field - mean) / (std + 1e-8)  # Add small epsilon for numerical stability\n",
    "\n",
    "\n",
    "def create_grid_coordinates(resolution: int) -> tuple[jax.Array, jax.Array]:\n",
    "    \"\"\"Create 2D grid coordinates for embedding visualization.\"\"\"\n",
    "    x = jnp.linspace(0, 1, resolution)\n",
    "    y = jnp.linspace(0, 1, resolution)\n",
    "    X, Y = jnp.meshgrid(x, y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c46c293",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Analysis Metrics\n",
    "\n",
    "Functions to compute statistical properties and spatial patterns of the flow fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dbd09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_field_statistics(fields: jax.Array) -> dict[str, float]:\n",
    "    \"\"\"Compute comprehensive statistics for field data.\"\"\"\n",
    "    return {\n",
    "        \"mean\": float(jnp.mean(fields)),\n",
    "        \"std\": float(jnp.std(fields)),\n",
    "        \"min\": float(jnp.min(fields)),\n",
    "        \"max\": float(jnp.max(fields)),\n",
    "        \"median\": float(jnp.median(fields)),\n",
    "        \"q25\": float(jnp.percentile(fields, 25)),\n",
    "        \"q75\": float(jnp.percentile(fields, 75)),\n",
    "        \"dynamic_range\": float(jnp.max(fields) - jnp.min(fields)),\n",
    "        \"coefficient_of_variation\": float(jnp.std(fields) / (jnp.mean(fields) + 1e-8)),\n",
    "    }\n",
    "\n",
    "\n",
    "def _analyze_spatial_patterns(inputs: jax.Array, outputs: jax.Array) -> dict[str, Any]:\n",
    "    \"\"\"Analyze spatial patterns in the data.\"\"\"\n",
    "    # Compute spatial gradients for each axis separately\n",
    "    input_grad_x = jnp.asarray(jnp.gradient(inputs, axis=-1))\n",
    "    input_grad_y = jnp.asarray(jnp.gradient(inputs, axis=-2))\n",
    "    input_grad_magnitude = jnp.sqrt(jnp.square(input_grad_x) + jnp.square(input_grad_y))\n",
    "\n",
    "    output_grad_x = jnp.asarray(jnp.gradient(outputs, axis=-1))\n",
    "    output_grad_y = jnp.asarray(jnp.gradient(outputs, axis=-2))\n",
    "    output_grad_magnitude = jnp.sqrt(\n",
    "        jnp.square(output_grad_x) + jnp.square(output_grad_y)\n",
    "    )\n",
    "\n",
    "    # Compute correlation between inputs and outputs\n",
    "    flat_inputs = inputs.reshape(inputs.shape[0], -1)\n",
    "    flat_outputs = outputs.reshape(outputs.shape[0], -1)\n",
    "    correlations = []\n",
    "\n",
    "    for i in range(flat_inputs.shape[0]):\n",
    "        corr = jnp.corrcoef(flat_inputs[i], flat_outputs[i])[0, 1]\n",
    "        if not jnp.isnan(corr):\n",
    "            correlations.append(float(corr))\n",
    "\n",
    "    return {\n",
    "        \"input_gradient_stats\": _compute_field_statistics(input_grad_magnitude),\n",
    "        \"output_gradient_stats\": _compute_field_statistics(output_grad_magnitude),\n",
    "        \"input_output_correlation\": {\n",
    "            \"mean\": float(np.mean(correlations)) if correlations else 0.0,\n",
    "            \"std\": float(np.std(correlations)) if correlations else 0.0,\n",
    "        },\n",
    "        \"gradient_correlation\": float(\n",
    "            jnp.corrcoef(\n",
    "                input_grad_magnitude.flatten(), output_grad_magnitude.flatten()\n",
    "            )[0, 1]\n",
    "        )\n",
    "        if input_grad_magnitude.size > 0\n",
    "        else 0.0,\n",
    "    }\n",
    "\n",
    "\n",
    "def _assess_data_quality(inputs: jax.Array, outputs: jax.Array) -> dict[str, Any]:\n",
    "    \"\"\"Assess data quality metrics.\"\"\"\n",
    "    return {\n",
    "        \"has_nan\": bool(jnp.any(jnp.isnan(inputs)) or jnp.any(jnp.isnan(outputs))),\n",
    "        \"has_inf\": bool(jnp.any(jnp.isinf(inputs)) or jnp.any(jnp.isinf(outputs))),\n",
    "        \"input_range_valid\": bool(jnp.all(inputs >= 0)),\n",
    "        \"output_finite\": bool(jnp.all(jnp.isfinite(outputs))),\n",
    "        \"shape_consistency\": inputs.shape[:-1] == outputs.shape[:-1],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7738ce25",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Core Analysis Logic\n",
    "\n",
    "The main driver for generating samples and running the analysis pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cce0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _analyze_resolution_samples(samples: list[dict], resolution: int) -> dict[str, Any]:\n",
    "    \"\"\"Analyze samples for a specific resolution.\"\"\"\n",
    "    inputs = jnp.stack([sample[\"input\"] for sample in samples])\n",
    "    outputs = jnp.stack([sample[\"output\"] for sample in samples])\n",
    "\n",
    "    return {\n",
    "        \"resolution\": resolution,\n",
    "        \"input_stats\": _compute_field_statistics(inputs),\n",
    "        \"output_stats\": _compute_field_statistics(outputs),\n",
    "        \"spatial_patterns\": _analyze_spatial_patterns(inputs, outputs),\n",
    "        \"data_quality\": _assess_data_quality(inputs, outputs),\n",
    "    }\n",
    "\n",
    "\n",
    "def _compare_resolutions(datasets: dict[int, dict]) -> dict[str, Any]:\n",
    "    \"\"\"Compare datasets across different resolutions.\"\"\"\n",
    "    resolutions = sorted(datasets.keys())\n",
    "\n",
    "    if len(resolutions) < 2:\n",
    "        return {}\n",
    "\n",
    "    comparisons = {\n",
    "        \"resolution_scaling\": {},\n",
    "        \"performance_scaling\": {},\n",
    "        \"quality_comparison\": {},\n",
    "    }\n",
    "\n",
    "    # Resolution scaling analysis\n",
    "    for i, res in enumerate(resolutions[1:], 1):\n",
    "        prev_res = resolutions[i - 1]\n",
    "        scale_factor = res / prev_res\n",
    "\n",
    "        # Performance scaling\n",
    "        prev_time = datasets[prev_res][\"generation_time\"]\n",
    "        curr_time = datasets[res][\"generation_time\"]\n",
    "        time_scaling = curr_time / prev_time\n",
    "\n",
    "        comparisons[\"performance_scaling\"][f\"{prev_res}_to_{res}\"] = {\n",
    "            \"resolution_scale\": scale_factor,\n",
    "            \"time_scale\": time_scaling,\n",
    "            \"efficiency_ratio\": scale_factor**2 / time_scaling,\n",
    "        }\n",
    "\n",
    "    return comparisons\n",
    "\n",
    "\n",
    "def analyze_darcy_flow_dataset(\n",
    "    n_samples: int = 100,\n",
    "    resolutions: list[int] | None = None,\n",
    "    sub_resolution: int = 8,\n",
    "    viscosity_range: tuple[float, float] = (1e-5, 1e-3),\n",
    "    force_coefficient: float = 1.0,\n",
    "    output_dir: str = \"darcy_analysis_output\",\n",
    ") -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Analyze Darcy flow dataset characteristics across multiple resolutions.\n",
    "\n",
    "    Args:\n",
    "        n_samples: Number of samples to generate for analysis\n",
    "        resolutions: List of grid resolutions to test\n",
    "        sub_resolution: Subsampling factor for coarse-graining\n",
    "        viscosity_range: Range of viscosity values to sample\n",
    "        force_coefficient: Force scaling coefficient\n",
    "        output_dir: Directory to save analysis results\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing comprehensive analysis results\n",
    "    \"\"\"\n",
    "    if resolutions is None:\n",
    "        resolutions = [64, 128]\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"DARCY FLOW DATASET ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    results = {\n",
    "        \"parameters\": {\n",
    "            \"n_samples\": n_samples,\n",
    "            \"resolutions\": resolutions,\n",
    "            \"sub_resolution\": sub_resolution,\n",
    "            \"viscosity_range\": viscosity_range,\n",
    "            \"force_coefficient\": force_coefficient,\n",
    "        },\n",
    "        \"datasets\": {},\n",
    "        \"comparisons\": {},\n",
    "        \"timing\": {},\n",
    "    }\n",
    "\n",
    "    # Analyze each resolution\n",
    "    for resolution in resolutions:\n",
    "        print()\n",
    "        print(f\"Analyzing resolution: {resolution}x{resolution}\")\n",
    "\n",
    "        # Create data source (Grain-based)\n",
    "        data_source = DarcyDataSource(\n",
    "            resolution=resolution,\n",
    "            n_samples=n_samples,\n",
    "            viscosity_range=viscosity_range,\n",
    "            seed=42,\n",
    "        )\n",
    "\n",
    "        # Generate samples and measure timing\n",
    "        start_time = time.time()\n",
    "        samples = [data_source[i] for i in range(n_samples)]\n",
    "        generation_time = time.time() - start_time\n",
    "\n",
    "        # Analyze samples\n",
    "        resolution_results = _analyze_resolution_samples(samples, resolution)\n",
    "        resolution_results[\"generation_time\"] = generation_time\n",
    "        resolution_results[\"samples_per_second\"] = n_samples / generation_time\n",
    "\n",
    "        results[\"datasets\"][resolution] = resolution_results\n",
    "\n",
    "        print(f\"  Generated {n_samples} samples in {generation_time:.2f}s\")\n",
    "        print(f\"  Rate: {n_samples / generation_time:.1f} samples/second\")\n",
    "\n",
    "    # Cross-resolution comparisons\n",
    "    if len(resolutions) > 1:\n",
    "        results[\"comparisons\"] = _compare_resolutions(results[\"datasets\"])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad82a8e",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Visualization\n",
    "\n",
    "Generating plots for statistics and comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a819e679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_resolution_comparison_plots(\n",
    "    results: dict[str, Any], save_path: str | None\n",
    ") -> None:\n",
    "    \"\"\"Create plots comparing different resolutions.\"\"\"\n",
    "    datasets = results[\"datasets\"]\n",
    "    if len(datasets) < 2:\n",
    "        return\n",
    "\n",
    "    _, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    resolutions = list(datasets.keys())\n",
    "\n",
    "    # Plot 1: Mean values comparison\n",
    "    input_means = [datasets[res][\"input_stats\"][\"mean\"] for res in resolutions]\n",
    "    output_means = [datasets[res][\"output_stats\"][\"mean\"] for res in resolutions]\n",
    "\n",
    "    axes[0, 0].plot(resolutions, input_means, \"o-\", label=\"Input (Permeability)\")\n",
    "    axes[0, 0].plot(resolutions, output_means, \"s-\", label=\"Output (Pressure)\")\n",
    "    axes[0, 0].set_xlabel(\"Resolution\")\n",
    "    axes[0, 0].set_ylabel(\"Mean Value\")\n",
    "    axes[0, 0].set_title(\"Mean Values vs Resolution\")\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 2: Standard deviation comparison\n",
    "    input_stds = [datasets[res][\"input_stats\"][\"std\"] for res in resolutions]\n",
    "    output_stds = [datasets[res][\"output_stats\"][\"std\"] for res in resolutions]\n",
    "\n",
    "    axes[0, 1].plot(resolutions, input_stds, \"o-\", label=\"Input (Permeability)\")\n",
    "    axes[0, 1].plot(resolutions, output_stds, \"s-\", label=\"Output (Pressure)\")\n",
    "    axes[0, 1].set_xlabel(\"Resolution\")\n",
    "    axes[0, 1].set_ylabel(\"Standard Deviation\")\n",
    "    axes[0, 1].set_title(\"Variability vs Resolution\")\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 3: Dynamic range comparison\n",
    "    input_ranges = [\n",
    "        datasets[res][\"input_stats\"][\"dynamic_range\"] for res in resolutions\n",
    "    ]\n",
    "    output_ranges = [\n",
    "        datasets[res][\"output_stats\"][\"dynamic_range\"] for res in resolutions\n",
    "    ]\n",
    "\n",
    "    axes[1, 0].plot(resolutions, input_ranges, \"o-\", label=\"Input (Permeability)\")\n",
    "    axes[1, 0].plot(resolutions, output_ranges, \"s-\", label=\"Output (Pressure)\")\n",
    "    axes[1, 0].set_xlabel(\"Resolution\")\n",
    "    axes[1, 0].set_ylabel(\"Dynamic Range\")\n",
    "    axes[1, 0].set_title(\"Dynamic Range vs Resolution\")\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 4: Performance scaling\n",
    "    generation_times = [datasets[res][\"generation_time\"] for res in resolutions]\n",
    "    samples_per_sec = [datasets[res][\"samples_per_second\"] for res in resolutions]\n",
    "\n",
    "    ax4_twin = axes[1, 1].twinx()\n",
    "    line1 = axes[1, 1].plot(\n",
    "        resolutions, generation_times, \"ro-\", label=\"Generation Time (s)\"\n",
    "    )\n",
    "    line2 = ax4_twin.plot(resolutions, samples_per_sec, \"bs-\", label=\"Samples/Second\")\n",
    "\n",
    "    axes[1, 1].set_xlabel(\"Resolution\")\n",
    "    axes[1, 1].set_ylabel(\"Generation Time (s)\", color=\"red\")\n",
    "    ax4_twin.set_ylabel(\"Samples per Second\", color=\"blue\")\n",
    "    axes[1, 1].set_title(\"Performance vs Resolution\")\n",
    "\n",
    "    lines = line1 + line2\n",
    "    axes[1, 1].legend(lines, [l.get_label() for l in lines], loc=\"center right\")\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(\n",
    "            f\"{save_path}_resolution_comparison.png\", dpi=300, bbox_inches=\"tight\"\n",
    "        )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def _create_statistical_summary_plots(\n",
    "    results: dict[str, Any], save_path: str | None\n",
    ") -> None:\n",
    "    \"\"\"Create statistical summary visualizations.\"\"\"\n",
    "    _ = results[\"datasets\"]\n",
    "\n",
    "    _, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "    # Summary statistics will be added based on available data\n",
    "    axes[0, 0].text(\n",
    "        0.5,\n",
    "        0.5,\n",
    "        \"Statistical Summary\\n(Implementation Placeholder)\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        transform=axes[0, 0].transAxes,\n",
    "    )\n",
    "    axes[0, 0].set_title(\"Input Statistics Summary\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(\n",
    "            f\"{save_path}_statistical_summary.png\", dpi=300, bbox_inches=\"tight\"\n",
    "        )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def _create_performance_analysis_plots(\n",
    "    results: dict[str, Any], save_path: str | None\n",
    ") -> None:\n",
    "    \"\"\"Create performance analysis visualizations.\"\"\"\n",
    "    if \"comparisons\" not in results or not results[\"comparisons\"]:\n",
    "        return\n",
    "\n",
    "    _, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # Performance analysis will be added based on available data\n",
    "    axes[0].text(\n",
    "        0.5,\n",
    "        0.5,\n",
    "        \"Performance Analysis\\n(Implementation Placeholder)\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        transform=axes[0].transAxes,\n",
    "    )\n",
    "    axes[0].set_title(\"Scaling Performance\")\n",
    "\n",
    "    axes[1].text(\n",
    "        0.5,\n",
    "        0.5,\n",
    "        \"Efficiency Metrics\\n(Implementation Placeholder)\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        transform=axes[1].transAxes,\n",
    "    )\n",
    "    axes[1].set_title(\"Computational Efficiency\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(\n",
    "            f\"{save_path}_performance_analysis.png\", dpi=300, bbox_inches=\"tight\"\n",
    "        )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def create_visualization(\n",
    "    results: dict[str, Any],\n",
    "    preprocessing_results: dict[str, Any],\n",
    "    save_path: str | None = None,\n",
    ") -> None:\n",
    "    \"\"\"Create comprehensive visualization of analysis results.\"\"\"\n",
    "    _create_resolution_comparison_plots(results, save_path)\n",
    "    _create_statistical_summary_plots(results, save_path)\n",
    "    _create_performance_analysis_plots(results, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80551714",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Results Summary\n",
    "\n",
    "| Metric | Description |\n",
    "|--------|-------------|\n",
    "| Field Statistics | Mean, std, min, max, median for permeability and pressure |\n",
    "| Spatial Gradients | Gradient magnitude statistics and input-output correlation |\n",
    "| Resolution Scaling | Time scaling and efficiency ratios across resolutions |\n",
    "| Data Quality | NaN/Inf checks, range validation, shape consistency |\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Use these analysis results to validate data before FNO training\n",
    "- Compare statistics across different viscosity ranges\n",
    "- See [Spectral Analysis](darcy_flow_spectral_analysis.md) for frequency domain analysis\n",
    "- See [FNO Darcy Comprehensive](../models/fno_darcy_comprehensive.md) for training with this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8329295c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Run comprehensive Darcy flow analysis validation.\"\"\"\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Analyze Darcy flow dataset characteristics\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--n_samples\",\n",
    "        type=int,\n",
    "        default=100,\n",
    "        help=\"Number of samples to generate for analysis\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--resolutions\",\n",
    "        nargs=\"+\",\n",
    "        type=int,\n",
    "        default=[64, 128],\n",
    "        help=\"Grid resolutions to analyze\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        type=str,\n",
    "        default=\"docs/assets/examples/darcy_flow_analysis_files\",\n",
    "        help=\"Output directory for results\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--save_plots\",\n",
    "        action=\"store_true\",\n",
    "        default=True,\n",
    "        help=\"Save generated plots to disk\",\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Run analysis\n",
    "    print(\"Starting comprehensive Darcy flow dataset analysis...\")\n",
    "    results = analyze_darcy_flow_dataset(\n",
    "        n_samples=args.n_samples,\n",
    "        resolutions=args.resolutions,\n",
    "        output_dir=args.output_dir,\n",
    "    )\n",
    "\n",
    "    # Create visualizations\n",
    "    preprocessing_results = {}  # Placeholder for future preprocessing analysis\n",
    "\n",
    "    save_path = f\"{args.output_dir}/darcy_analysis\" if args.save_plots else None\n",
    "    create_visualization(results, preprocessing_results, save_path)\n",
    "\n",
    "    # Print summary\n",
    "    print()\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ANALYSIS COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "    for res, data in results[\"datasets\"].items():\n",
    "        print(f\"Resolution {res}x{res}:\")\n",
    "        print(f\"  Generation time: {data['generation_time']:.2f}s\")\n",
    "        print(f\"  Samples/second: {data['samples_per_second']:.1f}\")\n",
    "        print(f\"  Input mean: {data['input_stats']['mean']:.4f}\")\n",
    "        print(f\"  Output mean: {data['output_stats']['mean']:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "py:percent,ipynb",
   "main_language": "python",
   "notebook_metadata_filter": "kernelspec,language_info,-jupytext.text_representation.jupytext_version,-jupytext.notebook_metadata_filter,-jupytext.cell_metadata_filter"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
