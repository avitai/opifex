{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34be2a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "!/usr/bin/env python3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bc1254",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# PDEBench Dataset Loading with Opifex\n",
    "\n",
    "| Metadata | Value |\n",
    "|----------|-------|\n",
    "| **Level** | Beginner |\n",
    "| **Runtime** | ~30 sec (GPU) |\n",
    "| **Prerequisites** | JAX, h5py, HDF5 file |\n",
    "| **Format** | Python + Jupyter |\n",
    "\n",
    "## Overview\n",
    "\n",
    "PDEBench is a comprehensive benchmark suite for scientific machine learning,\n",
    "providing HDF5-formatted simulation trajectories across 1D/2D/3D PDEs\n",
    "(Burgers, Navier-Stokes, Darcy Flow, etc.).\n",
    "\n",
    "Opifex's `PDEBenchSource` provides an eager-loading interface that converts\n",
    "HDF5 data to JAX arrays at initialization, then offers pure-JAX iteration\n",
    "for training neural operators.\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "1. **Create** a synthetic HDF5 file matching the PDEBench format\n",
    "2. **Load** the dataset with `PDEBenchSource` — all I/O at init\n",
    "3. **Inspect** shapes, sliding window pairs, and coordinate grids\n",
    "4. **Batch** data for training with `get_batch()`\n",
    "5. **Iterate** over the full dataset with epoch reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ae503d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import h5py\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from flax import nnx\n",
    "\n",
    "from opifex.data.sources.scientific import PDEBenchConfig, PDEBenchSource"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960bf54b",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 1: Create a Synthetic PDEBench HDF5 File\n",
    "\n",
    "PDEBench datasets follow a specific HDF5 structure:\n",
    "- `/tensor`: shape `(N, T, X[, Y], C)` — simulation trajectories\n",
    "- `/x`, `/y`, `/t`: optional coordinate grids\n",
    "\n",
    "We create a small synthetic dataset for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c6a3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_synthetic_pdebench_hdf5(\n",
    "    file_path: Path,\n",
    "    n_samples: int = 20,\n",
    "    n_timesteps: int = 20,\n",
    "    n_spatial: int = 64,\n",
    "    n_channels: int = 1,\n",
    ") -> None:\n",
    "    \"\"\"Create a synthetic 1D Burgers-like HDF5 file.\n",
    "\n",
    "    The data simulates a simple diffusing step function to\n",
    "    approximate the structure of real PDEBench Burgers data.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(42)\n",
    "\n",
    "    # Generate smooth initial conditions and evolve them\n",
    "    x = np.linspace(0, 2 * np.pi, n_spatial)\n",
    "    t = np.linspace(0, 1, n_timesteps)\n",
    "    data = np.zeros((n_samples, n_timesteps, n_spatial, n_channels))\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        # Random superposition of sinusoids as initial condition\n",
    "        u0 = rng.normal(0, 0.5) * np.sin(x) + rng.normal(0, 0.3) * np.sin(2 * x)\n",
    "        for j, tj in enumerate(t):\n",
    "            # Simple diffusion-like evolution\n",
    "            data[i, j, :, 0] = u0 * np.exp(-0.5 * tj)\n",
    "\n",
    "    with h5py.File(file_path, \"w\") as f:\n",
    "        f.create_dataset(\"tensor\", data=data.astype(np.float32))\n",
    "        f.create_dataset(\"x\", data=x.astype(np.float32))\n",
    "        f.create_dataset(\"t\", data=t.astype(np.float32))\n",
    "\n",
    "    print(f\"Created synthetic HDF5: {file_path}\")\n",
    "    print(f\"  tensor shape: {data.shape}\")\n",
    "    print(f\"  x shape:      {x.shape}\")\n",
    "    print(f\"  t shape:      {t.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfccc33",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 2: Load with PDEBenchSource\n",
    "\n",
    "`PDEBenchSource` handles all file I/O at `__init__`:\n",
    "1. Reads the HDF5 file\n",
    "2. Applies train/test split along the sample axis\n",
    "3. Creates sliding window input/target pairs over time\n",
    "4. Optionally normalizes to [0, 1]\n",
    "5. Converts everything to JAX arrays\n",
    "\n",
    "After init, the source is pure JAX — no more file I/O."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874b08fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary synthetic data\n",
    "tmp_dir = tempfile.mkdtemp()\n",
    "hdf5_path = Path(tmp_dir) / \"1D_Burgers_synth.hdf5\"\n",
    "create_synthetic_pdebench_hdf5(hdf5_path, n_samples=20, n_timesteps=20)\n",
    "\n",
    "# Configure and load\n",
    "config = PDEBenchConfig(\n",
    "    file_path=hdf5_path,\n",
    "    dataset_name=\"1D_Burgers\",\n",
    "    train_split=0.8,\n",
    "    split=\"train\",\n",
    "    input_steps=5,\n",
    "    output_steps=5,\n",
    "    normalize=True,\n",
    ")\n",
    "source = PDEBenchSource(config, rngs=nnx.Rngs(0))\n",
    "\n",
    "print(f\"\\nDataset loaded: {len(source)} sliding window pairs\")\n",
    "print(f\"  inputs shape:  {source.inputs.shape}\")\n",
    "print(f\"  targets shape: {source.targets.shape}\")\n",
    "print(f\"  coordinates:   {source.coordinates is not None}\")\n",
    "if source.coordinates is not None:\n",
    "    for k, v in source.coordinates.items():\n",
    "        print(f\"    {k}: {v.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84adeba3",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 3: Inspect Individual Elements\n",
    "\n",
    "Each element is a dict with:\n",
    "- `\"input\"`: shape `(input_steps, *spatial, channels)`\n",
    "- `\"target\"`: shape `(output_steps, *spatial, channels)`\n",
    "- `\"coordinates\"`: dict of spatial/temporal grids (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5fc406",
   "metadata": {},
   "outputs": [],
   "source": [
    "element = source[0]\n",
    "print(\"Element keys:\", list(element.keys()))\n",
    "print(f\"  input shape:  {element['input'].shape}\")\n",
    "print(f\"  target shape: {element['target'].shape}\")\n",
    "\n",
    "# Verify normalization\n",
    "print(\n",
    "    f\"\\n  input range:  [{float(jnp.min(element['input'])):.4f}, \"\n",
    "    f\"{float(jnp.max(element['input'])):.4f}]\"\n",
    ")\n",
    "print(\n",
    "    f\"  target range: [{float(jnp.min(element['target'])):.4f}, \"\n",
    "    f\"{float(jnp.max(element['target'])):.4f}]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b2dd93",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 4: Batch Retrieval for Training\n",
    "\n",
    "`get_batch()` supports two modes:\n",
    "1. **Stateful** (no key): sequential batches from current position\n",
    "2. **Stateless** (with key): random batches for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f20d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stateful sequential batch\n",
    "batch = source.get_batch(batch_size=4)\n",
    "print(\"Sequential batch:\")\n",
    "print(f\"  input shape:  {batch['input'].shape}\")\n",
    "print(f\"  target shape: {batch['target'].shape}\")\n",
    "\n",
    "# Stateless random batch\n",
    "key = jax.random.key(42)\n",
    "random_batch = source.get_batch(batch_size=8, key=key)\n",
    "print(\"\\nRandom batch:\")\n",
    "print(f\"  input shape:  {random_batch['input'].shape}\")\n",
    "print(f\"  target shape: {random_batch['target'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d55d8d",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 5: Full Epoch Iteration\n",
    "\n",
    "Iterate over all elements in the dataset.\n",
    "Call `reset()` to start a new epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece3af62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full iteration\n",
    "count = 0\n",
    "for _element in source:\n",
    "    count += 1\n",
    "print(f\"Iterated over {count} elements (dataset has {len(source)})\")\n",
    "\n",
    "# Reset for another epoch\n",
    "source.reset()\n",
    "batch_after_reset = source.get_batch(batch_size=2)\n",
    "print(f\"After reset, batch input shape: {batch_after_reset['input'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f21fa0",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Step 6: Train vs Test Split\n",
    "\n",
    "Create separate sources for training and evaluation\n",
    "by changing the `split` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4027cd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_config = PDEBenchConfig(\n",
    "    file_path=hdf5_path,\n",
    "    dataset_name=\"1D_Burgers\",\n",
    "    train_split=0.8,\n",
    "    split=\"test\",\n",
    "    input_steps=5,\n",
    "    output_steps=5,\n",
    "    normalize=True,\n",
    ")\n",
    "test_source = PDEBenchSource(test_config, rngs=nnx.Rngs(0))\n",
    "\n",
    "print(f\"Train samples: {len(source)}\")\n",
    "print(f\"Test samples:  {len(test_source)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a2bf24",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Results Summary\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| I/O Strategy | Eager — all at init, pure JAX after |\n",
    "| Window Pairing | Sliding window over time axis |\n",
    "| Normalization | Per-channel min-max to [0, 1] |\n",
    "| Batch Modes | Stateful (sequential) and stateless (random) |\n",
    "| Split Support | Train/test via `split` parameter |\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Use `PDEBenchSource` with real PDEBench `.hdf5` files from\n",
    "  [PDEBench](https://github.com/pdebench/PDEBench)\n",
    "- Train an FNO or DeepONet on the loaded data\n",
    "- See [Darcy Flow Analysis](darcy_flow_analysis.md) for a related example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec3ef12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Run the PDEBench loading example.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"PDEBench Loading Example — Complete\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Loaded {len(source)} train + {len(test_source)} test pairs\")\n",
    "    print(f\"Input shape:  {source.inputs.shape}\")\n",
    "    print(f\"Target shape: {source.targets.shape}\")\n",
    "    print(f\"Backend:      {jax.default_backend()}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "py:percent,ipynb",
   "main_language": "python",
   "notebook_metadata_filter": "kernelspec,language_info,-jupytext.text_representation.jupytext_version,-jupytext.notebook_metadata_filter,-jupytext.cell_metadata_filter"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
