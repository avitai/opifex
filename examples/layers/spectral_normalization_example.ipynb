{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e9b423b",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "# Spectral Normalization for Neural Operators\n",
    "\n",
    "| Metadata | Value |\n",
    "|----------|-------|\n",
    "| **Level** | Intermediate |\n",
    "| **Runtime** | ~5 min (CPU) |\n",
    "| **Prerequisites** | JAX, Flax NNX, Linear Algebra basics |\n",
    "| **Format** | Python + Jupyter |\n",
    "\n",
    "## Overview\n",
    "\n",
    "Spectral normalization controls the Lipschitz constant of neural network layers\n",
    "by normalizing weight matrices by their spectral norm (largest singular value).\n",
    "This is critical for PDE-solving neural operators where stability and convergence\n",
    "guarantees depend on bounded operator norms.\n",
    "\n",
    "This example demonstrates spectral normalized linear layers, convolutions, and\n",
    "attention mechanisms. It includes stability analysis comparing regular vs spectral\n",
    "normalized networks, adaptive bounds, and performance benchmarks.\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "1. Apply `SpectralLinear` and `SpectralNormalizedConv` for stable neural operator layers\n",
    "2. Use `SpectralMultiHeadAttention` for normalized attention mechanisms\n",
    "3. Configure `AdaptiveSpectralNorm` with learnable bounds\n",
    "4. Analyze Lipschitz constants to verify stability improvements\n",
    "5. Build complete spectral normalized neural operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000203ec",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "\n",
    "# Note: SpectralConvolution here is for spectral NORMALIZATION (different from FNO SpectralConvolution)\n",
    "# Complete spectral neural operators are in FNO spectral module\n",
    "from opifex.neural.operators.fno.spectral import create_spectral_neural_operator\n",
    "from opifex.neural.operators.specialized.spectral_normalization import (\n",
    "    AdaptiveSpectralNorm,\n",
    "    PowerIteration,\n",
    "    spectral_norm_summary,\n",
    "    SpectralLinear,\n",
    "    SpectralMultiHeadAttention,\n",
    "    SpectralNormalizedConv,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477ca7ce",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Test Problem Setup\n",
    "\n",
    "Create test problems for demonstrating spectral normalization benefits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aec7daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_problems():\n",
    "    \"\"\"Create test problems for demonstrating spectral normalization benefits.\"\"\"\n",
    "    # 1D Function approximation problem\n",
    "    x_1d = jnp.linspace(-2, 2, 100)\n",
    "    y_1d = jnp.sin(2 * jnp.pi * x_1d) + 0.5 * jnp.cos(4 * jnp.pi * x_1d)\n",
    "\n",
    "    # 2D Image denoising problem\n",
    "    x = jnp.linspace(-1, 1, 32)\n",
    "    y = jnp.linspace(-1, 1, 32)\n",
    "    X, Y = jnp.meshgrid(x, y)\n",
    "    clean_image = jnp.exp(-(X**2 + Y**2)) * jnp.sin(3 * X) * jnp.cos(3 * Y)\n",
    "    noise = 0.1 * jax.random.normal(jax.random.PRNGKey(42), clean_image.shape)\n",
    "    noisy_image = clean_image + noise\n",
    "\n",
    "    # PDE solution problem (heat equation)\n",
    "    nx, nt = 64, 50\n",
    "    x_pde = jnp.linspace(0, 1, nx)\n",
    "    t_pde = jnp.linspace(0, 0.1, nt)\n",
    "\n",
    "    # Initial condition: Gaussian pulse\n",
    "    initial_temp = jnp.exp(-50 * (x_pde - 0.5) ** 2)\n",
    "\n",
    "    return {\n",
    "        \"function_1d\": (x_1d, y_1d),\n",
    "        \"image_denoising\": (noisy_image, clean_image),\n",
    "        \"pde_initial\": (x_pde, t_pde, initial_temp),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fe5472",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 1. Basic Spectral Normalization Layers\n",
    "\n",
    "Comparing regular layers with their spectral normalized counterparts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fe06c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_basic_spectral_layers():\n",
    "    \"\"\"Demonstrate basic spectral normalization layers.\"\"\"\n",
    "    print(\"BASIC SPECTRAL NORMALIZATION LAYERS\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    rngs = nnx.Rngs(42)\n",
    "\n",
    "    # Linear layer comparison\n",
    "    print()\n",
    "    print(\"Linear Layer Comparison:\")\n",
    "    regular_linear = nnx.Linear(10, 5, rngs=rngs)\n",
    "    spectral_linear = SpectralLinear(10, 5, power_iterations=5, rngs=rngs)\n",
    "\n",
    "    # Test input\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    x = jax.random.normal(key, (8, 10))\n",
    "\n",
    "    # Regular forward pass\n",
    "    y_regular = regular_linear(x)\n",
    "    print(f\"   Regular Linear: {x.shape} -> {y_regular.shape}\")\n",
    "\n",
    "    # Spectral normalized forward pass\n",
    "    y_spectral = spectral_linear(x, training=True)\n",
    "    print(f\"   Spectral Linear: {x.shape} -> {y_spectral.shape}\")\n",
    "\n",
    "    # Analyze spectral norms\n",
    "    regular_spectral_norm = jnp.linalg.norm(\n",
    "        jnp.linalg.svd(regular_linear.kernel.value, compute_uv=False), ord=2\n",
    "    )\n",
    "    spectral_norm_estimate, _ = spectral_linear.power_iter(\n",
    "        spectral_linear.linear.kernel.value, training=False\n",
    "    )\n",
    "\n",
    "    print(f\"   Regular kernel spectral norm: {regular_spectral_norm:.3f}\")\n",
    "    print(f\"   Spectral normalized estimate: {spectral_norm_estimate:.3f}\")\n",
    "\n",
    "    # Convolution layer comparison\n",
    "    print()\n",
    "    print(\"Convolution Layer Comparison:\")\n",
    "    regular_conv = nnx.Conv(3, 16, kernel_size=3, rngs=rngs)\n",
    "    spectral_conv = SpectralNormalizedConv(\n",
    "        3, 16, kernel_size=3, power_iterations=3, rngs=rngs\n",
    "    )\n",
    "\n",
    "    # Test input\n",
    "    x_img = jax.random.normal(key, (4, 32, 32, 3))\n",
    "\n",
    "    y_regular_conv = regular_conv(x_img)\n",
    "    y_spectral_conv = spectral_conv(x_img, training=True)\n",
    "\n",
    "    print(f\"   Regular Conv: {x_img.shape} -> {y_regular_conv.shape}\")\n",
    "    print(f\"   Spectral Conv: {x_img.shape} -> {y_spectral_conv.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabd92b0",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 2. Spectral Normalized Attention\n",
    "\n",
    "Multi-head attention with spectral normalization for stable sequence processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329152fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_spectral_attention():\n",
    "    \"\"\"Demonstrate spectral normalized attention mechanisms.\"\"\"\n",
    "    print()\n",
    "    print(\"SPECTRAL NORMALIZED ATTENTION\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    rngs = nnx.Rngs(42)\n",
    "\n",
    "    # Create spectral normalized attention\n",
    "    spectral_attention = SpectralMultiHeadAttention(\n",
    "        num_heads=8, in_features=64, power_iterations=3, rngs=rngs\n",
    "    )\n",
    "\n",
    "    print(\"Attention configuration:\")\n",
    "    print(f\"   Number of heads: {spectral_attention.num_heads}\")\n",
    "    print(f\"   Feature dimension: {spectral_attention.qkv_features}\")\n",
    "    print(f\"   Head dimension: {spectral_attention.head_dim}\")\n",
    "\n",
    "    # Test sequence data (like neural operator coordinates)\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    batch_size, seq_len, features = 2, 32, 64\n",
    "    x = jax.random.normal(key, (batch_size, seq_len, features))\n",
    "\n",
    "    print()\n",
    "    print(f\"Processing sequence: {x.shape}\")\n",
    "\n",
    "    # Forward pass\n",
    "    start_time = time.time()\n",
    "    output = spectral_attention(x, training=True)\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"   Output shape: {output.shape}\")\n",
    "    print(f\"   Forward pass time: {(end_time - start_time) * 1000:.2f} ms\")\n",
    "\n",
    "    # Test with causal mask\n",
    "    mask = jnp.tril(\n",
    "        jnp.ones((batch_size, spectral_attention.num_heads, seq_len, seq_len))\n",
    "    )\n",
    "    output_masked = spectral_attention(x, mask=mask, training=True)\n",
    "\n",
    "    print(f\"   Masked output shape: {output_masked.shape}\")\n",
    "    print(f\"   Attention mask applied: {mask.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de569ee",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 3. Adaptive Spectral Normalization\n",
    "\n",
    "Flexible spectral bounds with optional learnable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2921dcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_adaptive_spectral_norm():\n",
    "    \"\"\"Demonstrate adaptive spectral normalization with learnable bounds.\"\"\"\n",
    "    print()\n",
    "    print(\"ADAPTIVE SPECTRAL NORMALIZATION\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    rngs = nnx.Rngs(42)\n",
    "\n",
    "    # Create different adaptive configurations\n",
    "    configs = {\n",
    "        \"Fixed bound (1.0)\": {\"initial_bound\": 1.0, \"learnable_bound\": False},\n",
    "        \"Fixed bound (0.5)\": {\"initial_bound\": 0.5, \"learnable_bound\": False},\n",
    "        \"Learnable bound\": {\"initial_bound\": 1.0, \"learnable_bound\": True},\n",
    "        \"Learnable relaxed\": {\"initial_bound\": 2.0, \"learnable_bound\": True},\n",
    "    }\n",
    "\n",
    "    models = {}\n",
    "    for name, config in configs.items():\n",
    "        base_linear = nnx.Linear(16, 8, rngs=rngs)\n",
    "        adaptive_layer = AdaptiveSpectralNorm(\n",
    "            base_linear, power_iterations=5, rngs=rngs, **config\n",
    "        )\n",
    "        models[name] = adaptive_layer\n",
    "\n",
    "        print(f\"{name}:\")\n",
    "        print(f\"   Initial bound: {config['initial_bound']}\")\n",
    "        print(f\"   Learnable: {config['learnable_bound']}\")\n",
    "\n",
    "    # Test with sample data\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    x = jax.random.normal(key, (5, 16))\n",
    "\n",
    "    print()\n",
    "    print(f\"Testing with input shape: {x.shape}\")\n",
    "\n",
    "    for name, model in models.items():\n",
    "        output = model(x, training=True)\n",
    "        bound_value = model.bound.value\n",
    "        print(f\"   {name}: bound = {bound_value:.3f}, output shape = {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6d3cdd",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 4. Power Iteration Algorithm\n",
    "\n",
    "The core algorithm for efficient spectral norm estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b13be48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_power_iteration_algorithm():\n",
    "    \"\"\"Demonstrate the core power iteration algorithm.\"\"\"\n",
    "    print()\n",
    "    print(\"POWER ITERATION ALGORITHM\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    rngs = nnx.Rngs(42)\n",
    "\n",
    "    # Test matrices with known properties\n",
    "    test_matrices = {\n",
    "        \"Identity\": jnp.eye(4),\n",
    "        \"Diagonal\": jnp.diag(jnp.array([3.0, 2.0, 1.0, 0.5])),\n",
    "        \"Random\": jax.random.normal(jax.random.PRNGKey(42), (6, 4)),\n",
    "        \"Large Random\": jax.random.normal(jax.random.PRNGKey(123), (128, 64)),\n",
    "    }\n",
    "\n",
    "    # Test different iteration counts\n",
    "    iteration_counts = [1, 3, 5, 10]\n",
    "\n",
    "    for matrix_name, matrix in test_matrices.items():\n",
    "        print()\n",
    "        print(f\"Matrix: {matrix_name} (shape: {matrix.shape})\")\n",
    "\n",
    "        # True spectral norm via SVD\n",
    "        true_spectral_norm = jnp.max(jnp.linalg.svd(matrix, compute_uv=False))\n",
    "        print(f\"   True spectral norm (SVD): {true_spectral_norm:.6f}\")\n",
    "\n",
    "        for num_iter in iteration_counts:\n",
    "            power_iter = PowerIteration(num_iterations=num_iter, rngs=rngs)\n",
    "\n",
    "            start_time = time.time()\n",
    "            estimated_norm, _ = power_iter(matrix, training=True)\n",
    "            end_time = time.time()\n",
    "\n",
    "            error = abs(estimated_norm - true_spectral_norm)\n",
    "            print(\n",
    "                f\"   {num_iter:2d} iterations: {estimated_norm:.6f} \"\n",
    "                f\"(error: {error:.6f}, time: {(end_time - start_time) * 1000:.2f} ms)\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c07313",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 5. Complete Spectral Neural Operators\n",
    "\n",
    "Building full spectral normalized architectures for PDE solving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b45685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_complete_neural_operators():\n",
    "    \"\"\"Demonstrate complete spectral normalized neural operators.\"\"\"\n",
    "    print()\n",
    "    print(\"COMPLETE SPECTRAL NEURAL OPERATORS\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    rngs = nnx.Rngs(42)\n",
    "\n",
    "    # Create different neural operator architectures\n",
    "    architectures = {\n",
    "        \"Small FNO-style\": {\n",
    "            \"input_dim\": 32,\n",
    "            \"output_dim\": 32,\n",
    "            \"hidden_dims\": (64, 64),\n",
    "            \"num_heads\": 4,\n",
    "            \"power_iterations\": 1,\n",
    "        },\n",
    "        \"Medium PDE solver\": {\n",
    "            \"input_dim\": 64,\n",
    "            \"output_dim\": 64,\n",
    "            \"hidden_dims\": (128, 128, 64),\n",
    "            \"num_heads\": 8,\n",
    "            \"power_iterations\": 3,\n",
    "        },\n",
    "        \"Large Multi-scale\": {\n",
    "            \"input_dim\": 128,\n",
    "            \"output_dim\": 64,\n",
    "            \"hidden_dims\": (256, 192, 128, 96),\n",
    "            \"num_heads\": 16,\n",
    "            \"power_iterations\": 5,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    models = {}\n",
    "    for name, config in architectures.items():\n",
    "        print()\n",
    "        print(f\"Creating {name}:\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        model = create_spectral_neural_operator(rngs=rngs, **config)\n",
    "        end_time = time.time()\n",
    "\n",
    "        models[name] = model\n",
    "\n",
    "        print(f\"   Input/Output dims: {config['input_dim']} -> {config['output_dim']}\")\n",
    "        print(f\"   Hidden layers: {config['hidden_dims']}\")\n",
    "        print(f\"   Attention heads: {config['num_heads']}\")\n",
    "        print(f\"   Creation time: {(end_time - start_time) * 1000:.2f} ms\")\n",
    "\n",
    "    # Test forward passes\n",
    "    print()\n",
    "    print(\"Testing forward passes:\")\n",
    "\n",
    "    for name, model in models.items():\n",
    "        config = architectures[name]\n",
    "\n",
    "        # Create test input\n",
    "        key = jax.random.PRNGKey(0)\n",
    "        batch_size = 4\n",
    "        x = jax.random.normal(key, (batch_size, config[\"input_dim\"]))\n",
    "\n",
    "        # Timed forward pass\n",
    "        start_time = time.time()\n",
    "        output = model(x, training=True)\n",
    "        end_time = time.time()\n",
    "\n",
    "        print(\n",
    "            f\"   {name}: {x.shape} -> {output.shape} \"\n",
    "            f\"({(end_time - start_time) * 1000:.2f} ms)\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947a8d93",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 6. Stability Analysis and Lipschitz Control\n",
    "\n",
    "Comparing Lipschitz constants between regular and spectral normalized networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b84339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_stability_analysis():  # noqa: PLR0915\n",
    "    \"\"\"Demonstrate stability analysis and Lipschitz constant control.\"\"\"\n",
    "    print()\n",
    "    print(\"STABILITY ANALYSIS & LIPSCHITZ CONTROL\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    rngs = nnx.Rngs(42)\n",
    "\n",
    "    # Create regular vs spectral normalized networks\n",
    "    input_dim, output_dim = 16, 8\n",
    "\n",
    "    regular_model = nnx.Sequential(\n",
    "        nnx.Linear(input_dim, 32, rngs=rngs),\n",
    "        nnx.relu,\n",
    "        nnx.Linear(32, 16, rngs=rngs),\n",
    "        nnx.relu,\n",
    "        nnx.Linear(16, output_dim, rngs=rngs),\n",
    "    )\n",
    "\n",
    "    spectral_model = nnx.Sequential(\n",
    "        SpectralLinear(input_dim, 32, power_iterations=5, rngs=rngs),\n",
    "        nnx.relu,\n",
    "        SpectralLinear(32, 16, power_iterations=5, rngs=rngs),\n",
    "        nnx.relu,\n",
    "        SpectralLinear(16, output_dim, power_iterations=5, rngs=rngs),\n",
    "    )\n",
    "\n",
    "    print(\"Network configurations:\")\n",
    "    print(\"   Regular: Linear layers with standard weights\")\n",
    "    print(\"   Spectral: SpectralLinear layers with spectral normalization\")\n",
    "\n",
    "    # Lipschitz constant estimation\n",
    "    print()\n",
    "    print(\"Lipschitz constant estimation:\")\n",
    "\n",
    "    num_samples = 100\n",
    "    lipschitz_estimates_regular = []\n",
    "    lipschitz_estimates_spectral = []\n",
    "\n",
    "    key = jax.random.PRNGKey(0)\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        # Generate random input pairs\n",
    "        x1 = jax.random.normal(jax.random.split(key)[0], (1, input_dim))\n",
    "        x2 = x1 + 0.1 * jax.random.normal(jax.random.split(key)[1], (1, input_dim))\n",
    "\n",
    "        # Forward passes\n",
    "        y1_regular = regular_model(x1)\n",
    "        y2_regular = regular_model(x2)\n",
    "\n",
    "        # Stabilize spectral model first\n",
    "        if i == 0:\n",
    "            for _ in range(5):  # Warm up spectral normalization\n",
    "                _ = spectral_model(x1, training=True)\n",
    "\n",
    "        y1_spectral = spectral_model(x1, training=False)\n",
    "        y2_spectral = spectral_model(x2, training=False)\n",
    "\n",
    "        # Compute Lipschitz estimates\n",
    "        input_diff = jnp.linalg.norm(x2 - x1)\n",
    "        output_diff_regular = jnp.linalg.norm(y2_regular - y1_regular)\n",
    "        output_diff_spectral = jnp.linalg.norm(y2_spectral - y1_spectral)\n",
    "\n",
    "        lipschitz_regular = output_diff_regular / (input_diff + 1e-8)\n",
    "        lipschitz_spectral = output_diff_spectral / (input_diff + 1e-8)\n",
    "\n",
    "        lipschitz_estimates_regular.append(float(lipschitz_regular))\n",
    "        lipschitz_estimates_spectral.append(float(lipschitz_spectral))\n",
    "\n",
    "        key = jax.random.split(key)[0]\n",
    "\n",
    "    # Statistical analysis\n",
    "    regular_stats = {\n",
    "        \"mean\": jnp.mean(jnp.array(lipschitz_estimates_regular)),\n",
    "        \"std\": jnp.std(jnp.array(lipschitz_estimates_regular)),\n",
    "        \"max\": jnp.max(jnp.array(lipschitz_estimates_regular)),\n",
    "    }\n",
    "\n",
    "    spectral_stats = {\n",
    "        \"mean\": jnp.mean(jnp.array(lipschitz_estimates_spectral)),\n",
    "        \"std\": jnp.std(jnp.array(lipschitz_estimates_spectral)),\n",
    "        \"max\": jnp.max(jnp.array(lipschitz_estimates_spectral)),\n",
    "    }\n",
    "\n",
    "    print(\"   Regular network:\")\n",
    "    print(\n",
    "        f\"     Mean Lipschitz: {regular_stats['mean']:.3f} +/- {regular_stats['std']:.3f}\"\n",
    "    )\n",
    "    print(f\"     Max Lipschitz: {regular_stats['max']:.3f}\")\n",
    "\n",
    "    print(\"   Spectral normalized network:\")\n",
    "    print(\n",
    "        f\"     Mean Lipschitz: {spectral_stats['mean']:.3f} +/- {spectral_stats['std']:.3f}\"\n",
    "    )\n",
    "    print(f\"     Max Lipschitz: {spectral_stats['max']:.3f}\")\n",
    "\n",
    "    # Spectral norm analysis\n",
    "    print()\n",
    "    print(\"Spectral norm analysis:\")\n",
    "    summary = spectral_norm_summary(spectral_model)\n",
    "\n",
    "    if summary.get(\"num_layers\", 0) > 0:\n",
    "        print(f\"   Spectral normalized layers: {summary['num_layers']}\")\n",
    "        print(f\"   Mean spectral norm: {summary['mean_spectral_norm']:.3f}\")\n",
    "        print(f\"   Max spectral norm: {summary['max_spectral_norm']:.3f}\")\n",
    "        print(f\"   Min spectral norm: {summary['min_spectral_norm']:.3f}\")\n",
    "    else:\n",
    "        print(f\"   Spectral normalized layers: {summary.get('num_layers', 0)}\")\n",
    "        print(\"   Mean spectral norm: N/A\")\n",
    "        print(\"   Max spectral norm: N/A\")\n",
    "        print(\"   Min spectral norm: N/A\")\n",
    "        if \"message\" in summary:\n",
    "            print(f\"   Note: {summary['message']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de783c4e",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 7. JAX Transformations Compatibility\n",
    "\n",
    "Verifying compatibility with JIT, grad, vmap, and Hessian computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9eea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_jax_transformations():\n",
    "    \"\"\"Demonstrate JAX transformations compatibility.\"\"\"\n",
    "    print()\n",
    "    print(\"JAX TRANSFORMATIONS COMPATIBILITY\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    rngs = nnx.Rngs(42)\n",
    "\n",
    "    # Create spectral normalized layer\n",
    "    layer = SpectralLinear(12, 6, power_iterations=3, rngs=rngs)\n",
    "\n",
    "    key = jax.random.PRNGKey(0)\n",
    "    x = jax.random.normal(key, (8, 12))\n",
    "\n",
    "    # Test JIT compilation\n",
    "    @jax.jit\n",
    "    def jit_forward(x_input):\n",
    "        return layer(x_input, training=True)\n",
    "\n",
    "    start_time = time.time()\n",
    "    output_jit = jit_forward(x)\n",
    "    end_time = time.time()\n",
    "    print(\n",
    "        f\"JIT compilation: {x.shape} -> {output_jit.shape} \"\n",
    "        f\"({(end_time - start_time) * 1000:.2f} ms)\"\n",
    "    )\n",
    "\n",
    "    # Test gradient computation\n",
    "    def loss_function(x_input):\n",
    "        output = layer(x_input, training=True)\n",
    "        return jnp.sum(output**2)\n",
    "\n",
    "    grad_fn = jax.grad(loss_function)\n",
    "    start_time = time.time()\n",
    "    gradients = grad_fn(x)\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(\n",
    "        f\"Gradient computation: gradient shape {gradients.shape}, \"\n",
    "        f\"norm = {jnp.linalg.norm(gradients):.3f} \"\n",
    "        f\"({(end_time - start_time) * 1000:.2f} ms)\"\n",
    "    )\n",
    "\n",
    "    # Test vectorized mapping (vmap)\n",
    "    batch_x = jax.random.normal(key, (16, 4, 12))  # (batch, mini_batch, features)\n",
    "\n",
    "    vectorized_forward = jax.vmap(\n",
    "        lambda x_single: layer(x_single, training=True), in_axes=0\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "    batch_output = vectorized_forward(batch_x)\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(\n",
    "        f\"Vectorized mapping (vmap): {batch_x.shape} -> {batch_output.shape} \"\n",
    "        f\"({(end_time - start_time) * 1000:.2f} ms)\"\n",
    "    )\n",
    "\n",
    "    # Test higher-order transformations\n",
    "    hessian_fn = jax.hessian(loss_function)\n",
    "    small_x = x[:2, :]  # Smaller input for Hessian computation\n",
    "\n",
    "    start_time = time.time()\n",
    "    hessian = hessian_fn(small_x)\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(\n",
    "        f\"Hessian computation: shape {hessian.shape} \"\n",
    "        f\"({(end_time - start_time) * 1000:.2f} ms)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29eac26e",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 8. Performance Benchmarks\n",
    "\n",
    "Comparing computational overhead of spectral normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b9a568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_performance_benchmark():\n",
    "    \"\"\"Run performance benchmarks comparing spectral vs regular layers.\"\"\"\n",
    "    print()\n",
    "    print(\"PERFORMANCE BENCHMARKS\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    rngs = nnx.Rngs(42)\n",
    "\n",
    "    # Benchmark configurations\n",
    "    configs = [\n",
    "        {\"name\": \"Small\", \"input_dim\": 32, \"output_dim\": 16, \"batch_size\": 64},\n",
    "        {\"name\": \"Medium\", \"input_dim\": 128, \"output_dim\": 64, \"batch_size\": 32},\n",
    "        {\"name\": \"Large\", \"input_dim\": 512, \"output_dim\": 256, \"batch_size\": 8},\n",
    "    ]\n",
    "\n",
    "    for config in configs:\n",
    "        print()\n",
    "        print(f\"{config['name']} benchmark:\")\n",
    "        print(f\"   Dimensions: {config['input_dim']} -> {config['output_dim']}\")\n",
    "        print(f\"   Batch size: {config['batch_size']}\")\n",
    "\n",
    "        # Create layers\n",
    "        regular_layer = nnx.Linear(config[\"input_dim\"], config[\"output_dim\"], rngs=rngs)\n",
    "        spectral_layer = SpectralLinear(\n",
    "            config[\"input_dim\"], config[\"output_dim\"], power_iterations=3, rngs=rngs\n",
    "        )\n",
    "\n",
    "        # Create test data\n",
    "        key = jax.random.PRNGKey(0)\n",
    "        x = jax.random.normal(key, (config[\"batch_size\"], config[\"input_dim\"]))\n",
    "\n",
    "        # JIT compile with proper closure capture\n",
    "        @jax.jit\n",
    "        def regular_forward(x_input, layer=regular_layer):\n",
    "            return layer(x_input)\n",
    "\n",
    "        @jax.jit\n",
    "        def spectral_forward(x_input, layer=spectral_layer):\n",
    "            return layer(x_input, training=True)\n",
    "\n",
    "        # Warm up\n",
    "        _ = regular_forward(x)\n",
    "        _ = spectral_forward(x)\n",
    "\n",
    "        # Benchmark regular layer\n",
    "        num_runs = 100\n",
    "        times_regular = []\n",
    "\n",
    "        for _ in range(num_runs):\n",
    "            start = time.time()\n",
    "            _ = regular_forward(x)\n",
    "            end = time.time()\n",
    "            times_regular.append((end - start) * 1000)\n",
    "\n",
    "        # Benchmark spectral layer\n",
    "        times_spectral = []\n",
    "\n",
    "        for _ in range(num_runs):\n",
    "            start = time.time()\n",
    "            _ = spectral_forward(x)\n",
    "            end = time.time()\n",
    "            times_spectral.append((end - start) * 1000)\n",
    "\n",
    "        # Results\n",
    "        mean_regular = jnp.mean(jnp.array(times_regular))\n",
    "        std_regular = jnp.std(jnp.array(times_regular))\n",
    "        mean_spectral = jnp.mean(jnp.array(times_spectral))\n",
    "        std_spectral = jnp.std(jnp.array(times_spectral))\n",
    "\n",
    "        overhead = (mean_spectral - mean_regular) / mean_regular * 100\n",
    "\n",
    "        print(f\"   Regular layer: {mean_regular:.2f} +/- {std_regular:.2f} ms\")\n",
    "        print(f\"   Spectral layer: {mean_spectral:.2f} +/- {std_spectral:.2f} ms\")\n",
    "        print(f\"   Overhead: {overhead:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cffdcb9",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## 9. Visualization\n",
    "\n",
    "Demonstrating spectral normalization effects on function approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e191a541",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_visualization_demo():\n",
    "    \"\"\"Create visualizations demonstrating spectral normalization effects.\"\"\"\n",
    "    print()\n",
    "    print(\"VISUALIZATION DEMONSTRATIONS\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    rngs = nnx.Rngs(42)\n",
    "\n",
    "    # Test on simple 2D function\n",
    "    x = jnp.linspace(-2, 2, 100)\n",
    "    y_true = jnp.sin(3 * x) * jnp.exp(-(x**2))\n",
    "\n",
    "    # Add noise\n",
    "    noise = 0.1 * jax.random.normal(jax.random.PRNGKey(42), y_true.shape)\n",
    "    y_noisy = y_true + noise\n",
    "\n",
    "    # Create models\n",
    "    regular_model = nnx.Sequential(\n",
    "        nnx.Linear(1, 32, rngs=rngs),\n",
    "        nnx.tanh,\n",
    "        nnx.Linear(32, 32, rngs=rngs),\n",
    "        nnx.tanh,\n",
    "        nnx.Linear(32, 1, rngs=rngs),\n",
    "    )\n",
    "\n",
    "    spectral_model = nnx.Sequential(\n",
    "        SpectralLinear(1, 32, power_iterations=5, rngs=rngs),\n",
    "        nnx.tanh,\n",
    "        SpectralLinear(32, 32, power_iterations=5, rngs=rngs),\n",
    "        nnx.tanh,\n",
    "        SpectralLinear(32, 1, power_iterations=5, rngs=rngs),\n",
    "    )\n",
    "\n",
    "    # Simple training simulation (just a few steps for demonstration)\n",
    "    x_input = x.reshape(-1, 1)\n",
    "    y_target = y_noisy.reshape(-1, 1)\n",
    "\n",
    "    print(\"Function approximation demonstration:\")\n",
    "    print(f\"   Training data: {x_input.shape} -> {y_target.shape}\")\n",
    "    print(\"   True function: sin(3x) * exp(-x^2)\")\n",
    "    print(\"   Noise level: 10%\")\n",
    "\n",
    "    # Quick \"training\" simulation\n",
    "    for i in range(5):\n",
    "        # Regular model prediction\n",
    "        y_pred_regular = regular_model(x_input)\n",
    "\n",
    "        # Spectral model prediction (stabilize first)\n",
    "        if i == 0:\n",
    "            for _ in range(3):\n",
    "                _ = spectral_model(x_input, training=True)\n",
    "        y_pred_spectral = spectral_model(x_input, training=False)\n",
    "\n",
    "        if i % 2 == 0:\n",
    "            mse_regular = jnp.mean((y_pred_regular - y_target) ** 2)\n",
    "            mse_spectral = jnp.mean((y_pred_spectral - y_target) ** 2)\n",
    "\n",
    "            print(\n",
    "                f\"   Step {i}: Regular MSE = {mse_regular:.6f}, \"\n",
    "                f\"Spectral MSE = {mse_spectral:.6f}\"\n",
    "            )\n",
    "\n",
    "    print(\"   Note: In practice, spectral normalization provides more stable training\")\n",
    "    print(\"         and better generalization, especially for longer training periods.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b65ab11",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Results Summary\n",
    "\n",
    "| Component | Benefit | Overhead |\n",
    "|-----------|---------|----------|\n",
    "| SpectralLinear | Bounded Lipschitz constant | ~10-30% |\n",
    "| SpectralNormalizedConv | Stable spatial processing | ~15-25% |\n",
    "| SpectralMultiHeadAttention | Stable attention weights | ~10-20% |\n",
    "| AdaptiveSpectralNorm | Flexible per-layer control | ~10-20% |\n",
    "| PowerIteration | Efficient norm estimation | O(n) per iteration |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Spectral normalization controls Lipschitz constants for stable training\n",
    "- Power iteration provides efficient O(n) spectral norm estimation\n",
    "- Adaptive bounds allow layer-specific flexibility\n",
    "- JAX transformations (JIT, grad, vmap) work seamlessly\n",
    "- Modest overhead (~10-30%) for significant stability improvements\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "### Experiments to Try\n",
    "\n",
    "1. Apply spectral normalization to FNO spectral layers for stable Darcy flow training\n",
    "2. Compare training convergence with and without spectral normalization on PINN problems\n",
    "3. Use `AdaptiveSpectralNorm` with learnable bounds for multi-scale architectures\n",
    "\n",
    "### Related Examples\n",
    "\n",
    "- [FNO Darcy Comprehensive](../models/fno_darcy_comprehensive.md) - Apply spectral layers in training\n",
    "- [Grid Embeddings](grid_embeddings_example.md) - Spatial coordinate injection\n",
    "- [Neural Operator Benchmark](../comparative_studies/neural_operator_benchmark.md) - Cross-architecture comparison\n",
    "\n",
    "### API Reference\n",
    "\n",
    "- [`SpectralLinear`](../../api/neural.md) - Spectral normalized linear layer\n",
    "- [`SpectralNormalizedConv`](../../api/neural.md) - Spectral normalized convolution\n",
    "- [`SpectralMultiHeadAttention`](../../api/neural.md) - Spectral normalized attention\n",
    "- [`AdaptiveSpectralNorm`](../../api/neural.md) - Adaptive spectral bounds\n",
    "- [`PowerIteration`](../../api/neural.md) - Spectral norm estimation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd71822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Run all spectral normalization demonstrations.\"\"\"\n",
    "    print(\"SPECTRAL NORMALIZATION FOR NEURAL OPERATORS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Comprehensive demonstrations of spectral normalization techniques\")\n",
    "    print(\"for enhancing neural operator stability and controlling Lipschitz constants\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Run all demonstrations\n",
    "    demonstrate_basic_spectral_layers()\n",
    "    demonstrate_spectral_attention()\n",
    "    demonstrate_adaptive_spectral_norm()\n",
    "    demonstrate_power_iteration_algorithm()\n",
    "    demonstrate_complete_neural_operators()\n",
    "    demonstrate_stability_analysis()\n",
    "    demonstrate_jax_transformations()\n",
    "    run_performance_benchmark()\n",
    "    create_visualization_demo()\n",
    "\n",
    "    print()\n",
    "    print(\"=\" * 60)\n",
    "    print(\"SPECTRAL NORMALIZATION DEMONSTRATIONS COMPLETE\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    print(\"Key Takeaways:\")\n",
    "    print(\"- Spectral normalization helps control Lipschitz constants\")\n",
    "    print(\"- Power iteration provides efficient spectral norm estimation\")\n",
    "    print(\"- Adaptive bounds allow flexible control over normalization\")\n",
    "    print(\"- JAX transformations work seamlessly with spectral layers\")\n",
    "    print(\"- Modest performance overhead (~10-30%) for improved stability\")\n",
    "    print(\"- Particularly beneficial for PDE-solving neural operators\")\n",
    "    print()\n",
    "    print(\"Usage Recommendations:\")\n",
    "    print(\"- Use SpectralLinear for critical stability layers\")\n",
    "    print(\"- Apply SpectralNormalizedConv for spatial neural operators\")\n",
    "    print(\"- Consider AdaptiveSpectralNorm for layer-specific tuning\")\n",
    "    print(\"- Increase power_iterations for better spectral norm accuracy\")\n",
    "    print(\"- Monitor spectral norms using spectral_norm_summary()\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "py:percent,ipynb",
   "main_language": "python",
   "notebook_metadata_filter": "kernelspec,language_info,-jupytext.text_representation.jupytext_version,-jupytext.notebook_metadata_filter,-jupytext.cell_metadata_filter"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
