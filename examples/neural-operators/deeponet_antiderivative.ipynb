{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "875e1858",
   "metadata": {},
   "source": [
    "# DeepONet on Antiderivative Operator\n",
    "\n",
    "| Property      | Value                                    |\n",
    "|---------------|------------------------------------------|\n",
    "| Level         | Beginner                                 |\n",
    "| Runtime       | ~30s (CPU), ~5s (GPU)                    |\n",
    "| Memory        | ~500 MB                                  |\n",
    "| Prerequisites | JAX, Flax NNX, Neural Operators basics   |\n",
    "\n",
    "## Overview\n",
    "\n",
    "Train a DeepONet to learn the antiderivative operator, the canonical benchmark\n",
    "from the original DeepONet paper (Lu et al., 2021). Given a function v(x),\n",
    "the operator learns to predict u(x) = ∫₀ˣ v(t) dt.\n",
    "\n",
    "This example demonstrates:\n",
    "\n",
    "- **DeepONet architecture** with branch (function encoder) and trunk (location encoder)\n",
    "- **Custom training loop** for operators with two distinct inputs\n",
    "- **Antiderivative data generation** using Gaussian Random Field (GRF) basis\n",
    "- **Zero initial condition constraint** via output transformation\n",
    "\n",
    "Equivalent to DeepXDE's `antiderivative_aligned.py` example,\n",
    "reimplemented using Opifex APIs.\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "1. Understand branch-trunk DeepONet architecture\n",
    "2. Generate synthetic operator learning data\n",
    "3. Implement custom training loop for multi-input operators\n",
    "4. Apply physics constraints via output transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1b7479",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48058b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import optax\n",
    "from flax import nnx\n",
    "\n",
    "\n",
    "mpl.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from opifex.neural.operators.deeponet import DeepONet\n",
    "\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Opifex Example: DeepONet on Antiderivative Operator\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"JAX backend: {jax.default_backend()}\")\n",
    "print(f\"JAX devices: {jax.devices()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2343c9",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "The antiderivative operator maps v(x) → u(x) where du/dx = v(x) and u(0) = 0.\n",
    "We generate input functions v using a Gaussian Random Field (GRF) basis\n",
    "with random coefficients, ensuring diverse function shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e725eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SENSORS = 50  # Number of sensor points for input function\n",
    "N_TRAIN = 1000  # Training samples\n",
    "N_TEST = 200  # Test samples\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 100\n",
    "LEARNING_RATE = 1e-3\n",
    "LATENT_DIM = 64  # Shared dimension for branch/trunk outputs\n",
    "\n",
    "SEED = 42\n",
    "OUTPUT_DIR = Path(\"docs/assets/examples/deeponet_antiderivative\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Sensors: {N_SENSORS}\")\n",
    "print(f\"Training samples: {N_TRAIN}, Test samples: {N_TEST}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}, Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}, Latent dim: {LATENT_DIM}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce73dba8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Data Generation\n",
    "\n",
    "Generate input functions v(x) using a truncated Fourier series with random\n",
    "coefficients. For each v, compute the antiderivative u(x) = ∫₀ˣ v(t) dt\n",
    "using cumulative trapezoidal integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16762b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_grf_function(x: np.ndarray, n_modes: int, rng: np.random.Generator):\n",
    "    \"\"\"Generate a smooth random function using Gaussian Random Field basis.\n",
    "\n",
    "    Uses sine basis with decaying random coefficients to ensure smoothness.\n",
    "    \"\"\"\n",
    "    coeffs = rng.standard_normal(n_modes)\n",
    "    # Decay coefficients for smoothness (higher modes contribute less)\n",
    "    decay = 1.0 / (np.arange(1, n_modes + 1) ** 0.5)\n",
    "    coeffs = coeffs * decay\n",
    "\n",
    "    # Sum sine basis functions\n",
    "    v = np.zeros_like(x)\n",
    "    for k in range(n_modes):\n",
    "        v += coeffs[k] * np.sin((k + 1) * np.pi * x)\n",
    "    return v\n",
    "\n",
    "\n",
    "def compute_antiderivative(x: np.ndarray, v: np.ndarray):\n",
    "    \"\"\"Compute antiderivative u(x) = ∫₀ˣ v(t) dt using trapezoidal rule.\"\"\"\n",
    "    dx = x[1] - x[0]\n",
    "    u = np.zeros_like(v)\n",
    "    u[1:] = np.cumsum(0.5 * (v[:-1] + v[1:])) * dx\n",
    "    return u\n",
    "\n",
    "\n",
    "def generate_dataset(n_samples: int, n_sensors: int, seed: int):\n",
    "    \"\"\"Generate antiderivative operator dataset.\n",
    "\n",
    "    Returns:\n",
    "        branch_input: (n_samples, n_sensors) - input function values v(x)\n",
    "        trunk_input: (n_sensors, 1) - evaluation locations x\n",
    "        targets: (n_samples, n_sensors) - antiderivative values u(x)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    x = np.linspace(0, 1, n_sensors)\n",
    "\n",
    "    branch_inputs = []\n",
    "    targets = []\n",
    "\n",
    "    for _ in range(n_samples):\n",
    "        # Generate random input function\n",
    "        v = generate_grf_function(x, n_modes=10, rng=rng)\n",
    "        # Compute antiderivative\n",
    "        u = compute_antiderivative(x, v)\n",
    "\n",
    "        branch_inputs.append(v)\n",
    "        targets.append(u)\n",
    "\n",
    "    branch_input = np.stack(branch_inputs, axis=0)  # (n_samples, n_sensors)\n",
    "    trunk_input = x[:, np.newaxis]  # (n_sensors, 1)\n",
    "    targets = np.stack(targets, axis=0)  # (n_samples, n_sensors)\n",
    "\n",
    "    return branch_input, trunk_input, targets\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"Generating antiderivative dataset...\")\n",
    "X_branch_train, trunk_coords, Y_train = generate_dataset(N_TRAIN, N_SENSORS, SEED)\n",
    "X_branch_test, _, Y_test = generate_dataset(N_TEST, N_SENSORS, SEED + 1000)\n",
    "\n",
    "print(f\"Training data: branch={X_branch_train.shape}, trunk={trunk_coords.shape}\")\n",
    "print(f\"Training targets: {Y_train.shape}\")\n",
    "print(f\"Test data: branch={X_branch_test.shape}, targets={Y_test.shape}\")\n",
    "print(f\"Input:  function values v(x) at {N_SENSORS} sensors\")\n",
    "print(f\"Output: antiderivative u(x) at {N_SENSORS} locations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a668e8e9",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "\n",
    "DeepONet has two networks: the **branch network** encodes the input function\n",
    "v(x) sampled at sensor locations, and the **trunk network** encodes the\n",
    "query locations x. The final output is the dot product of their representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e412148",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"Creating DeepONet model...\")\n",
    "model = DeepONet(\n",
    "    branch_sizes=[N_SENSORS, 128, 128, LATENT_DIM],  # v(x) → latent\n",
    "    trunk_sizes=[1, 128, 128, LATENT_DIM],  # x → latent\n",
    "    activation=\"tanh\",\n",
    "    rngs=nnx.Rngs(SEED),\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "params = nnx.state(model, nnx.Param)\n",
    "param_count = sum(x.size for x in jax.tree_util.tree_leaves(params))\n",
    "print(\"Model: DeepONet\")\n",
    "print(f\"  Branch network: {N_SENSORS} → 128 → 128 → {LATENT_DIM}\")\n",
    "print(f\"  Trunk network: 1 → 128 → 128 → {LATENT_DIM}\")\n",
    "print(f\"  Latent dimension: {LATENT_DIM}\")\n",
    "print(f\"  Total parameters: {param_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efb8964",
   "metadata": {},
   "source": [
    "## Custom Training Loop\n",
    "\n",
    "DeepONet requires a custom training loop because it has two distinct inputs\n",
    "(branch for function values, trunk for locations). We enforce the zero initial\n",
    "condition u(0) = 0 by multiplying predictions by the x-coordinate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7065ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"Setting up training...\")\n",
    "\n",
    "# Convert to JAX arrays\n",
    "X_branch_train_jax = jnp.array(X_branch_train)\n",
    "X_branch_test_jax = jnp.array(X_branch_test)\n",
    "trunk_jax = jnp.array(trunk_coords)  # Shared across all samples\n",
    "Y_train_jax = jnp.array(Y_train)\n",
    "Y_test_jax = jnp.array(Y_test)\n",
    "\n",
    "# Create optimizer\n",
    "opt = nnx.Optimizer(model, optax.adam(LEARNING_RATE), wrt=nnx.Param)\n",
    "\n",
    "\n",
    "def apply_zero_ic(predictions: jnp.ndarray, x_coords: jnp.ndarray) -> jnp.ndarray:\n",
    "    \"\"\"Apply zero initial condition: u(0) = 0 by multiplying by x.\"\"\"\n",
    "    # x_coords shape: (n_locations, 1), predictions: (batch, n_locations)\n",
    "    return predictions * x_coords.squeeze()\n",
    "\n",
    "\n",
    "@nnx.jit\n",
    "def train_step(model, opt, x_branch, x_trunk, y_target):\n",
    "    \"\"\"Single training step with MSE loss.\"\"\"\n",
    "\n",
    "    def loss_fn(model):\n",
    "        # Broadcast trunk to batch dimension\n",
    "        batch_size = x_branch.shape[0]\n",
    "        trunk_batch = jnp.broadcast_to(x_trunk[None], (batch_size, *x_trunk.shape))\n",
    "        # Forward pass\n",
    "        y_pred = model(x_branch, trunk_batch)\n",
    "        # Apply zero IC constraint\n",
    "        y_pred = apply_zero_ic(y_pred, x_trunk)\n",
    "        # MSE loss\n",
    "        return jnp.mean((y_pred - y_target) ** 2)\n",
    "\n",
    "    loss, grads = nnx.value_and_grad(loss_fn)(model)\n",
    "    opt.update(model, grads)\n",
    "    return loss\n",
    "\n",
    "\n",
    "@nnx.jit\n",
    "def eval_model(model, x_branch, x_trunk):\n",
    "    \"\"\"Evaluate model with zero IC constraint.\"\"\"\n",
    "    batch_size = x_branch.shape[0]\n",
    "    trunk_batch = jnp.broadcast_to(x_trunk[None], (batch_size, *x_trunk.shape))\n",
    "    return apply_zero_ic(model(x_branch, trunk_batch), x_trunk)\n",
    "\n",
    "\n",
    "print(f\"Optimizer: Adam (lr={LEARNING_RATE})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c5b3ce",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c94d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"Starting training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "n_batches = N_TRAIN // BATCH_SIZE\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_losses = []\n",
    "    # Shuffle training data\n",
    "    perm = np.random.permutation(N_TRAIN)\n",
    "    X_shuffled = X_branch_train_jax[perm]\n",
    "    Y_shuffled = Y_train_jax[perm]\n",
    "\n",
    "    for i in range(n_batches):\n",
    "        start_idx = i * BATCH_SIZE\n",
    "        end_idx = start_idx + BATCH_SIZE\n",
    "        x_batch = X_shuffled[start_idx:end_idx]\n",
    "        y_batch = Y_shuffled[start_idx:end_idx]\n",
    "\n",
    "        loss = train_step(model, opt, x_batch, trunk_jax, y_batch)\n",
    "        epoch_losses.append(float(loss))\n",
    "\n",
    "    train_loss = np.mean(epoch_losses)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Validation\n",
    "    val_pred = eval_model(model, X_branch_test_jax, trunk_jax)\n",
    "    val_loss = float(jnp.mean((val_pred - Y_test_jax) ** 2))\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    if (epoch + 1) % 20 == 0 or epoch == 0:\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1:3d}/{NUM_EPOCHS}: train_loss={train_loss:.6f}, val_loss={val_loss:.6f}\"\n",
    "        )\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training completed in {training_time:.1f}s\")\n",
    "print(f\"Final train loss: {train_losses[-1]:.6f}\")\n",
    "print(f\"Final val loss:   {val_losses[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509a1ade",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e72c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"Running evaluation...\")\n",
    "\n",
    "# Get predictions on test set\n",
    "predictions = eval_model(model, X_branch_test_jax, trunk_jax)\n",
    "\n",
    "# Compute metrics\n",
    "test_mse = float(jnp.mean((predictions - Y_test_jax) ** 2))\n",
    "\n",
    "# Per-sample relative L2 error\n",
    "pred_diff = predictions - Y_test_jax\n",
    "per_sample_rel_l2 = jnp.linalg.norm(pred_diff, axis=1) / jnp.linalg.norm(\n",
    "    Y_test_jax, axis=1\n",
    ")\n",
    "mean_rel_l2 = float(jnp.mean(per_sample_rel_l2))\n",
    "\n",
    "print(f\"Test MSE:         {test_mse:.6f}\")\n",
    "print(f\"Test Relative L2: {mean_rel_l2:.6f}\")\n",
    "print(f\"Min Relative L2:  {float(jnp.min(per_sample_rel_l2)):.6f}\")\n",
    "print(f\"Max Relative L2:  {float(jnp.max(per_sample_rel_l2)):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2c129e",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Visualize sample predictions and training curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda20174",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"Generating visualizations...\")\n",
    "\n",
    "x_grid = np.linspace(0, 1, N_SENSORS)\n",
    "\n",
    "# --- Sample predictions ---\n",
    "n_vis = 4\n",
    "fig, axes = plt.subplots(n_vis, 3, figsize=(12, 3 * n_vis))\n",
    "fig.suptitle(\n",
    "    \"DeepONet Antiderivative Predictions (Opifex)\", fontsize=14, fontweight=\"bold\"\n",
    ")\n",
    "\n",
    "for i in range(n_vis):\n",
    "    # Input function v(x)\n",
    "    axes[i, 0].plot(x_grid, X_branch_test[i], \"b-\", linewidth=1.5)\n",
    "    axes[i, 0].set_title(\"Input v(x)\" if i == 0 else \"\")\n",
    "    axes[i, 0].set_ylabel(f\"Sample {i}\")\n",
    "    axes[i, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # True antiderivative u(x)\n",
    "    axes[i, 1].plot(x_grid, Y_test[i], \"b-\", linewidth=1.5, label=\"Truth\")\n",
    "    axes[i, 1].plot(\n",
    "        x_grid, np.array(predictions[i]), \"r--\", linewidth=1.5, label=\"DeepONet\"\n",
    "    )\n",
    "    axes[i, 1].set_title(\"Antiderivative u(x)\" if i == 0 else \"\")\n",
    "    if i == 0:\n",
    "        axes[i, 1].legend(fontsize=8)\n",
    "    axes[i, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Error\n",
    "    error = np.array(predictions[i]) - Y_test[i]\n",
    "    axes[i, 2].plot(x_grid, error, \"k-\", linewidth=1.0)\n",
    "    axes[i, 2].axhline(y=0, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "    axes[i, 2].set_title(\"Error\" if i == 0 else \"\")\n",
    "    axes[i, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"predictions.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "print(f\"Sample predictions saved to {OUTPUT_DIR / 'predictions.png'}\")\n",
    "\n",
    "# --- Training curves ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "fig.suptitle(\"DeepONet Training Progress\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "epochs_arr = np.arange(1, NUM_EPOCHS + 1)\n",
    "axes[0].semilogy(epochs_arr, train_losses, \"b-\", linewidth=1.5, label=\"Train\")\n",
    "axes[0].semilogy(epochs_arr, val_losses, \"r-\", linewidth=1.5, label=\"Validation\")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"MSE Loss (log scale)\")\n",
    "axes[0].set_title(\"Training Curves\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Error distribution\n",
    "per_sample_errors = np.array(per_sample_rel_l2)\n",
    "axes[1].hist(\n",
    "    per_sample_errors, bins=30, alpha=0.7, color=\"steelblue\", edgecolor=\"black\"\n",
    ")\n",
    "axes[1].set_xlabel(\"Relative L2 Error\")\n",
    "axes[1].set_ylabel(\"Frequency\")\n",
    "axes[1].set_title(\"Error Distribution\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"training.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "print(f\"Training curves saved to {OUTPUT_DIR / 'training.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f0eefa",
   "metadata": {},
   "source": [
    "## Results Summary\n",
    "\n",
    "The DeepONet learns to approximate the antiderivative operator with low error.\n",
    "Key observations:\n",
    "- The zero IC constraint (multiplying by x) ensures u(0) = 0\n",
    "- Smooth GRF-based input functions are well-captured by the learned operator\n",
    "- Error is typically largest near x=1 where the integral accumulates\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Try different input function distributions (step functions, polynomials)\n",
    "- Experiment with physics-informed loss (adding du/dx = v constraint)\n",
    "- Scale to higher-dimensional problems\n",
    "- Compare against `FourierEnhancedDeepONet` for spectral input functions\n",
    "\n",
    "### Related Examples\n",
    "\n",
    "- [DeepONet on Darcy Flow](deeponet-darcy.md) — 2D operator learning\n",
    "- [FNO on Burgers Equation](fno-burgers.md) — Temporal evolution operator\n",
    "- [Operator Comparison Tour](operator-tour.md) — Compare all operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d534af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(f\"DeepONet Antiderivative example completed in {training_time:.1f}s\")\n",
    "print(f\"Test MSE: {test_mse:.6f}, Relative L2: {mean_rel_l2:.6f}\")\n",
    "print(f\"Results saved to: {OUTPUT_DIR}\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "py:percent,ipynb",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
